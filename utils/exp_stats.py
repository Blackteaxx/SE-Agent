#!/usr/bin/env python3
"""
å®éªŒç»Ÿè®¡åˆ†æè„šæœ¬ (exp_stats.py)

åˆ†æ SE_Perf å®éªŒè¿è¡Œç»Ÿè®¡ï¼ŒåŒ…æ‹¬ï¼š
1. æ¯ä¸ªä»»åŠ¡çš„æ€»è¿è¡Œæ—¶é—´
2. LLM è°ƒç”¨æ¬¡æ•°å’Œé‡è¯•ç»Ÿè®¡
3. è¯„ä¼°è€—æ—¶ç»Ÿè®¡

ç”¨æ³•:
    python utils/exp_stats.py <trajectory_dir> [--compare <other_dir>]
    
ç¤ºä¾‹:
    python utils/exp_stats.py trajectories_perf/deepseek-v3/Plan-Random-Local-Global-45its_20251218_160428
    
    # å¯¹æ¯”ä¸¤ä¸ªç›®å½•
    python utils/exp_stats.py trajectories_perf/deepseek-v3/Plan-Random-Local-Global-45its_20251218_160428 \
        --compare trajectories_perf/deepseek-v3/Plan-Weighted-45its_20251218_153854
"""

import argparse
import json
import os
import re
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import NamedTuple

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ (å…¨å±€ï¼Œé¿å…é‡å¤ç¼–è¯‘)
RE_MAX_RETRY = re.compile(rb"attempt=10/10")
RE_LIMITING = re.compile(rb"5513-chatGPt\.limiting")
RE_LLM_CALL = re.compile(r"è°ƒç”¨LLM:".encode())
# åªåŒ¹é… step_2 çš„è¯„ä¼°æ—¶é—´ï¼ˆstep_1 æ˜¯åˆå§‹åŒ–ï¼Œè€—æ—¶ 0.00sï¼‰
RE_EVAL_TIME = re.compile(rb"step_2.*\xe8\x80\x97\xe6\x97\xb6 ([\d.]+)s")  # "step_2...è€—æ—¶ XXXs"
# æå–æ—¥å¿—æ—¶é—´æˆ³: "2025-12-18 16:04:38,703"
RE_LOG_TIMESTAMP = re.compile(rb"^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})")


class TaskStats(NamedTuple):
    """ä»»åŠ¡ç»Ÿè®¡ç»“æœ"""

    task_name: str
    # ä»»åŠ¡è¿è¡Œæ—¶é—´
    total_run_time: float  # æ€»è¿è¡Œæ—¶é—´ (ç§’)
    # LLM é‡è¯•ç›¸å…³
    max_retry_count: int  # attempt=10/10 çš„æ¬¡æ•°
    total_limiting_count: int  # é™æµæ¬¡æ•°
    total_llm_calls: int  # æ€» LLM è°ƒç”¨æ¬¡æ•°
    # è¯„ä¼°è€—æ—¶ç›¸å…³
    eval_count: int  # è¯„ä¼°æ¬¡æ•°
    total_eval_time: float  # æ€»è¯„ä¼°æ—¶é—´ (ç§’)
    avg_eval_time: float  # å¹³å‡è¯„ä¼°æ—¶é—´ (ç§’)
    max_eval_time: float  # æœ€å¤§è¯„ä¼°æ—¶é—´ (ç§’)
    min_eval_time: float  # æœ€å°è¯„ä¼°æ—¶é—´ (ç§’)


def parse_log_timestamp(ts_bytes: bytes) -> datetime | None:
    """è§£ææ—¥å¿—æ—¶é—´æˆ³"""
    try:
        ts_str = ts_bytes.decode("utf-8")
        return datetime.strptime(ts_str, "%Y-%m-%d %H:%M:%S,%f")
    except (ValueError, UnicodeDecodeError):
        return None


def analyze_se_framework_log(log_path: Path) -> dict:
    """åˆ†æ se_framework.log æ–‡ä»¶ï¼ˆä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼ + é¢„ç¼–è¯‘æ­£åˆ™ï¼Œæ›´å¿«ï¼‰"""
    stats = {
        "max_retry_count": 0,
        "total_limiting_count": 0,
        "total_llm_calls": 0,
        "total_run_time": 0.0,
    }

    if not log_path.exists():
        return stats

    try:
        # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–ï¼Œé¿å…ç¼–ç è½¬æ¢å¼€é”€
        with open(log_path, "rb") as f:
            content = f.read()

        # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
        stats["max_retry_count"] = len(RE_MAX_RETRY.findall(content))
        stats["total_limiting_count"] = len(RE_LIMITING.findall(content))
        stats["total_llm_calls"] = len(RE_LLM_CALL.findall(content))

        # æå–å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼Œè®¡ç®—æ€»è¿è¡Œæ—¶é—´
        lines = content.split(b"\n")
        start_time = None
        end_time = None
        end_time_marker = None

        # æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æˆ³
        for line in lines:
            match = RE_LOG_TIMESTAMP.match(line)
            if match:
                start_time = parse_log_timestamp(match.group(1))
                break

        # æ‰¾åˆ°é¦–æ¬¡å‡ºç°â€œç”Ÿæˆæœ€ç»ˆç»“æœ final.jsonâ€æ‰€åœ¨è¡Œçš„æ—¶é—´æˆ³ä½œä¸ºç»“æŸæ—¶é—´
        for line in lines:
            if b"final.json" in line:
                try:
                    text = line.decode("utf-8", errors="ignore")
                    if "ç”Ÿæˆæœ€ç»ˆç»“æœ final.json" in text:
                        m = RE_LOG_TIMESTAMP.match(line)
                        if m:
                            end_time_marker = parse_log_timestamp(m.group(1))
                            break
                except Exception:
                    continue

        # æ‰¾æœ€åä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æˆ³
        for line in reversed(lines):
            match = RE_LOG_TIMESTAMP.match(line)
            if match:
                end_time = parse_log_timestamp(match.group(1))
                break

        if start_time:
            chosen_end = end_time_marker or end_time
            if chosen_end:
                stats["total_run_time"] = (chosen_end - start_time).total_seconds()

    except Exception as e:
        print(f"Warning: æ— æ³•åˆ†æ {log_path}: {e}", file=sys.stderr)

    return stats


def analyze_perfagent_logs(task_dir: Path) -> dict:
    """åˆ†æ perfagent.log æ–‡ä»¶è·å–è¯„ä¼°è€—æ—¶ï¼ˆä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼ + é¢„ç¼–è¯‘æ­£åˆ™ï¼Œæ›´å¿«ï¼‰"""
    stats = {
        "eval_count": 0,
        "total_eval_time": 0.0,
        "max_eval_time": 0.0,
        "min_eval_time": float("inf"),
        "eval_times": [],
    }

    task_name = task_dir.name

    # æŸ¥æ‰¾æ‰€æœ‰ iteration_*/task_name/perfagent.log
    for iteration_dir in task_dir.glob("iteration_*"):
        inner_perfagent = iteration_dir / task_name / "perfagent.log"
        if inner_perfagent.exists():
            try:
                # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–
                with open(inner_perfagent, "rb") as f:
                    content = f.read()

                # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼æå– step_2 çš„è¯„ä¼°æ—¶é—´ï¼ˆè·³è¿‡ step_1 åˆå§‹åŒ–ï¼‰
                times = RE_EVAL_TIME.findall(content)
                for t in times:
                    try:
                        time_val = float(t.decode("utf-8"))
                        stats["eval_times"].append(time_val)
                        stats["eval_count"] += 1
                        stats["total_eval_time"] += time_val
                        stats["max_eval_time"] = max(stats["max_eval_time"], time_val)
                        stats["min_eval_time"] = min(stats["min_eval_time"], time_val)
                    except ValueError:
                        pass
            except Exception:
                pass

    if stats["min_eval_time"] == float("inf"):
        stats["min_eval_time"] = 0.0

    return stats


def analyze_single_task(task_dir: Path) -> TaskStats:
    """åˆ†æå•ä¸ªä»»åŠ¡ç›®å½•ï¼ˆä¾›å¤šè¿›ç¨‹è°ƒç”¨ï¼‰"""
    task_name = task_dir.name

    # åˆ†æ se_framework.log
    se_log = task_dir / "se_framework.log"
    se_stats = analyze_se_framework_log(se_log)

    # åˆ†æ perfagent.log
    eval_stats = analyze_perfagent_logs(task_dir)

    # è®¡ç®—å¹³å‡å€¼
    avg_eval_time = 0.0
    if eval_stats["eval_count"] > 0:
        avg_eval_time = eval_stats["total_eval_time"] / eval_stats["eval_count"]

    return TaskStats(
        task_name=task_name,
        total_run_time=se_stats["total_run_time"],
        max_retry_count=se_stats["max_retry_count"],
        total_limiting_count=se_stats["total_limiting_count"],
        total_llm_calls=se_stats["total_llm_calls"],
        eval_count=eval_stats["eval_count"],
        total_eval_time=eval_stats["total_eval_time"],
        avg_eval_time=avg_eval_time,
        max_eval_time=eval_stats["max_eval_time"],
        min_eval_time=eval_stats["min_eval_time"],
    )


def analyze_directory(traj_dir: Path, max_workers: int | None = None) -> list[TaskStats]:
    """åˆ†ææ•´ä¸ªè½¨è¿¹ç›®å½•ï¼ˆä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡ŒåŠ é€Ÿï¼‰"""
    results = []

    if not traj_dir.exists():
        print(f"Error: ç›®å½•ä¸å­˜åœ¨: {traj_dir}", file=sys.stderr)
        return results

    # éå†æ‰€æœ‰ä»»åŠ¡ç›®å½•
    task_dirs = [d for d in traj_dir.iterdir() if d.is_dir() and not d.name.startswith(".")]

    if not task_dirs:
        return results

    # é»˜è®¤ä½¿ç”¨ CPU æ ¸å¿ƒæ•°
    if max_workers is None:
        max_workers = min(os.cpu_count() or 4, len(task_dirs))

    print(f"  ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œåˆ†æ {len(task_dirs)} ä¸ªä»»åŠ¡...")

    # ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(analyze_single_task, task_dir): task_dir for task_dir in task_dirs}

        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                task_dir = futures[future]
                print(f"Warning: åˆ†æä»»åŠ¡ {task_dir.name} å¤±è´¥: {e}", file=sys.stderr)

    # æŒ‰ä»»åŠ¡åæ’åº
    results.sort(key=lambda x: x.task_name)

    return results


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´ï¼Œæ˜¾ç¤ºä¸º å°æ—¶:åˆ†é’Ÿ:ç§’"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    if hours > 0:
        return f"{hours}h {minutes}m {secs:.0f}s"
    elif minutes > 0:
        return f"{minutes}m {secs:.0f}s"
    else:
        return f"{secs:.1f}s"


def print_stats(results: list[TaskStats], title: str):
    """æ‰“å°ç»Ÿè®¡ç»“æœ"""
    print(f"\n{'=' * 80}")
    print(f"  {title}")
    print(f"{'=' * 80}")

    # è¿è¡Œæ—¶é—´ç»Ÿè®¡
    run_times = [r.total_run_time for r in results if r.total_run_time > 0]
    total_run_time = sum(run_times)
    avg_run_time = total_run_time / len(run_times) if run_times else 0
    max_run_time = max(run_times) if run_times else 0
    min_run_time = min(run_times) if run_times else 0

    # æ€»ä½“ç»Ÿè®¡
    total_max_retry = sum(r.max_retry_count for r in results)
    total_limiting = sum(r.total_limiting_count for r in results)
    total_llm_calls = sum(r.total_llm_calls for r in results)
    tasks_with_retry = sum(1 for r in results if r.max_retry_count > 0)

    print("\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  - ä»»åŠ¡æ€»æ•°: {len(results)}")
    print(f"  - æ€»è¿è¡Œæ—¶é—´: {format_duration(total_run_time)} (å¹³å‡: {format_duration(avg_run_time)})")
    print(f"  - è¿è¡Œæ—¶é—´èŒƒå›´: {format_duration(min_run_time)} ~ {format_duration(max_run_time)}")
    print(f"  - æœ‰æœ€å¤§é‡è¯•çš„ä»»åŠ¡æ•°: {tasks_with_retry}")
    print(f"  - æ€»è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° (attempt=10/10): {total_max_retry}")
    print(f"  - æ€»é™æµæ¬¡æ•°: {total_limiting}")
    print(f"  - æ€» LLM è°ƒç”¨æ¬¡æ•°: {total_llm_calls}")

    # è¿è¡Œæ—¶é—´ TOP 20
    print("\nğŸ• è¿è¡Œæ—¶é—´ TOP 20:")
    sorted_by_runtime = sorted(results, key=lambda x: x.total_run_time, reverse=True)[:20]
    for r in sorted_by_runtime:
        if r.total_run_time > 0:
            print(f"  {r.task_name}: {format_duration(r.total_run_time)}")

    # LLM æœ€å¤§é‡è¯• TOP 20
    print("\nğŸ”´ LLM è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° TOP 20 (attempt=10/10):")
    sorted_by_retry = sorted(results, key=lambda x: x.max_retry_count, reverse=True)[:20]
    for r in sorted_by_retry:
        if r.max_retry_count > 0:
            print(f"  {r.task_name}: {r.max_retry_count} æ¬¡")

    # è¯„ä¼°è€—æ—¶ TOP 20 (æŒ‰å¹³å‡è€—æ—¶)
    print("\nâ±ï¸  è¯„ä¼°è€—æ—¶ TOP 20 (æŒ‰å¹³å‡è€—æ—¶æ’åº):")
    sorted_by_avg = sorted(results, key=lambda x: x.avg_eval_time, reverse=True)[:20]
    for r in sorted_by_avg:
        if r.eval_count > 0:
            print(f"  {r.task_name}: æ¬¡æ•°={r.eval_count}, å¹³å‡={r.avg_eval_time:.1f}s, æœ€å¤§={r.max_eval_time:.1f}s")

    # å¼‚å¸¸æƒ…å†µ (æœ€å¤§è¯„ä¼°æ—¶é—´ > 300s)
    print("\nâš ï¸  å¼‚å¸¸è¯„ä¼°è€—æ—¶ (å•æ¬¡ > 300s):")
    sorted_by_max = sorted(results, key=lambda x: x.max_eval_time, reverse=True)
    for r in sorted_by_max:
        if r.max_eval_time > 300:
            print(f"  {r.task_name}: æœ€å¤§={r.max_eval_time:.1f}s ({r.max_eval_time / 60:.1f}åˆ†é’Ÿ)")


def compare_stats(results1: list[TaskStats], results2: list[TaskStats], title1: str, title2: str):
    """å¯¹æ¯”ä¸¤ä¸ªç›®å½•çš„ç»Ÿè®¡ç»“æœ"""
    print(f"\n{'=' * 80}")
    print(f"  å¯¹æ¯”åˆ†æ: {title1} vs {title2}")
    print(f"{'=' * 80}")

    # åˆ›å»ºæŸ¥æ‰¾å­—å…¸
    dict1 = {r.task_name: r for r in results1}
    dict2 = {r.task_name: r for r in results2}

    # æ€»ä½“å¯¹æ¯”
    total1_retry = sum(r.max_retry_count for r in results1)
    total2_retry = sum(r.max_retry_count for r in results2)
    total1_limiting = sum(r.total_limiting_count for r in results1)
    total2_limiting = sum(r.total_limiting_count for r in results2)
    total1_llm = sum(r.total_llm_calls for r in results1)
    total2_llm = sum(r.total_llm_calls for r in results2)

    print("\nğŸ“Š æ€»ä½“å¯¹æ¯”:")
    print(f"  {'æŒ‡æ ‡':<30} {title1:>15} {title2:>15} {'å·®å¼‚':>10}")
    print(f"  {'-' * 70}")
    print(f"  {'ä»»åŠ¡æ•°':<30} {len(results1):>15} {len(results2):>15}")
    print(
        f"  {'è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°':<30} {total1_retry:>15} {total2_retry:>15} {total1_retry / max(total2_retry, 1):.1f}x"
    )
    print(
        f"  {'æ€»é™æµæ¬¡æ•°':<30} {total1_limiting:>15} {total2_limiting:>15} {total1_limiting / max(total2_limiting, 1):.1f}x"
    )
    print(f"  {'æ€»LLMè°ƒç”¨æ¬¡æ•°':<30} {total1_llm:>15} {total2_llm:>15} {total1_llm / max(total2_llm, 1):.1f}x")

    # ç›¸åŒä»»åŠ¡å¯¹æ¯”
    common_tasks = set(dict1.keys()) & set(dict2.keys())
    print(f"\nâ±ï¸  ç›¸åŒä»»åŠ¡è¯„ä¼°è€—æ—¶å¯¹æ¯” (å…± {len(common_tasks)} ä¸ª):")
    print(f"  {'ä»»åŠ¡å':<50} {title1:>12} {title2:>12} {'å·®å¼‚':>8}")
    print(f"  {'-' * 85}")

    comparisons = []
    for task in common_tasks:
        r1, r2 = dict1[task], dict2[task]
        if r1.avg_eval_time > 0 and r2.avg_eval_time > 0:
            diff = (r1.avg_eval_time - r2.avg_eval_time) / r2.avg_eval_time * 100
            comparisons.append((task, r1.avg_eval_time, r2.avg_eval_time, diff))

    # æŒ‰å·®å¼‚æ’åº
    comparisons.sort(key=lambda x: x[3], reverse=True)
    for task, avg1, avg2, diff in comparisons[:15]:
        sign = "+" if diff > 0 else ""
        print(f"  {task:<50} {avg1:>10.1f}s {avg2:>10.1f}s {sign}{diff:>6.1f}%")


def export_json(results: list[TaskStats], output_path: Path):
    """å¯¼å‡ºç»“æœä¸º JSON"""
    data = []
    for r in results:
        data.append(
            {
                "task_name": r.task_name,
                "total_run_time": r.total_run_time,
                "max_retry_count": r.max_retry_count,
                "total_limiting_count": r.total_limiting_count,
                "total_llm_calls": r.total_llm_calls,
                "eval_count": r.eval_count,
                "total_eval_time": r.total_eval_time,
                "avg_eval_time": r.avg_eval_time,
                "max_eval_time": r.max_eval_time,
                "min_eval_time": r.min_eval_time,
            }
        )

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“ ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æ SE_Perf å®éªŒç»Ÿè®¡ï¼ˆè¿è¡Œæ—¶é—´ã€LLMè°ƒç”¨ã€è¯„ä¼°è€—æ—¶ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("trajectory_dir", type=str, help="è½¨è¿¹ç›®å½•è·¯å¾„")
    parser.add_argument("--compare", type=str, help="å¯¹æ¯”ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--workers", "-w", type=int, default=None, help="å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°)")

    args = parser.parse_args()

    traj_dir = Path(args.trajectory_dir)

    print(f"æ­£åœ¨åˆ†æ: {traj_dir}")
    results = analyze_directory(traj_dir, max_workers=args.workers)

    if not results:
        print("æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡æ•°æ®")
        return 1

    title = traj_dir.name
    print_stats(results, title)

    if args.compare:
        compare_dir = Path(args.compare)
        print(f"\næ­£åœ¨åˆ†æå¯¹æ¯”ç›®å½•: {compare_dir}")
        compare_results = analyze_directory(compare_dir, max_workers=args.workers)
        if compare_results:
            compare_title = compare_dir.name
            print_stats(compare_results, compare_title)
            compare_stats(results, compare_results, title, compare_title)

    if args.output:
        export_json(results, Path(args.output))

    return 0


if __name__ == "__main__":
    sys.exit(main())
