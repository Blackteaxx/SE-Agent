#!/usr/bin/env python3
"""
å®éªŒç»Ÿè®¡åˆ†æè„šæœ¬ (exp_stats.py)

åˆ†æ SE_Perf å®éªŒè¿è¡Œç»Ÿè®¡ï¼ŒåŒ…æ‹¬ï¼š
1. æ¯ä¸ªä»»åŠ¡çš„æ€»è¿è¡Œæ—¶é—´
2. LLM è°ƒç”¨æ¬¡æ•°å’Œè€—æ—¶ç»Ÿè®¡
3. è¯„ä¼°è€—æ—¶ç»Ÿè®¡

ç”¨æ³•:
    python utils/exp_stats.py <trajectory_dir> [--compare <other_dir>]
    
ç¤ºä¾‹:
    python utils/exp_stats.py trajectories_perf/deepseek-v3/Plan-Random-Local-Global-45its_20251218_160428
    
    # å¯¹æ¯”ä¸¤ä¸ªç›®å½•
    python utils/exp_stats.py trajectories_perf/deepseek-v3/Plan-Random-Local-Global-45its_20251218_160428 \
        --compare trajectories_perf/deepseek-v3/Plan-Weighted-45its_20251218_153854
"""

import argparse
import json
import os
import re
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import NamedTuple

# ============================================================================
# æ–°æ ¼å¼ perfagent.log çš„æ­£åˆ™è¡¨è¾¾å¼
# ============================================================================

# æå–è¿­ä»£å·: "[è¿­ä»£ 1 å¼€å§‹]" æˆ– "[è¿­ä»£ 1 å®Œæˆ]"
RE_ITER_START = re.compile(r"\[è¿­ä»£ (\d+) å¼€å§‹\]")
RE_ITER_END = re.compile(r"\[è¿­ä»£ (\d+) å®Œæˆ\]")

# æå–æ€§èƒ½è¯„ä¼°è€—æ—¶: "æ€§èƒ½è¯„ä¼°è€—æ—¶: 14.10s"
RE_EVAL_TIME = re.compile(r"æ€§èƒ½è¯„ä¼°è€—æ—¶[ï¼š:]\s*([\d.]+)s")

# æå– LLM è°ƒç”¨è€—æ—¶: "[LLMè°ƒç”¨å®Œæˆ] è€—æ—¶: 31.80s" æˆ– "LLMè°ƒç”¨è€—æ—¶: 31.80s"
RE_LLM_TIME = re.compile(r"(?:\[LLMè°ƒç”¨å®Œæˆ\]\s*è€—æ—¶[ï¼š:]|LLMè°ƒç”¨è€—æ—¶[ï¼š:])\s*([\d.]+)s")

# æå–è¿­ä»£æ€»è€—æ—¶: "è¿­ä»£æ€»è€—æ—¶: 45.90s"
RE_ITER_TOTAL_TIME = re.compile(r"è¿­ä»£æ€»è€—æ—¶[ï¼š:]\s*([\d.]+)s")

# æå–æ€»è¿è¡Œæ—¶é—´: "æ€»è€—æ—¶: 45.90s" æˆ– "# æ€»è€—æ—¶: 45.90s"
RE_TOTAL_TIME = re.compile(r"æ€»è€—æ—¶[ï¼š:]\s*([\d.]+)s")

# æå–æ‰§è¡Œè¿­ä»£æ•°: "æ‰§è¡Œè¿­ä»£æ•°: 1"
RE_ITER_COUNT = re.compile(r"æ‰§è¡Œè¿­ä»£æ•°[ï¼š:]\s*(\d+)")

# æå–æˆåŠŸæ”¹è¿›è¿­ä»£æ•°: "æˆåŠŸæ”¹è¿›è¿­ä»£æ•°: 1"
RE_SUCCESS_ITER_COUNT = re.compile(r"æˆåŠŸæ”¹è¿›è¿­ä»£æ•°[ï¼š:]\s*(\d+)")

# æå–ä¼˜åŒ–æˆåŠŸæ ‡è®°: "ä¼˜åŒ–æˆåŠŸ: âœ… æ˜¯" æˆ– "ä¼˜åŒ–æˆåŠŸ: âŒ å¦"
RE_OPT_SUCCESS = re.compile(r"ä¼˜åŒ–æˆåŠŸ[ï¼š:]\s*(âœ…|âŒ)")

# æå– pass_rate: "pass_rate: 8.00%"
RE_PASS_RATE = re.compile(r"pass_rate[ï¼š:]\s*([\d.]+)%")

# æå–æœ€ç»ˆ integral: "æœ€ç»ˆ integral: inf MB*s" æˆ– "integral: infMB*s"
RE_FINAL_INTEGRAL = re.compile(r"(?:æœ€ç»ˆ\s*)?integral[ï¼š:]\s*([\d.]+|inf)(?:\s*MB\*s|MB\*s)?")

# æå–æ”¹è¿›å¹…åº¦: "æ”¹è¿›å¹…åº¦: 0.00%"
RE_IMPROVEMENT = re.compile(r"æ”¹è¿›å¹…åº¦[ï¼š:]\s*([-\d.]+)%")

# ============================================================================
# æ—§æ ¼å¼å…¼å®¹ (se_framework.log)
# ============================================================================
RE_MAX_RETRY = re.compile(rb"attempt=10/10")
RE_LIMITING = re.compile(rb"5513-chatGPt\.limiting")
RE_LLM_CALL = re.compile(r"è°ƒç”¨LLM:".encode())
RE_LOG_TIMESTAMP = re.compile(rb"^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})")


class EvalDetail(NamedTuple):
    """å•æ¬¡è¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯"""

    iteration: int  # è¿­ä»£å·
    eval_time: float  # è¯„ä¼°æ—¶é—´ï¼ˆç§’ï¼‰
    llm_time: float  # LLMè°ƒç”¨æ—¶é—´ï¼ˆç§’ï¼‰
    iter_total_time: float  # è¿­ä»£æ€»æ—¶é—´ï¼ˆç§’ï¼‰


class BestIterInfo(NamedTuple):
    """æœ€ä¼˜è¿­ä»£ä¿¡æ¯"""

    best_iteration: int  # è¾¾åˆ°æœ€ä¼˜æ€§èƒ½çš„è¿­ä»£æ¬¡æ•°
    best_performance: float  # æœ€ä¼˜æ€§èƒ½å€¼
    total_iterations: int  # æ€»è¿­ä»£æ¬¡æ•°
    first_valid_iteration: int  # ç¬¬ä¸€æ¬¡æœ‰æ•ˆï¼ˆé Infinityï¼‰çš„è¿­ä»£
    performance_history: tuple[tuple[int, float], ...]  # (iteration, performance) å†å²


class TaskStats(NamedTuple):
    """ä»»åŠ¡ç»Ÿè®¡ç»“æœ"""

    task_name: str
    # ä»»åŠ¡è¿è¡Œæ—¶é—´
    total_run_time: float  # æ€»è¿è¡Œæ—¶é—´ (ç§’)
    # LLM é‡è¯•ç›¸å…³
    max_retry_count: int  # attempt=10/10 çš„æ¬¡æ•°
    total_limiting_count: int  # é™æµæ¬¡æ•°
    total_llm_calls: int  # æ€» LLM è°ƒç”¨æ¬¡æ•°
    # LLM è€—æ—¶ç›¸å…³
    total_llm_time: float  # æ€» LLM è°ƒç”¨æ—¶é—´ (ç§’)
    avg_llm_time: float  # å¹³å‡ LLM è°ƒç”¨æ—¶é—´ (ç§’)
    # è¯„ä¼°è€—æ—¶ç›¸å…³
    eval_count: int  # è¯„ä¼°æ¬¡æ•°
    total_eval_time: float  # æ€»è¯„ä¼°æ—¶é—´ (ç§’)
    avg_eval_time: float  # å¹³å‡è¯„ä¼°æ—¶é—´ (ç§’)
    max_eval_time: float  # æœ€å¤§è¯„ä¼°æ—¶é—´ (ç§’)
    min_eval_time: float  # æœ€å°è¯„ä¼°æ—¶é—´ (ç§’)
    # è¿­ä»£ç›¸å…³
    iter_count: int  # æ‰§è¡Œè¿­ä»£æ•°
    success_iter_count: int  # æˆåŠŸæ”¹è¿›è¿­ä»£æ•°
    # ç»“æœç›¸å…³
    opt_success: bool  # ä¼˜åŒ–æ˜¯å¦æˆåŠŸ
    final_pass_rate: float  # æœ€ç»ˆ pass_rate
    improvement_pct: float  # æ”¹è¿›å¹…åº¦
    # å¼‚å¸¸è¯„ä¼°è¯¦æƒ…ï¼ˆåŒ…å«è¿­ä»£å·ï¼‰
    max_eval_detail: EvalDetail | None  # æœ€å¤§è¯„ä¼°æ—¶é—´çš„è¯¦æƒ…
    eval_details: tuple[EvalDetail, ...]  # æ‰€æœ‰è¯„ä¼°è¯¦æƒ…
    # æœ€ä¼˜è¿­ä»£ä¿¡æ¯
    best_iter_info: BestIterInfo | None  # è¾¾åˆ°æœ€ä¼˜æ€§èƒ½çš„è¿­ä»£ä¿¡æ¯


def parse_log_timestamp(ts_bytes: bytes) -> datetime | None:
    """è§£ææ—¥å¿—æ—¶é—´æˆ³"""
    try:
        ts_str = ts_bytes.decode("utf-8")
        return datetime.strptime(ts_str, "%Y-%m-%d %H:%M:%S,%f")
    except (ValueError, UnicodeDecodeError):
        return None


def analyze_traj_pool(task_dir: Path) -> BestIterInfo | None:
    """
    åˆ†æ traj.pool æ–‡ä»¶ï¼Œæ‰¾åˆ°è¾¾åˆ°æœ€ä¼˜æ€§èƒ½çš„è¿­ä»£æ¬¡æ•°ã€‚

    Returns:
        BestIterInfo | None: æœ€ä¼˜è¿­ä»£ä¿¡æ¯ï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å› None
    """
    traj_pool_path = task_dir / "traj.pool"
    if not traj_pool_path.exists():
        return None

    try:
        with open(traj_pool_path, encoding="utf-8", errors="ignore") as f:
            data = json.load(f)

        if not isinstance(data, dict):
            return None

        # traj.pool çš„ç»“æ„æ˜¯ {"task_name": {"problem": ..., "iter1_sol1": {...}, "iter2_sol1": {...}, ...}}
        task_name = task_dir.name
        task_data = data.get(task_name)
        if not isinstance(task_data, dict):
            # å°è¯•è·å–ç¬¬ä¸€ä¸ªé”®
            for key in data:
                if isinstance(data[key], dict) and "problem" in data[key]:
                    task_data = data[key]
                    break

        if not task_data:
            return None

        # æå–æ‰€æœ‰è¿­ä»£çš„æ€§èƒ½æ•°æ®
        performance_history: list[tuple[int, float]] = []

        for key, value in task_data.items():
            if key == "problem" or not isinstance(value, dict):
                continue

            iteration = value.get("iteration")
            performance = value.get("performance")

            if iteration is None or performance is None:
                continue

            try:
                iter_num = int(iteration)
                # å¤„ç† "Infinity" å­—ç¬¦ä¸²å’Œæ•°å­—
                if isinstance(performance, str):
                    if performance.lower() in ("infinity", "inf"):
                        perf_val = float("inf")
                    else:
                        perf_val = float(performance)
                else:
                    perf_val = float(performance)

                performance_history.append((iter_num, perf_val))
            except (ValueError, TypeError):
                continue

        if not performance_history:
            return None

        # æŒ‰è¿­ä»£å·æ’åº
        performance_history.sort(key=lambda x: x[0])

        # æ‰¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼ˆæœ€å°å€¼ï¼Œæ’é™¤ infï¼‰
        finite_perfs = [(it, p) for it, p in performance_history if p != float("inf")]

        if not finite_perfs:
            # æ‰€æœ‰éƒ½æ˜¯ inf
            return BestIterInfo(
                best_iteration=0,
                best_performance=float("inf"),
                total_iterations=len(performance_history),
                first_valid_iteration=0,
                performance_history=tuple(performance_history),
            )

        # æ‰¾åˆ°ç¬¬ä¸€æ¬¡æœ‰æ•ˆçš„è¿­ä»£
        first_valid_iteration = finite_perfs[0][0]

        # æ‰¾åˆ°æœ€ä¼˜æ€§èƒ½çš„è¿­ä»£ï¼ˆç´¯ç§¯æœ€ä¼˜ï¼‰
        best_perf = float("inf")
        best_iter = 0

        for it, p in performance_history:
            if p < best_perf:
                best_perf = p
                best_iter = it

        return BestIterInfo(
            best_iteration=best_iter,
            best_performance=best_perf,
            total_iterations=len(performance_history),
            first_valid_iteration=first_valid_iteration,
            performance_history=tuple(performance_history),
        )

    except Exception as e:
        print(f"Warning: åˆ†æ {traj_pool_path} å¤±è´¥: {e}", file=sys.stderr)
        return None


def analyze_se_framework_log(log_path: Path) -> dict:
    """åˆ†æ se_framework.log æ–‡ä»¶ï¼ˆä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼ + é¢„ç¼–è¯‘æ­£åˆ™ï¼Œæ›´å¿«ï¼‰"""
    stats = {
        "max_retry_count": 0,
        "total_limiting_count": 0,
        "total_llm_calls": 0,
        "total_run_time": 0.0,
    }

    if not log_path.exists():
        return stats

    try:
        # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–ï¼Œé¿å…ç¼–ç è½¬æ¢å¼€é”€
        with open(log_path, "rb") as f:
            content = f.read()

        # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
        stats["max_retry_count"] = len(RE_MAX_RETRY.findall(content))
        stats["total_limiting_count"] = len(RE_LIMITING.findall(content))
        stats["total_llm_calls"] = len(RE_LLM_CALL.findall(content))

        # æå–å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼Œè®¡ç®—æ€»è¿è¡Œæ—¶é—´
        lines = content.split(b"\n")
        start_time = None
        end_time = None
        end_time_marker = None

        # æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æˆ³
        for line in lines:
            match = RE_LOG_TIMESTAMP.match(line)
            if match:
                start_time = parse_log_timestamp(match.group(1))
                break

        # æ‰¾åˆ°é¦–æ¬¡å‡ºç°"ç”Ÿæˆæœ€ç»ˆç»“æœ final.json"æ‰€åœ¨è¡Œçš„æ—¶é—´æˆ³ä½œä¸ºç»“æŸæ—¶é—´
        for line in lines:
            if b"final.json" in line:
                try:
                    text = line.decode("utf-8", errors="ignore")
                    if "ç”Ÿæˆæœ€ç»ˆç»“æœ final.json" in text:
                        m = RE_LOG_TIMESTAMP.match(line)
                        if m:
                            end_time_marker = parse_log_timestamp(m.group(1))
                            break
                except Exception:
                    continue

        # æ‰¾æœ€åä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æˆ³
        for line in reversed(lines):
            match = RE_LOG_TIMESTAMP.match(line)
            if match:
                end_time = parse_log_timestamp(match.group(1))
                break

        if start_time:
            chosen_end = end_time_marker or end_time
            if chosen_end:
                stats["total_run_time"] = (chosen_end - start_time).total_seconds()

    except Exception as e:
        print(f"Warning: æ— æ³•åˆ†æ {log_path}: {e}", file=sys.stderr)

    return stats


def analyze_perfagent_log_new_format(log_content: str, iteration_num: int) -> dict | None:
    """åˆ†ææ–°æ ¼å¼çš„ perfagent.log å†…å®¹ï¼Œè¿”å›å•æ¬¡è¿­ä»£çš„ç»Ÿè®¡ä¿¡æ¯"""
    result = {
        "iteration": iteration_num,
        "eval_time": 0.0,
        "llm_time": 0.0,
        "iter_total_time": 0.0,
        "total_run_time": 0.0,
        "iter_count": 0,
        "success_iter_count": 0,
        "opt_success": False,
        "final_pass_rate": 0.0,
        "improvement_pct": 0.0,
    }

    # æå–æ€§èƒ½è¯„ä¼°è€—æ—¶ï¼ˆå¯èƒ½æœ‰å¤šä¸ªï¼Œå–æœ€åä¸€ä¸ªæˆ–æ±‚å’Œï¼‰
    eval_times = RE_EVAL_TIME.findall(log_content)
    if eval_times:
        # å–æ‰€æœ‰è¯„ä¼°æ—¶é—´çš„æ€»å’Œï¼ˆä¸€ä¸ªè¿­ä»£å¯èƒ½æœ‰å¤šæ¬¡è¯„ä¼°ï¼‰
        result["eval_time"] = sum(float(t) for t in eval_times)

    # æå– LLM è°ƒç”¨è€—æ—¶ï¼ˆå¯èƒ½æœ‰å¤šä¸ªï¼‰
    llm_times = RE_LLM_TIME.findall(log_content)
    if llm_times:
        result["llm_time"] = sum(float(t) for t in llm_times)

    # æå–è¿­ä»£æ€»è€—æ—¶
    iter_total_match = RE_ITER_TOTAL_TIME.search(log_content)
    if iter_total_match:
        result["iter_total_time"] = float(iter_total_match.group(1))

    # æå–æ€»è¿è¡Œæ—¶é—´ï¼ˆä»æœ€åä¸€ä¸ªåŒ¹é…ï¼‰
    total_time_matches = RE_TOTAL_TIME.findall(log_content)
    if total_time_matches:
        result["total_run_time"] = float(total_time_matches[-1])

    # æå–æ‰§è¡Œè¿­ä»£æ•°
    iter_count_match = RE_ITER_COUNT.search(log_content)
    if iter_count_match:
        result["iter_count"] = int(iter_count_match.group(1))

    # æå–æˆåŠŸæ”¹è¿›è¿­ä»£æ•°
    success_iter_match = RE_SUCCESS_ITER_COUNT.search(log_content)
    if success_iter_match:
        result["success_iter_count"] = int(success_iter_match.group(1))

    # æå–ä¼˜åŒ–æˆåŠŸæ ‡è®°
    opt_success_match = RE_OPT_SUCCESS.search(log_content)
    if opt_success_match:
        result["opt_success"] = opt_success_match.group(1) == "âœ…"

    # æå–æœ€ç»ˆ pass_rate
    pass_rate_matches = RE_PASS_RATE.findall(log_content)
    if pass_rate_matches:
        result["final_pass_rate"] = float(pass_rate_matches[-1])

    # æå–æ”¹è¿›å¹…åº¦
    improvement_match = RE_IMPROVEMENT.search(log_content)
    if improvement_match:
        result["improvement_pct"] = float(improvement_match.group(1))

    return result


# æ—§æ ¼å¼ batch æ—¥å¿—çš„æ—¶é—´æˆ³æ­£åˆ™
RE_BATCH_LOG_TIMESTAMP = re.compile(r"^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})")


def analyze_perfagent_log_batch_format(log_content: str, iteration_num: int) -> dict | None:
    """
    åˆ†ææ—§æ ¼å¼çš„ perfagent.log (batch æ—¥å¿—)ï¼Œä»æ—¶é—´æˆ³æ¨ç®—è¿è¡Œæ—¶é—´ã€‚

    æ—§æ ¼å¼æ—¥å¿—ç‰¹ç‚¹ï¼š
    - æ˜¯ perfagent.run_batch çš„è¾“å‡º
    - ä¸åŒ…å« [LLMè°ƒç”¨å®Œæˆ] è€—æ—¶: ä¿¡æ¯
    - å¯ä»¥ä»æ—¶é—´æˆ³å·®æ¨ç®—å¤§è‡´è¿è¡Œæ—¶é—´
    """
    result = {
        "iteration": iteration_num,
        "eval_time": 0.0,
        "llm_time": 0.0,
        "iter_total_time": 0.0,
        "total_run_time": 0.0,
        "iter_count": 1,  # batch æ ¼å¼é€šå¸¸æ˜¯å•æ¬¡è¿­ä»£
        "success_iter_count": 0,
        "opt_success": False,
        "final_pass_rate": 0.0,
        "improvement_pct": 0.0,
    }

    lines = log_content.strip().split("\n")
    if not lines:
        return None

    # æ£€æŸ¥æ˜¯å¦æ˜¯ batch æ ¼å¼æ—¥å¿—ï¼ˆåŒ…å« "perfagent.run_batch" æˆ– "PerfAgent æ‰¹é‡è¿è¡Œ"ï¼‰
    is_batch_format = any("run_batch" in line or "æ‰¹é‡è¿è¡Œ" in line for line in lines[:10])
    if not is_batch_format:
        return None

    # æå–æ—¶é—´æˆ³
    timestamps: list[datetime] = []
    for line in lines:
        match = RE_BATCH_LOG_TIMESTAMP.match(line)
        if match:
            try:
                ts = datetime.strptime(match.group(1), "%Y-%m-%d %H:%M:%S,%f")
                timestamps.append(ts)
            except ValueError:
                pass

    if len(timestamps) >= 2:
        start_time = timestamps[0]
        end_time = timestamps[-1]
        total_time = (end_time - start_time).total_seconds()

        # è¿™ä¸ªæ—¶é—´ä¸»è¦æ˜¯ LLM è°ƒç”¨æ—¶é—´ï¼ˆè¯„ä¼°æ—¶é—´é€šå¸¸è¾ƒçŸ­ï¼‰
        result["iter_total_time"] = total_time
        result["llm_time"] = total_time  # è¿‘ä¼¼ä¸º LLM æ—¶é—´
        result["total_run_time"] = total_time

    # æ£€æŸ¥æ˜¯å¦ä¼˜åŒ–æˆåŠŸ
    if "ä¼˜åŒ–æˆåŠŸ" in log_content:
        result["opt_success"] = True
        result["success_iter_count"] = 1

    return result


def analyze_perfagent_logs(task_dir: Path) -> dict:
    """åˆ†æ perfagent.log æ–‡ä»¶è·å–è¯„ä¼°è€—æ—¶ï¼ˆæ–°æ ¼å¼ï¼‰"""
    stats = {
        "eval_count": 0,
        "total_eval_time": 0.0,
        "max_eval_time": 0.0,
        "min_eval_time": float("inf"),
        "eval_times": [],
        "eval_details": [],  # å­˜å‚¨ EvalDetail
        "max_eval_detail": None,
        # LLM è€—æ—¶
        "total_llm_time": 0.0,
        "llm_times": [],
        # æ€»è¿è¡Œæ—¶é—´ï¼ˆä» perfagent.log è·å–ï¼Œæ›´å‡†ç¡®ï¼‰
        "total_run_time": 0.0,
        # è¿­ä»£ç»Ÿè®¡
        "iter_count": 0,
        "success_iter_count": 0,
        # ç»“æœç»Ÿè®¡
        "opt_success": False,
        "final_pass_rate": 0.0,
        "improvement_pct": 0.0,
    }

    task_name = task_dir.name
    last_log_stats = None  # ä¿å­˜æœ€åä¸€ä¸ª iteration çš„å®Œæ•´ç»Ÿè®¡

    # æŸ¥æ‰¾æ‰€æœ‰ iteration_*/task_name/perfagent.log æˆ– iteration_*/perfagent.log
    iteration_dirs = sorted(
        task_dir.glob("iteration_*"), key=lambda x: int(x.name.split("_")[1]) if "_" in x.name else 0
    )

    for iteration_dir in iteration_dirs:
        # å°è¯•ä¸¤ç§è·¯å¾„æ ¼å¼ï¼š
        # 1. æ–°æ ¼å¼: iteration_X/task_name/perfagent.log (SE_Perf with Local-Global memory)
        # 2. æ—§æ ¼å¼: iteration_X/perfagent.log (older SE_Perf runs)
        inner_perfagent = iteration_dir / task_name / "perfagent.log"
        if not inner_perfagent.exists():
            inner_perfagent = iteration_dir / "perfagent.log"
        if inner_perfagent.exists():
            try:
                # æå–è¿­ä»£å·
                iter_name = iteration_dir.name
                iteration_num = int(iter_name.split("_")[1]) if "_" in iter_name else 0

                # è¯»å–æ—¥å¿—å†…å®¹
                with open(inner_perfagent, encoding="utf-8", errors="ignore") as f:
                    content = f.read()

                # ä¼˜å…ˆå°è¯•æ–°æ ¼å¼è§£æ
                log_stats = analyze_perfagent_log_new_format(content, iteration_num)

                # å¦‚æœæ–°æ ¼å¼è§£ææ²¡æœ‰ LLM æ—¶é—´ä¿¡æ¯ï¼Œå°è¯•æ—§æ ¼å¼ï¼ˆbatch æ—¥å¿—ï¼‰
                if log_stats and log_stats["llm_time"] == 0 and log_stats["eval_time"] == 0:
                    batch_stats = analyze_perfagent_log_batch_format(content, iteration_num)
                    if batch_stats and batch_stats["llm_time"] > 0:
                        log_stats = batch_stats

                if log_stats:
                    last_log_stats = log_stats

                    eval_time = log_stats["eval_time"]
                    llm_time = log_stats["llm_time"]
                    iter_total_time = log_stats["iter_total_time"]

                    if eval_time > 0:
                        detail = EvalDetail(
                            iteration=iteration_num,
                            eval_time=eval_time,
                            llm_time=llm_time,
                            iter_total_time=iter_total_time,
                        )
                        stats["eval_times"].append(eval_time)
                        stats["eval_details"].append(detail)
                        stats["eval_count"] += 1
                        stats["total_eval_time"] += eval_time

                        if eval_time > stats["max_eval_time"]:
                            stats["max_eval_time"] = eval_time
                            stats["max_eval_detail"] = detail

                        stats["min_eval_time"] = min(stats["min_eval_time"], eval_time)

                    if llm_time > 0:
                        stats["llm_times"].append(llm_time)
                        stats["total_llm_time"] += llm_time

            except Exception as e:
                print(f"Warning: åˆ†æ {inner_perfagent} å¤±è´¥: {e}", file=sys.stderr)

    # ä»æœ€åä¸€ä¸ª iteration çš„æ—¥å¿—è·å–æ±‡æ€»ä¿¡æ¯
    if last_log_stats:
        stats["total_run_time"] = last_log_stats.get("total_run_time", 0.0)
        stats["iter_count"] = last_log_stats.get("iter_count", 0)
        stats["success_iter_count"] = last_log_stats.get("success_iter_count", 0)
        stats["opt_success"] = last_log_stats.get("opt_success", False)
        stats["final_pass_rate"] = last_log_stats.get("final_pass_rate", 0.0)
        stats["improvement_pct"] = last_log_stats.get("improvement_pct", 0.0)

    if stats["min_eval_time"] == float("inf"):
        stats["min_eval_time"] = 0.0

    return stats


def analyze_single_task(task_dir: Path) -> TaskStats:
    """åˆ†æå•ä¸ªä»»åŠ¡ç›®å½•ï¼ˆä¾›å¤šè¿›ç¨‹è°ƒç”¨ï¼‰"""
    task_name = task_dir.name

    # åˆ†æ se_framework.log
    se_log = task_dir / "se_framework.log"
    se_stats = analyze_se_framework_log(se_log)

    # åˆ†æ perfagent.logï¼ˆæ–°æ ¼å¼ï¼‰
    eval_stats = analyze_perfagent_logs(task_dir)

    # åˆ†æ traj.poolï¼ˆæœ€ä¼˜è¿­ä»£ä¿¡æ¯ï¼‰
    best_iter_info = analyze_traj_pool(task_dir)

    # è®¡ç®—å¹³å‡å€¼
    avg_eval_time = 0.0
    if eval_stats["eval_count"] > 0:
        avg_eval_time = eval_stats["total_eval_time"] / eval_stats["eval_count"]

    avg_llm_time = 0.0
    llm_call_count = len(eval_stats["llm_times"])
    if llm_call_count > 0:
        avg_llm_time = eval_stats["total_llm_time"] / llm_call_count

    # ä¼˜å…ˆä½¿ç”¨ perfagent.log ä¸­çš„æ€»è¿è¡Œæ—¶é—´ï¼ˆæ›´å‡†ç¡®ï¼‰
    total_run_time = eval_stats["total_run_time"]
    if total_run_time <= 0:
        total_run_time = se_stats["total_run_time"]

    return TaskStats(
        task_name=task_name,
        total_run_time=total_run_time,
        max_retry_count=se_stats["max_retry_count"],
        total_limiting_count=se_stats["total_limiting_count"],
        total_llm_calls=se_stats["total_llm_calls"] or llm_call_count,
        total_llm_time=eval_stats["total_llm_time"],
        avg_llm_time=avg_llm_time,
        eval_count=eval_stats["eval_count"],
        total_eval_time=eval_stats["total_eval_time"],
        avg_eval_time=avg_eval_time,
        max_eval_time=eval_stats["max_eval_time"],
        min_eval_time=eval_stats["min_eval_time"],
        iter_count=eval_stats["iter_count"],
        success_iter_count=eval_stats["success_iter_count"],
        opt_success=eval_stats["opt_success"],
        final_pass_rate=eval_stats["final_pass_rate"],
        improvement_pct=eval_stats["improvement_pct"],
        max_eval_detail=eval_stats["max_eval_detail"],
        eval_details=tuple(eval_stats["eval_details"]),
        best_iter_info=best_iter_info,
    )


def analyze_directory(traj_dir: Path, max_workers: int | None = None) -> list[TaskStats]:
    """åˆ†ææ•´ä¸ªè½¨è¿¹ç›®å½•ï¼ˆä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡ŒåŠ é€Ÿï¼‰"""
    results = []

    if not traj_dir.exists():
        print(f"Error: ç›®å½•ä¸å­˜åœ¨: {traj_dir}", file=sys.stderr)
        return results

    # éå†æ‰€æœ‰ä»»åŠ¡ç›®å½•
    task_dirs = [d for d in traj_dir.iterdir() if d.is_dir() and not d.name.startswith(".")]

    if not task_dirs:
        return results

    # é»˜è®¤ä½¿ç”¨ CPU æ ¸å¿ƒæ•°
    if max_workers is None:
        max_workers = min(os.cpu_count() or 4, len(task_dirs))

    print(f"  ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œåˆ†æ {len(task_dirs)} ä¸ªä»»åŠ¡...")

    # ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(analyze_single_task, task_dir): task_dir for task_dir in task_dirs}

        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                task_dir = futures[future]
                print(f"Warning: åˆ†æä»»åŠ¡ {task_dir.name} å¤±è´¥: {e}", file=sys.stderr)

    # æŒ‰ä»»åŠ¡åæ’åº
    results.sort(key=lambda x: x.task_name)

    return results


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´ï¼Œæ˜¾ç¤ºä¸º å°æ—¶:åˆ†é’Ÿ:ç§’"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    if hours > 0:
        return f"{hours}h {minutes}m {secs:.0f}s"
    elif minutes > 0:
        return f"{minutes}m {secs:.0f}s"
    else:
        return f"{secs:.1f}s"


def print_best_iteration_stats(results: list[TaskStats]):
    """æ‰“å°æœ€ä¼˜è¿­ä»£æ¬¡æ•°ç»Ÿè®¡ï¼ˆç”¨äºè¿­ä»£é¢„ç®—é€‰å–ï¼‰"""
    # æ”¶é›†æœ‰æ•ˆçš„æœ€ä¼˜è¿­ä»£æ•°æ®
    best_iters: list[int] = []
    first_valid_iters: list[int] = []
    tasks_with_best_info = 0
    tasks_never_valid = 0  # ä»æœªè¾¾åˆ°æœ‰æ•ˆæ€§èƒ½çš„ä»»åŠ¡

    for r in results:
        if r.best_iter_info is None:
            continue
        tasks_with_best_info += 1

        if r.best_iter_info.best_performance == float("inf"):
            tasks_never_valid += 1
        else:
            best_iters.append(r.best_iter_info.best_iteration)
            if r.best_iter_info.first_valid_iteration > 0:
                first_valid_iters.append(r.best_iter_info.first_valid_iteration)

    if not best_iters:
        print("\nğŸ¯ æœ€ä¼˜è¿­ä»£æ¬¡æ•°ç»Ÿè®¡:")
        print("  (æ— æœ‰æ•ˆæ•°æ®)")
        return

    # è®¡ç®—ç»Ÿè®¡é‡
    best_iters.sort()
    n = len(best_iters)

    avg_best = sum(best_iters) / n
    median_best = best_iters[n // 2] if n % 2 == 1 else (best_iters[n // 2 - 1] + best_iters[n // 2]) / 2
    min_best = min(best_iters)
    max_best = max(best_iters)

    # è®¡ç®—åˆ†ä½æ•°
    def percentile(data: list[int], p: float) -> float:
        k = (len(data) - 1) * p / 100
        f = int(k)
        c = f + 1 if f + 1 < len(data) else f
        return data[f] + (k - f) * (data[c] - data[f])

    p25 = percentile(best_iters, 25)
    p50 = percentile(best_iters, 50)
    p75 = percentile(best_iters, 75)
    p90 = percentile(best_iters, 90)
    p95 = percentile(best_iters, 95)

    print("\nğŸ¯ æœ€ä¼˜è¿­ä»£æ¬¡æ•°ç»Ÿè®¡ï¼ˆç”¨äºè¿­ä»£é¢„ç®—é€‰å–ï¼‰:")
    print(f"  - æœ‰æ•ˆä»»åŠ¡æ•°: {n}/{tasks_with_best_info} (ä»æœªè¾¾åˆ°æœ‰æ•ˆæ€§èƒ½: {tasks_never_valid})")
    print(f"  - å¹³å‡è¾¾åˆ°æœ€ä¼˜çš„è¿­ä»£æ¬¡æ•°: {avg_best:.1f}")
    print(f"  - ä¸­ä½æ•°: {median_best:.1f}")
    print(f"  - èŒƒå›´: {min_best} ~ {max_best}")
    print("  - åˆ†ä½æ•°:")
    print(f"      25%: {p25:.0f} æ¬¡è¿­ä»£")
    print(f"      50%: {p50:.0f} æ¬¡è¿­ä»£")
    print(f"      75%: {p75:.0f} æ¬¡è¿­ä»£")
    print(f"      90%: {p90:.0f} æ¬¡è¿­ä»£")
    print(f"      95%: {p95:.0f} æ¬¡è¿­ä»£")

    # è¿­ä»£æ¬¡æ•°åˆ†å¸ƒç›´æ–¹å›¾
    print("\nğŸ“Š è¾¾åˆ°æœ€ä¼˜æ€§èƒ½çš„è¿­ä»£æ¬¡æ•°åˆ†å¸ƒ:")
    # å®šä¹‰åŒºé—´
    bins = [(1, 5), (6, 10), (11, 15), (16, 20), (21, 25), (26, 30), (31, 35), (36, 40), (41, 45), (46, 50)]
    bin_counts: dict[str, int] = {}
    for low, high in bins:
        count = sum(1 for x in best_iters if low <= x <= high)
        if count > 0 or low <= 20:  # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„åŒºé—´æˆ–å‰å‡ ä¸ªåŒºé—´
            bin_counts[f"{low}-{high}"] = count

    # è¶…è¿‡ 50 çš„
    over_50 = sum(1 for x in best_iters if x > 50)
    if over_50 > 0:
        bin_counts[">50"] = over_50

    max_count = max(bin_counts.values()) if bin_counts else 1
    for bin_name, count in bin_counts.items():
        bar_len = int(count / max_count * 30)
        bar = "â–ˆ" * bar_len
        pct = count / n * 100
        print(f"  {bin_name:>6} æ¬¡: {bar:<30} {count:>3} ä¸ª ({pct:.1f}%)")

    # ç´¯ç§¯åˆ†å¸ƒ
    print("\nğŸ“ˆ ç´¯ç§¯åˆ†å¸ƒï¼ˆè¿­ä»£é¢„ç®—å»ºè®®ï¼‰:")
    cumulative_targets = [5, 10, 15, 20, 25, 30, 35, 40, 45]
    for target in cumulative_targets:
        count = sum(1 for x in best_iters if x <= target)
        pct = count / n * 100
        print(f"  â‰¤{target:>2} æ¬¡è¿­ä»£: {count:>3}/{n} ä»»åŠ¡è¾¾åˆ°æœ€ä¼˜ ({pct:.1f}%)")

    # ç¬¬ä¸€æ¬¡æœ‰æ•ˆè¿­ä»£ç»Ÿè®¡
    if first_valid_iters:
        first_valid_iters.sort()
        avg_first = sum(first_valid_iters) / len(first_valid_iters)
        median_first = first_valid_iters[len(first_valid_iters) // 2]
        print("\nğŸ“ ç¬¬ä¸€æ¬¡è¾¾åˆ°æœ‰æ•ˆæ€§èƒ½ï¼ˆé Infinityï¼‰çš„è¿­ä»£:")
        print(f"  - å¹³å‡: {avg_first:.1f} æ¬¡")
        print(f"  - ä¸­ä½æ•°: {median_first} æ¬¡")
        print(f"  - èŒƒå›´: {min(first_valid_iters)} ~ {max(first_valid_iters)}")


def print_stats(results: list[TaskStats], title: str):
    """æ‰“å°ç»Ÿè®¡ç»“æœ"""
    print(f"\n{'=' * 80}")
    print(f"  {title}")
    print(f"{'=' * 80}")

    # è¿è¡Œæ—¶é—´ç»Ÿè®¡
    run_times = [r.total_run_time for r in results if r.total_run_time > 0]
    total_run_time = sum(run_times)
    avg_run_time = total_run_time / len(run_times) if run_times else 0
    max_run_time = max(run_times) if run_times else 0
    min_run_time = min(run_times) if run_times else 0

    # LLM è€—æ—¶ç»Ÿè®¡
    total_llm_time = sum(r.total_llm_time for r in results)
    total_eval_time = sum(r.total_eval_time for r in results)

    # æ€»ä½“ç»Ÿè®¡
    total_max_retry = sum(r.max_retry_count for r in results)
    total_limiting = sum(r.total_limiting_count for r in results)
    total_llm_calls = sum(r.total_llm_calls for r in results)
    tasks_with_retry = sum(1 for r in results if r.max_retry_count > 0)

    # ä¼˜åŒ–ç»“æœç»Ÿè®¡
    success_count = sum(1 for r in results if r.opt_success)
    total_iter_count = sum(r.iter_count for r in results)
    total_success_iter = sum(r.success_iter_count for r in results)

    print("\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  - ä»»åŠ¡æ€»æ•°: {len(results)}")
    print(f"  - æ€»è¿è¡Œæ—¶é—´: {format_duration(total_run_time)} (å¹³å‡: {format_duration(avg_run_time)})")
    print(f"  - è¿è¡Œæ—¶é—´èŒƒå›´: {format_duration(min_run_time)} ~ {format_duration(max_run_time)}")
    print(
        f"  - æ€» LLM è°ƒç”¨æ—¶é—´: {format_duration(total_llm_time)} ({total_llm_time / max(total_run_time, 1) * 100:.1f}%)"
    )
    print(f"  - æ€»è¯„ä¼°æ—¶é—´: {format_duration(total_eval_time)} ({total_eval_time / max(total_run_time, 1) * 100:.1f}%)")
    print(f"  - æœ‰æœ€å¤§é‡è¯•çš„ä»»åŠ¡æ•°: {tasks_with_retry}")
    print(f"  - æ€»è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° (attempt=10/10): {total_max_retry}")
    print(f"  - æ€»é™æµæ¬¡æ•°: {total_limiting}")
    print(f"  - æ€» LLM è°ƒç”¨æ¬¡æ•°: {total_llm_calls}")

    print("\nğŸ“ˆ ä¼˜åŒ–ç»“æœç»Ÿè®¡:")
    print(f"  - ä¼˜åŒ–æˆåŠŸä»»åŠ¡æ•°: {success_count}/{len(results)} ({success_count / max(len(results), 1) * 100:.1f}%)")
    print(f"  - æ€»æ‰§è¡Œè¿­ä»£æ•°: {total_iter_count}")
    print(f"  - æ€»æˆåŠŸæ”¹è¿›è¿­ä»£æ•°: {total_success_iter}")

    # è¿è¡Œæ—¶é—´ TOP 20
    print("\nğŸ• è¿è¡Œæ—¶é—´ TOP 20:")
    sorted_by_runtime = sorted(results, key=lambda x: x.total_run_time, reverse=True)[:20]
    for r in sorted_by_runtime:
        if r.total_run_time > 0:
            print(f"  {r.task_name}: {format_duration(r.total_run_time)}")

    # LLM æœ€å¤§é‡è¯• TOP 20
    print("\nğŸ”´ LLM è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° TOP 20 (attempt=10/10):")
    sorted_by_retry = sorted(results, key=lambda x: x.max_retry_count, reverse=True)[:20]
    for r in sorted_by_retry:
        if r.max_retry_count > 0:
            print(f"  {r.task_name}: {r.max_retry_count} æ¬¡")

    # è¯„ä¼°è€—æ—¶ TOP 20 (æŒ‰å¹³å‡è€—æ—¶)
    print("\nâ±ï¸  è¯„ä¼°è€—æ—¶ TOP 20 (æŒ‰å¹³å‡è€—æ—¶æ’åº):")
    sorted_by_avg = sorted(results, key=lambda x: x.avg_eval_time, reverse=True)[:20]
    for r in sorted_by_avg:
        if r.eval_count > 0:
            print(f"  {r.task_name}: æ¬¡æ•°={r.eval_count}, å¹³å‡={r.avg_eval_time:.1f}s, æœ€å¤§={r.max_eval_time:.1f}s")

    # LLM è€—æ—¶ TOP 20
    print("\nğŸ¤– LLM è°ƒç”¨è€—æ—¶ TOP 20 (æŒ‰æ€»è€—æ—¶æ’åº):")
    sorted_by_llm = sorted(results, key=lambda x: x.total_llm_time, reverse=True)[:20]
    for r in sorted_by_llm:
        if r.total_llm_time > 0:
            print(f"  {r.task_name}: æ€»è®¡={format_duration(r.total_llm_time)}, å¹³å‡={r.avg_llm_time:.1f}s")

    # å¼‚å¸¸æƒ…å†µ (æœ€å¤§è¯„ä¼°æ—¶é—´ > 300s)
    print("\nâš ï¸  å¼‚å¸¸è¯„ä¼°è€—æ—¶ (å•æ¬¡ > 300s):")
    sorted_by_max = sorted(results, key=lambda x: x.max_eval_time, reverse=True)
    found_anomaly = False
    for r in sorted_by_max:
        if r.max_eval_time > 300:
            found_anomaly = True
            iter_info = f"iter_{r.max_eval_detail.iteration}" if r.max_eval_detail else "?"
            print(f"  {r.task_name} [{iter_info}]: æœ€å¤§={r.max_eval_time:.1f}s ({r.max_eval_time / 60:.1f}åˆ†é’Ÿ)")
    if not found_anomaly:
        print("  (æ— å¼‚å¸¸)")

    # Pass Rate ç»Ÿè®¡
    print("\nğŸ“‹ Pass Rate åˆ†å¸ƒ:")
    pass_rate_bins = {"0%": 0, "1-50%": 0, "51-99%": 0, "100%": 0}
    for r in results:
        if r.final_pass_rate == 0:
            pass_rate_bins["0%"] += 1
        elif r.final_pass_rate == 100:
            pass_rate_bins["100%"] += 1
        elif r.final_pass_rate <= 50:
            pass_rate_bins["1-50%"] += 1
        else:
            pass_rate_bins["51-99%"] += 1
    for bin_name, count in pass_rate_bins.items():
        pct = count / max(len(results), 1) * 100
        print(f"  {bin_name}: {count} ä¸ªä»»åŠ¡ ({pct:.1f}%)")

    # æœ€ä¼˜è¿­ä»£æ¬¡æ•°ç»Ÿè®¡
    print_best_iteration_stats(results)


def compare_stats(results1: list[TaskStats], results2: list[TaskStats], title1: str, title2: str):
    """å¯¹æ¯”ä¸¤ä¸ªç›®å½•çš„ç»Ÿè®¡ç»“æœ"""
    print(f"\n{'=' * 80}")
    print(f"  å¯¹æ¯”åˆ†æ: {title1} vs {title2}")
    print(f"{'=' * 80}")

    # åˆ›å»ºæŸ¥æ‰¾å­—å…¸
    dict1 = {r.task_name: r for r in results1}
    dict2 = {r.task_name: r for r in results2}

    # æ€»ä½“å¯¹æ¯”
    total1_retry = sum(r.max_retry_count for r in results1)
    total2_retry = sum(r.max_retry_count for r in results2)
    total1_limiting = sum(r.total_limiting_count for r in results1)
    total2_limiting = sum(r.total_limiting_count for r in results2)
    total1_llm = sum(r.total_llm_calls for r in results1)
    total2_llm = sum(r.total_llm_calls for r in results2)
    total1_llm_time = sum(r.total_llm_time for r in results1)
    total2_llm_time = sum(r.total_llm_time for r in results2)

    print("\nğŸ“Š æ€»ä½“å¯¹æ¯”:")
    print(f"  {'æŒ‡æ ‡':<30} {title1:>15} {title2:>15} {'å·®å¼‚':>10}")
    print(f"  {'-' * 70}")
    print(f"  {'ä»»åŠ¡æ•°':<30} {len(results1):>15} {len(results2):>15}")
    print(
        f"  {'è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°':<30} {total1_retry:>15} {total2_retry:>15} {total1_retry / max(total2_retry, 1):.1f}x"
    )
    print(
        f"  {'æ€»é™æµæ¬¡æ•°':<30} {total1_limiting:>15} {total2_limiting:>15} {total1_limiting / max(total2_limiting, 1):.1f}x"
    )
    print(f"  {'æ€»LLMè°ƒç”¨æ¬¡æ•°':<30} {total1_llm:>15} {total2_llm:>15} {total1_llm / max(total2_llm, 1):.1f}x")
    print(
        f"  {'æ€»LLMè°ƒç”¨æ—¶é—´(s)':<30} {total1_llm_time:>15.0f} {total2_llm_time:>15.0f} {total1_llm_time / max(total2_llm_time, 1):.1f}x"
    )

    # ç›¸åŒä»»åŠ¡å¯¹æ¯”
    common_tasks = set(dict1.keys()) & set(dict2.keys())
    print(f"\nâ±ï¸  ç›¸åŒä»»åŠ¡è¯„ä¼°è€—æ—¶å¯¹æ¯” (å…± {len(common_tasks)} ä¸ª):")
    print(f"  {'ä»»åŠ¡å':<50} {title1:>12} {title2:>12} {'å·®å¼‚':>8}")
    print(f"  {'-' * 85}")

    comparisons = []
    for task in common_tasks:
        r1, r2 = dict1[task], dict2[task]
        if r1.avg_eval_time > 0 and r2.avg_eval_time > 0:
            diff = (r1.avg_eval_time - r2.avg_eval_time) / r2.avg_eval_time * 100
            comparisons.append((task, r1.avg_eval_time, r2.avg_eval_time, diff))

    # æŒ‰å·®å¼‚æ’åº
    comparisons.sort(key=lambda x: x[3], reverse=True)
    for task, avg1, avg2, diff in comparisons[:15]:
        sign = "+" if diff > 0 else ""
        print(f"  {task:<50} {avg1:>10.1f}s {avg2:>10.1f}s {sign}{diff:>6.1f}%")


def export_json(results: list[TaskStats], output_path: Path):
    """å¯¼å‡ºç»“æœä¸º JSON"""
    data = []
    for r in results:
        data.append(
            {
                "task_name": r.task_name,
                "total_run_time": r.total_run_time,
                "max_retry_count": r.max_retry_count,
                "total_limiting_count": r.total_limiting_count,
                "total_llm_calls": r.total_llm_calls,
                "total_llm_time": r.total_llm_time,
                "avg_llm_time": r.avg_llm_time,
                "eval_count": r.eval_count,
                "total_eval_time": r.total_eval_time,
                "avg_eval_time": r.avg_eval_time,
                "max_eval_time": r.max_eval_time,
                "min_eval_time": r.min_eval_time,
                "iter_count": r.iter_count,
                "success_iter_count": r.success_iter_count,
                "opt_success": r.opt_success,
                "final_pass_rate": r.final_pass_rate,
                "improvement_pct": r.improvement_pct,
            }
        )

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“ ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æ SE_Perf å®éªŒç»Ÿè®¡ï¼ˆè¿è¡Œæ—¶é—´ã€LLMè°ƒç”¨ã€è¯„ä¼°è€—æ—¶ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("trajectory_dir", type=str, help="è½¨è¿¹ç›®å½•è·¯å¾„")
    parser.add_argument("--compare", type=str, help="å¯¹æ¯”ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--workers", "-w", type=int, default=None, help="å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°)")

    args = parser.parse_args()

    traj_dir = Path(args.trajectory_dir)

    print(f"æ­£åœ¨åˆ†æ: {traj_dir}")
    results = analyze_directory(traj_dir, max_workers=args.workers)

    if not results:
        print("æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡æ•°æ®")
        return 1

    title = traj_dir.name
    print_stats(results, title)

    if args.compare:
        compare_dir = Path(args.compare)
        print(f"\næ­£åœ¨åˆ†æå¯¹æ¯”ç›®å½•: {compare_dir}")
        compare_results = analyze_directory(compare_dir, max_workers=args.workers)
        if compare_results:
            compare_title = compare_dir.name
            print_stats(compare_results, compare_title)
            compare_stats(results, compare_results, title, compare_title)

    if args.output:
        export_json(results, Path(args.output))

    return 0


if __name__ == "__main__":
    sys.exit(main())
