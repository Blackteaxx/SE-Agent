#!/usr/bin/env python3
"""
ä»»åŠ¡æ€§èƒ½åˆ†æè„šæœ¬

åˆ†æ SE_Perf è¿è¡Œçš„ä»»åŠ¡æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. LLM è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° (attempt=10/10) çš„ç»Ÿè®¡
2. æ¯ä¸ªä»»åŠ¡çš„è¯„ä¼°è€—æ—¶ç»Ÿè®¡

ç”¨æ³•:
    python utils/analyze_task_performance.py <trajectory_dir> [--compare <other_dir>]
    
ç¤ºä¾‹:
    python utils/analyze_task_performance.py trajectories_perf/deepseek-v3/Plan-Random-Local-Global-45its_20251218_160428
    
    # å¯¹æ¯”ä¸¤ä¸ªç›®å½•
    python utils/analyze_task_performance.py trajectories_perf/deepseek-v3/Plan-Random-Local-Global-45its_20251218_160428 \
        --compare trajectories_perf/deepseek-v3/Plan-Weighted-45its_20251218_153854
"""

import argparse
import json
import re
import sys
from pathlib import Path
from typing import NamedTuple


class TaskStats(NamedTuple):
    """ä»»åŠ¡ç»Ÿè®¡ç»“æœ"""

    task_name: str
    # LLM é‡è¯•ç›¸å…³
    max_retry_count: int  # attempt=10/10 çš„æ¬¡æ•°
    total_limiting_count: int  # é™æµæ¬¡æ•°
    total_llm_calls: int  # æ€» LLM è°ƒç”¨æ¬¡æ•°
    # è¯„ä¼°è€—æ—¶ç›¸å…³
    eval_count: int  # è¯„ä¼°æ¬¡æ•°
    total_eval_time: float  # æ€»è¯„ä¼°æ—¶é—´ (ç§’)
    avg_eval_time: float  # å¹³å‡è¯„ä¼°æ—¶é—´ (ç§’)
    max_eval_time: float  # æœ€å¤§è¯„ä¼°æ—¶é—´ (ç§’)
    min_eval_time: float  # æœ€å°è¯„ä¼°æ—¶é—´ (ç§’)


def analyze_se_framework_log(log_path: Path) -> dict:
    """åˆ†æ se_framework.log æ–‡ä»¶"""
    stats = {
        "max_retry_count": 0,
        "total_limiting_count": 0,
        "total_llm_calls": 0,
    }

    if not log_path.exists():
        return stats

    try:
        with open(log_path, encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # ç»Ÿè®¡ attempt=10/10 (è¾¾åˆ°æœ€å¤§é‡è¯•)
        stats["max_retry_count"] = len(re.findall(r"attempt=10/10", content))

        # ç»Ÿè®¡é™æµæ¬¡æ•°
        stats["total_limiting_count"] = len(re.findall(r"5513-chatGPt\.limiting", content))

        # ç»Ÿè®¡ LLM è°ƒç”¨æ¬¡æ•°
        stats["total_llm_calls"] = len(re.findall(r"è°ƒç”¨LLM:", content))

    except Exception as e:
        print(f"Warning: æ— æ³•åˆ†æ {log_path}: {e}", file=sys.stderr)

    return stats


def analyze_perfagent_logs(task_dir: Path) -> dict:
    """åˆ†æ perfagent.log æ–‡ä»¶è·å–è¯„ä¼°è€—æ—¶"""
    stats = {
        "eval_count": 0,
        "total_eval_time": 0.0,
        "max_eval_time": 0.0,
        "min_eval_time": float("inf"),
        "eval_times": [],
    }

    task_name = task_dir.name

    # æŸ¥æ‰¾æ‰€æœ‰ iteration_*/task_name/perfagent.log
    for iteration_dir in task_dir.glob("iteration_*"):
        inner_perfagent = iteration_dir / task_name / "perfagent.log"
        if inner_perfagent.exists():
            try:
                with open(inner_perfagent, encoding="utf-8", errors="ignore") as f:
                    content = f.read()

                # æå– "è€—æ—¶ XXXs" æ ¼å¼çš„æ—¶é—´
                times = re.findall(r"è€—æ—¶ ([\d.]+)s", content)
                for t in times:
                    try:
                        time_val = float(t)
                        stats["eval_times"].append(time_val)
                        stats["eval_count"] += 1
                        stats["total_eval_time"] += time_val
                        stats["max_eval_time"] = max(stats["max_eval_time"], time_val)
                        stats["min_eval_time"] = min(stats["min_eval_time"], time_val)
                    except ValueError:
                        pass
            except Exception:
                pass

    if stats["min_eval_time"] == float("inf"):
        stats["min_eval_time"] = 0.0

    return stats


def analyze_directory(traj_dir: Path) -> list[TaskStats]:
    """åˆ†ææ•´ä¸ªè½¨è¿¹ç›®å½•"""
    results = []

    if not traj_dir.exists():
        print(f"Error: ç›®å½•ä¸å­˜åœ¨: {traj_dir}", file=sys.stderr)
        return results

    # éå†æ‰€æœ‰ä»»åŠ¡ç›®å½•
    task_dirs = [d for d in traj_dir.iterdir() if d.is_dir() and not d.name.startswith(".")]

    for task_dir in sorted(task_dirs):
        task_name = task_dir.name

        # åˆ†æ se_framework.log
        se_log = task_dir / "se_framework.log"
        se_stats = analyze_se_framework_log(se_log)

        # åˆ†æ perfagent.log
        eval_stats = analyze_perfagent_logs(task_dir)

        # è®¡ç®—å¹³å‡å€¼
        avg_eval_time = 0.0
        if eval_stats["eval_count"] > 0:
            avg_eval_time = eval_stats["total_eval_time"] / eval_stats["eval_count"]

        results.append(
            TaskStats(
                task_name=task_name,
                max_retry_count=se_stats["max_retry_count"],
                total_limiting_count=se_stats["total_limiting_count"],
                total_llm_calls=se_stats["total_llm_calls"],
                eval_count=eval_stats["eval_count"],
                total_eval_time=eval_stats["total_eval_time"],
                avg_eval_time=avg_eval_time,
                max_eval_time=eval_stats["max_eval_time"],
                min_eval_time=eval_stats["min_eval_time"],
            )
        )

    return results


def print_stats(results: list[TaskStats], title: str):
    """æ‰“å°ç»Ÿè®¡ç»“æœ"""
    print(f"\n{'=' * 80}")
    print(f"  {title}")
    print(f"{'=' * 80}")

    # æ€»ä½“ç»Ÿè®¡
    total_max_retry = sum(r.max_retry_count for r in results)
    total_limiting = sum(r.total_limiting_count for r in results)
    total_llm_calls = sum(r.total_llm_calls for r in results)
    tasks_with_retry = sum(1 for r in results if r.max_retry_count > 0)

    print("\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  - ä»»åŠ¡æ€»æ•°: {len(results)}")
    print(f"  - æœ‰æœ€å¤§é‡è¯•çš„ä»»åŠ¡æ•°: {tasks_with_retry}")
    print(f"  - æ€»è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° (attempt=10/10): {total_max_retry}")
    print(f"  - æ€»é™æµæ¬¡æ•°: {total_limiting}")
    print(f"  - æ€» LLM è°ƒç”¨æ¬¡æ•°: {total_llm_calls}")

    # LLM æœ€å¤§é‡è¯• TOP 20
    print("\nğŸ”´ LLM è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° TOP 20 (attempt=10/10):")
    sorted_by_retry = sorted(results, key=lambda x: x.max_retry_count, reverse=True)[:20]
    for r in sorted_by_retry:
        if r.max_retry_count > 0:
            print(f"  {r.task_name}: {r.max_retry_count} æ¬¡")

    # è¯„ä¼°è€—æ—¶ TOP 20 (æŒ‰å¹³å‡è€—æ—¶)
    print("\nâ±ï¸  è¯„ä¼°è€—æ—¶ TOP 20 (æŒ‰å¹³å‡è€—æ—¶æ’åº):")
    sorted_by_avg = sorted(results, key=lambda x: x.avg_eval_time, reverse=True)[:20]
    for r in sorted_by_avg:
        if r.eval_count > 0:
            print(f"  {r.task_name}: æ¬¡æ•°={r.eval_count}, å¹³å‡={r.avg_eval_time:.1f}s, æœ€å¤§={r.max_eval_time:.1f}s")

    # å¼‚å¸¸æƒ…å†µ (æœ€å¤§è¯„ä¼°æ—¶é—´ > 300s)
    print("\nâš ï¸  å¼‚å¸¸è¯„ä¼°è€—æ—¶ (å•æ¬¡ > 300s):")
    sorted_by_max = sorted(results, key=lambda x: x.max_eval_time, reverse=True)
    for r in sorted_by_max:
        if r.max_eval_time > 300:
            print(f"  {r.task_name}: æœ€å¤§={r.max_eval_time:.1f}s ({r.max_eval_time / 60:.1f}åˆ†é’Ÿ)")


def compare_stats(results1: list[TaskStats], results2: list[TaskStats], title1: str, title2: str):
    """å¯¹æ¯”ä¸¤ä¸ªç›®å½•çš„ç»Ÿè®¡ç»“æœ"""
    print(f"\n{'=' * 80}")
    print(f"  å¯¹æ¯”åˆ†æ: {title1} vs {title2}")
    print(f"{'=' * 80}")

    # åˆ›å»ºæŸ¥æ‰¾å­—å…¸
    dict1 = {r.task_name: r for r in results1}
    dict2 = {r.task_name: r for r in results2}

    # æ€»ä½“å¯¹æ¯”
    total1_retry = sum(r.max_retry_count for r in results1)
    total2_retry = sum(r.max_retry_count for r in results2)
    total1_limiting = sum(r.total_limiting_count for r in results1)
    total2_limiting = sum(r.total_limiting_count for r in results2)
    total1_llm = sum(r.total_llm_calls for r in results1)
    total2_llm = sum(r.total_llm_calls for r in results2)

    print("\nğŸ“Š æ€»ä½“å¯¹æ¯”:")
    print(f"  {'æŒ‡æ ‡':<30} {title1:>15} {title2:>15} {'å·®å¼‚':>10}")
    print(f"  {'-' * 70}")
    print(f"  {'ä»»åŠ¡æ•°':<30} {len(results1):>15} {len(results2):>15}")
    print(
        f"  {'è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°':<30} {total1_retry:>15} {total2_retry:>15} {total1_retry / max(total2_retry, 1):.1f}x"
    )
    print(
        f"  {'æ€»é™æµæ¬¡æ•°':<30} {total1_limiting:>15} {total2_limiting:>15} {total1_limiting / max(total2_limiting, 1):.1f}x"
    )
    print(f"  {'æ€»LLMè°ƒç”¨æ¬¡æ•°':<30} {total1_llm:>15} {total2_llm:>15} {total1_llm / max(total2_llm, 1):.1f}x")

    # ç›¸åŒä»»åŠ¡å¯¹æ¯”
    common_tasks = set(dict1.keys()) & set(dict2.keys())
    print(f"\nâ±ï¸  ç›¸åŒä»»åŠ¡è¯„ä¼°è€—æ—¶å¯¹æ¯” (å…± {len(common_tasks)} ä¸ª):")
    print(f"  {'ä»»åŠ¡å':<50} {title1:>12} {title2:>12} {'å·®å¼‚':>8}")
    print(f"  {'-' * 85}")

    comparisons = []
    for task in common_tasks:
        r1, r2 = dict1[task], dict2[task]
        if r1.avg_eval_time > 0 and r2.avg_eval_time > 0:
            diff = (r1.avg_eval_time - r2.avg_eval_time) / r2.avg_eval_time * 100
            comparisons.append((task, r1.avg_eval_time, r2.avg_eval_time, diff))

    # æŒ‰å·®å¼‚æ’åº
    comparisons.sort(key=lambda x: x[3], reverse=True)
    for task, avg1, avg2, diff in comparisons[:15]:
        sign = "+" if diff > 0 else ""
        print(f"  {task:<50} {avg1:>10.1f}s {avg2:>10.1f}s {sign}{diff:>6.1f}%")


def export_json(results: list[TaskStats], output_path: Path):
    """å¯¼å‡ºç»“æœä¸º JSON"""
    data = []
    for r in results:
        data.append(
            {
                "task_name": r.task_name,
                "max_retry_count": r.max_retry_count,
                "total_limiting_count": r.total_limiting_count,
                "total_llm_calls": r.total_llm_calls,
                "eval_count": r.eval_count,
                "total_eval_time": r.total_eval_time,
                "avg_eval_time": r.avg_eval_time,
                "max_eval_time": r.max_eval_time,
                "min_eval_time": r.min_eval_time,
            }
        )

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“ ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æ SE_Perf ä»»åŠ¡æ€§èƒ½", formatter_class=argparse.RawDescriptionHelpFormatter, epilog=__doc__
    )
    parser.add_argument("trajectory_dir", type=str, help="è½¨è¿¹ç›®å½•è·¯å¾„")
    parser.add_argument("--compare", type=str, help="å¯¹æ¯”ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    traj_dir = Path(args.trajectory_dir)

    print(f"æ­£åœ¨åˆ†æ: {traj_dir}")
    results = analyze_directory(traj_dir)

    if not results:
        print("æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡æ•°æ®")
        return 1

    title = traj_dir.name
    print_stats(results, title)

    if args.compare:
        compare_dir = Path(args.compare)
        print(f"\næ­£åœ¨åˆ†æå¯¹æ¯”ç›®å½•: {compare_dir}")
        compare_results = analyze_directory(compare_dir)
        if compare_results:
            compare_title = compare_dir.name
            print_stats(compare_results, compare_title)
            compare_stats(results, compare_results, title, compare_title)

    if args.output:
        export_json(results, Path(args.output))

    return 0


if __name__ == "__main__":
    sys.exit(main())
