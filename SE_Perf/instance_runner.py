#!/usr/bin/env python3
"""
æŒ‰å®ä¾‹è¿è¡Œçš„å…¥å£è„šæœ¬ï¼ˆæ§åˆ¶å…¨å±€å¹¶è¡Œåº¦ï¼‰ï¼Œå¹¶è°ƒç”¨ SE_Perf/perf_run.pyã€‚

ç›®æ ‡ï¼š
- å°†åŸæœ¬â€œä¸€ä¸ªè¿­ä»£ä¸­æ‰€æœ‰å®ä¾‹ä¸€èµ·è·‘â€çš„æ–¹å¼ï¼Œæ”¹ä¸ºâ€œä»¥å®ä¾‹ä¸ºå•ä½çš„ç‹¬ç«‹è¿è¡Œâ€ã€‚
- é€šè¿‡æœ¬è„šæœ¬ç»Ÿä¸€æ§åˆ¶å…¨å±€å¹¶è¡Œç¨‹åº¦ï¼ˆåŒæ—¶å¯åŠ¨å¤šå°‘ä¸ªå®ä¾‹çš„ perf_runï¼‰ã€‚
- å¯¹å®ä¾‹ç›®å½•è¿›è¡Œä¸´æ—¶é‡ç»„ï¼šå¤–å±‚ä¸ºå®ä¾‹åæ–‡ä»¶å¤¹ï¼Œå†…å±‚æ‰æ˜¯ JSONï¼ˆæ¯æ¬¡ä»…åŒ…å«ä¸€ä¸ª JSONï¼‰ï¼Œä»¥ä¾› perf_run ä»…å¤„ç†è¯¥å®ä¾‹ã€‚
- å¯¹è¾“å‡ºç›®å½•è¿›è¡Œæ”¹å†™ï¼šä½¿ç”¨ `output_dir/instance_name` ä½œä¸ºè¿è¡Œæ ¹ï¼Œæ–¹ä¾¿ä¸‹æ¸¸æŒ‰å®ä¾‹èšåˆã€‚

ç”¨æ³•ç¤ºä¾‹ï¼š
  python SE_Perf/instance_runner.py --config configs/se_perf_configs/deepseek-v3.1-Plan-AutoSelect.yaml \
         --max-parallel 2 --mode execute

å¯é€‰ï¼š
- ä½¿ç”¨ `--dry-run` ä»…é¢„è§ˆæœ¬è„šæœ¬å°†æ‰§è¡Œçš„å‘½ä»¤ï¼Œä¸å®é™…è°ƒç”¨ perf_runã€‚
- ä½¿ç”¨ `--limit` ä»…é€‰æ‹©å‰ N ä¸ªå®ä¾‹è¿›è¡Œå¿«é€Ÿè¯•è·‘ã€‚
"""

import argparse
import concurrent.futures
import shutil
import subprocess
import sys
from datetime import datetime
from pathlib import Path

import yaml
from core.utils.collect_outputs import collect_outputs

# ensure 'core' package is importable when running as script
sys.path.insert(0, str(Path(__file__).parent))


def _load_yaml(path: Path) -> dict:
    """è¯»å– YAML æ–‡ä»¶ä¸ºå­—å…¸ã€‚"""
    with open(path, encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def _ensure_abs(path_like: str | Path, base: Path) -> Path:
    """å°†è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ä»¥ base ä¸ºæ ¹ï¼‰ã€‚"""
    p = Path(path_like)
    return p if p.is_absolute() else (base / p)


def _discover_instances(instances_dir: Path) -> list[Path]:
    """æšä¸¾å®ä¾‹ JSON æ–‡ä»¶ã€‚"""
    return sorted(instances_dir.glob("*.json"))


def _prepare_temp_space(inst_files: list[Path], tmp_root: Path) -> dict[str, Path]:
    """
    ç”Ÿæˆä¸´æ—¶ç©ºé—´ç»“æ„ï¼š
    - tmp_root/instance_name/instance_name.json

    è¿”å›ï¼š{instance_name: tmp_instance_dir}
    """
    tmp_root.mkdir(parents=True, exist_ok=True)
    mapping: dict[str, Path] = {}
    for fp in inst_files:
        name = fp.stem
        inst_dir = tmp_root / name
        inst_dir.mkdir(parents=True, exist_ok=True)
        target = inst_dir / f"{name}.json"
        shutil.copy(fp, target)
        mapping[name] = inst_dir
    return mapping


def _write_per_instance_config(
    base_cfg: dict,
    tmp_instance_dir: Path,
    instance_name: str,
    config_out_path: Path,
    timestamp: str,
    override_base_out: str | None = None,
) -> Path:
    """
    åŸºäºåŸå§‹ SE é…ç½®ç”Ÿæˆâ€œå•å®ä¾‹â€é…ç½®ï¼š
    - instances.instances_dir æŒ‡å‘ tmp_instance_dirï¼ˆä»…å«è¯¥å®ä¾‹ JSONï¼‰
    - output_dir åœ¨åŸæœ‰åŸºç¡€ä¸Šè¿½åŠ  "/instance_name"ï¼ˆperf_run å†…éƒ¨ä¼šå†æ‹¼æ¥è¿­ä»£ç›®å½•ï¼‰
    """
    cfg = dict(base_cfg)  # æµ…æ‹·è´è¶³å¤Ÿï¼ˆå­—æ®µå‡ä¸ºåŸç”Ÿç±»å‹æˆ–åµŒå¥—å­—å…¸ï¼‰

    instances = cfg.get("instances", {}) or {}
    instances["instances_dir"] = str(tmp_instance_dir)
    cfg["instances"] = instances

    orig_out = str(cfg.get("output_dir", "trajectories_perf/run_{timestamp}")).rstrip("/")
    base_out = str(override_base_out) if override_base_out else orig_out
    final_base_out = base_out.replace("{timestamp}", timestamp)
    cfg["output_dir"] = f"{final_base_out}/{instance_name}"

    config_out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(config_out_path, "w", encoding="utf-8") as f:
        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)
    return config_out_path


def _build_perf_cmd(config_path: Path, mode: str, project_root: Path) -> list[str]:
    """
    æ„å»ºè°ƒç”¨ perf_run.py çš„å‘½ä»¤ï¼ˆä¸æ‰§è¡Œï¼Œä»…è¿”å›å‘½ä»¤åˆ—è¡¨ï¼‰ã€‚
    """
    return [
        sys.executable,
        str(project_root / "SE_Perf" / "perf_run.py"),
        "--config",
        str(config_path),
        "--mode",
        mode,
    ]


import json


def _log_token_usage(output_dir: Path, logger=None):
    """
    ç»Ÿè®¡å¹¶è®°å½• Token ä½¿ç”¨æƒ…å†µ
    """
    token_log_file = output_dir / "token_usage.jsonl"
    if not token_log_file.exists():
        return

    total_prompt = 0
    total_completion = 0
    total = 0
    by_context: dict[str, dict[str, int]] = {}

    try:
        with open(token_log_file, encoding="utf-8") as f:
            for line in f:
                try:
                    rec = json.loads(line)
                    pt = int(rec.get("prompt_tokens") or 0)
                    ct = int(rec.get("completion_tokens") or 0)
                    tt = int(rec.get("total_tokens") or (pt + ct))
                    ctx = str(rec.get("context") or "unknown")

                    total_prompt += pt
                    total_completion += ct
                    total += tt

                    agg = by_context.setdefault(ctx, {"prompt": 0, "completion": 0, "total": 0})
                    agg["prompt"] += pt
                    agg["completion"] += ct
                    agg["total"] += tt
                except Exception:
                    continue

        print("\nğŸ“ˆ Token ä½¿ç”¨ç»Ÿè®¡:")
        print(f"  Total: {total} (Prompt: {total_prompt}, Completion: {total_completion})")
        if by_context:
            print("  æŒ‰ä¸Šä¸‹æ–‡åˆ†ç±»:")
            for ctx, vals in by_context.items():
                print(f"    - {ctx}: prompt={vals['prompt']}, completion={vals['completion']}, total={vals['total']}")

        # å¦‚æœæä¾›äº† loggerï¼Œåˆ™è®°å½•è¯¦ç»† JSON
        if logger:
            logger.info(
                json.dumps(
                    {
                        "token_usage_total": {"prompt": total_prompt, "completion": total_completion, "total": total},
                        "by_context": by_context,
                        "token_log_file": str(token_log_file),
                    },
                    ensure_ascii=False,
                )
            )
    except Exception:
        pass


def _aggregate_token_stats(instance_output_dirs: list[Path], base_root_dir: str):
    """
    èšåˆæ‰€æœ‰å®ä¾‹çš„ Token æ¶ˆè€—
    """
    total_prompt = 0
    total_completion = 0
    total = 0
    by_context: dict[str, dict[str, int]] = {}

    print("\n=== å…¨å±€ Token æ¶ˆè€—ç»Ÿè®¡ ===")

    for inst_dir in instance_output_dirs:
        token_file = inst_dir / "token_usage.jsonl"
        if not token_file.exists():
            continue

        try:
            with open(token_file, encoding="utf-8") as f:
                for line in f:
                    try:
                        rec = json.loads(line)
                        pt = int(rec.get("prompt_tokens") or 0)
                        ct = int(rec.get("completion_tokens") or 0)
                        tt = int(rec.get("total_tokens") or (pt + ct))
                        ctx = str(rec.get("context") or "unknown")

                        total_prompt += pt
                        total_completion += ct
                        total += tt

                        agg = by_context.setdefault(ctx, {"prompt": 0, "completion": 0, "total": 0})
                        agg["prompt"] += pt
                        agg["completion"] += ct
                        agg["total"] += tt
                    except Exception:
                        continue
        except Exception:
            pass

    print(f"ğŸ“ˆ Total All Instances: {total} (Prompt: {total_prompt}, Completion: {total_completion})")
    if by_context:
        print("  æŒ‰ä¸Šä¸‹æ–‡åˆ†ç±» (All Instances):")
        for ctx, vals in by_context.items():
            print(f"    - {ctx}: prompt={vals['prompt']}, completion={vals['completion']}, total={vals['total']}")

    # å°è¯•å†™å…¥æ€»çš„ token_usage.json åˆ°æ ¹ç›®å½•
    try:
        root_token_file = Path(base_root_dir) / "total_token_usage.json"
        with open(root_token_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "token_usage_total": {"prompt": total_prompt, "completion": total_completion, "total": total},
                    "by_context": by_context,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        print(f"å·²ä¿å­˜å…¨å±€ Token ç»Ÿè®¡è‡³: {root_token_file}")
    except Exception as e:
        print(f"ä¿å­˜å…¨å±€ Token ç»Ÿè®¡å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description="æŒ‰å®ä¾‹è¿è¡Œçš„å…¥å£è„šæœ¬ï¼ˆæ§åˆ¶å¹¶è¡Œåº¦ï¼‰ï¼Œå°è£…è°ƒç”¨ perf_run.py")
    parser.add_argument("--config", required=True, help="SE é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä½œä¸ºåŸºç¡€æ¨¡æ¿ï¼‰")
    parser.add_argument("--instances-dir", help="è¦†ç›–é…ç½®ä¸­çš„ instances_dirï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max-parallel", type=int, default=1, help="åŒæ—¶å¹¶å‘çš„å®ä¾‹æ•°ï¼ˆå…¨å±€å¹¶è¡Œåº¦ï¼‰")
    parser.add_argument("--mode", choices=["demo", "execute"], default="execute", help="perf_run.py çš„è¿è¡Œæ¨¡å¼")
    parser.add_argument("--output-dir", help="è¦†ç›–é…ç½®ä¸­çš„ output_dirï¼ˆå¯é€‰ï¼Œç”¨äºç»­è·‘æˆ–æŒ‡å®šè¾“å‡ºæ ¹ç›®å½•ï¼‰")
    parser.add_argument("--limit", type=int, help="ä»…å¤„ç†å‰ N ä¸ªå®ä¾‹ï¼ˆå¿«é€Ÿè¯•è·‘ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="ä»…é¢„è§ˆå‘½ä»¤ï¼Œä¸å®é™…æ‰§è¡Œ")

    args = parser.parse_args()

    project_root = Path(__file__).resolve().parents[1]
    base_cfg_path = _ensure_abs(args.config, project_root)
    base_cfg = _load_yaml(base_cfg_path)

    # è§£æå®ä¾‹ç›®å½•ï¼ˆä¼˜å…ˆ CLI è¦†ç›–ï¼‰
    if args.instances_dir:
        inst_dir = _ensure_abs(args.instances_dir, project_root)
    else:
        inst_dir = _ensure_abs((base_cfg.get("instances", {}) or {}).get("instances_dir", "instances"), project_root)

    inst_files = _discover_instances(inst_dir)
    if args.limit is not None:
        inst_files = inst_files[: max(0, int(args.limit))]

    if not inst_files:
        print(f"æœªåœ¨ {inst_dir} æ‰¾åˆ°å®ä¾‹ JSON æ–‡ä»¶")
        sys.exit(1)

    import uuid

    # ä¸´æ—¶ç©ºé—´ï¼štmp/instance_runner/<timestamp-like>_<random_suffix>
    # æ·»åŠ éšæœºåç¼€é˜²æ­¢å¤šæ¬¡è¿è¡Œæ—¶ç›®å½•å†²çª
    random_suffix = str(uuid.uuid4())[:8]
    tmp_root = project_root / "tmp" / "instance_runner" / f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random_suffix}"
    tmp_root.mkdir(parents=True, exist_ok=True)

    mapping = _prepare_temp_space(inst_files, tmp_root)

    # ç”Ÿæˆ timestamp å¹¶è®¡ç®—è¾“å‡ºæ ¹ç›®å½•ï¼ˆæ”¯æŒ CLI è¦†ç›–ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if args.output_dir:
        # å¦‚æœä¼ å…¥çš„è¦†ç›–ç›®å½•åŒ…å«å ä½ç¬¦ï¼Œåˆ™æ›¿æ¢ï¼›å¦åˆ™æŒ‰åŸæ ·ä½¿ç”¨
        override_root = str(_ensure_abs(args.output_dir, project_root))
        base_root_dir = override_root.replace("{timestamp}", timestamp)
    else:
        base_root_dir = base_cfg.get("output_dir", "trajectories_perf/run_{timestamp}").replace(
            "{timestamp}", timestamp
        )

    # æ„å»º per-instance é…ç½®æ–‡ä»¶å¹¶è¿è¡Œ
    per_instance_cfg_paths: dict[str, Path] = {}
    for name, tmp_inst_dir in mapping.items():
        cfg_out = tmp_inst_dir / "se_config.yaml"
        per_instance_cfg_paths[name] = _write_per_instance_config(
            base_cfg,
            tmp_inst_dir,
            name,
            cfg_out,
            timestamp,
            override_base_out=base_root_dir,
        )

    # å¹¶è¡Œæ‰§è¡Œï¼ˆç®€å•çš„æ‰¹æ¬¡è°ƒåº¦ï¼šçª—å£å¤§å° = max_parallelï¼‰
    names = list(per_instance_cfg_paths.keys())
    total = len(names)
    max_parallel = max(1, int(args.max_parallel))
    i = 0
    successes = 0
    failures = 0
    print(f"å‡†å¤‡è¿è¡Œ {total} ä¸ªå®ä¾‹ï¼›å¹¶è¡Œåº¦ = {max_parallel}ï¼›æ¨¡å¼ = {args.mode}ï¼›dry_run = {args.dry_run}")

    if args.dry_run:
        for name in names:
            cfg_path = per_instance_cfg_paths[name]
            cmd = _build_perf_cmd(cfg_path, args.mode, project_root)
            print(f"[DRY-RUN] {name}: {' '.join(cmd)}")
            successes += 1
    else:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallel) as executor:
            future_map: dict[concurrent.futures.Future, str] = {}
            for name in names:
                cfg_path = per_instance_cfg_paths[name]
                cmd = _build_perf_cmd(cfg_path, args.mode, project_root)
                fut = executor.submit(subprocess.run, cmd, cwd=str(project_root), text=True)
                future_map[fut] = name
            for fut in concurrent.futures.as_completed(future_map):
                name = future_map[fut]
                try:
                    res = fut.result()
                    rc = getattr(res, "returncode", None)
                    if rc == 0:
                        print(f"âœ… å®Œæˆå®ä¾‹: {name}")
                        successes += 1
                    else:
                        print(f"âŒ å¤±è´¥å®ä¾‹: {name}ï¼ˆè¿”å›ç  {rc}ï¼‰")
                        failures += 1
                except Exception as e:
                    print(f"âŒ å¤±è´¥å®ä¾‹: {name}ï¼ˆå¼‚å¸¸ {e}ï¼‰")
                    failures += 1

    print(f"è¿è¡Œç»“æŸï¼šæˆåŠŸ {successes}/{total}ï¼›å¤±è´¥ {failures}")

    if not args.dry_run:
        try:
            res = collect_outputs(base_root_dir, names)
            print(f"èšåˆ traj.pool: {res.get('traj_pool')}")
            print(f"èšåˆ all_hist.json: {res.get('all_hist')}")
            print(f"èšåˆ final.json: {res.get('final')}")
        except Exception:
            print("æ‰¹æ¬¡èšåˆå¤±è´¥")

        # ç»Ÿè®¡æ‰€æœ‰å®ä¾‹çš„ Token æ¶ˆè€—
        try:
            # per_instance_cfg_paths é‡Œçš„ key æ˜¯ instance_nameï¼Œä½†æˆ‘ä»¬éœ€è¦çš„æ˜¯å®é™…çš„è¾“å‡ºç›®å½•
            # åœ¨ main å‡½æ•°å‰é¢æˆ‘ä»¬è®¡ç®—äº†: final_base_out = orig_out.replace("{timestamp}", timestamp)
            # å¹¶ä¸” per instance çš„ output_dir æ˜¯ f"{final_base_out}/{instance_name}"
            # æ‰€ä»¥æˆ‘ä»¬å¯ä»¥é‡å»ºè¿™äº›è·¯å¾„

            instance_output_dirs = []
            for name in names:
                inst_out_dir = Path(base_root_dir) / name
                instance_output_dirs.append(inst_out_dir)

            _aggregate_token_stats(instance_output_dirs, base_root_dir)
        except Exception as e:
            print(f"Token èšåˆç»Ÿè®¡å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
