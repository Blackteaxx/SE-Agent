#!/usr/bin/env python3
"""
æŒ‰å®ä¾‹è¿è¡Œçš„å…¥å£è„šæœ¬ï¼ˆæ§åˆ¶å…¨å±€å¹¶è¡Œåº¦ï¼‰ï¼Œå¹¶è°ƒç”¨ SE_Perf/perf_run.pyã€‚

ç›®æ ‡ï¼š
- å°†åŸæœ¬â€œä¸€ä¸ªè¿­ä»£ä¸­æ‰€æœ‰å®ä¾‹ä¸€èµ·è·‘â€çš„æ–¹å¼ï¼Œæ”¹ä¸ºâ€œä»¥å®ä¾‹ä¸ºå•ä½çš„ç‹¬ç«‹è¿è¡Œâ€ã€‚
- é€šè¿‡æœ¬è„šæœ¬ç»Ÿä¸€æ§åˆ¶å…¨å±€å¹¶è¡Œç¨‹åº¦ï¼ˆåŒæ—¶å¯åŠ¨å¤šå°‘ä¸ªå®ä¾‹çš„ perf_runï¼‰ã€‚
- å¯¹å®ä¾‹ç›®å½•è¿›è¡Œä¸´æ—¶é‡ç»„ï¼šå¤–å±‚ä¸ºå®ä¾‹åæ–‡ä»¶å¤¹ï¼Œå†…å±‚æ‰æ˜¯ JSONï¼ˆæ¯æ¬¡ä»…åŒ…å«ä¸€ä¸ª JSONï¼‰ï¼Œä»¥ä¾› perf_run ä»…å¤„ç†è¯¥å®ä¾‹ã€‚
- å¯¹è¾“å‡ºç›®å½•è¿›è¡Œæ”¹å†™ï¼šä½¿ç”¨ `output_dir/instance_name` ä½œä¸ºè¿è¡Œæ ¹ï¼Œæ–¹ä¾¿ä¸‹æ¸¸æŒ‰å®ä¾‹èšåˆã€‚

ç”¨æ³•ç¤ºä¾‹ï¼š
  python SE_Perf/instance_runner.py --config configs/se_perf_configs/deepseek-v3.1-Plan-AutoSelect.yaml \
         --max-parallel 2 --mode execute

å¯é€‰ï¼š
- ä½¿ç”¨ `--dry-run` ä»…é¢„è§ˆæœ¬è„šæœ¬å°†æ‰§è¡Œçš„å‘½ä»¤ï¼Œä¸å®é™…è°ƒç”¨ perf_runã€‚
- ä½¿ç”¨ `--limit` ä»…é€‰æ‹©å‰ N ä¸ªå®ä¾‹è¿›è¡Œå¿«é€Ÿè¯•è·‘ã€‚
"""

import argparse
import concurrent.futures
import os
import shutil
import subprocess
import sys
import signal
import threading
from datetime import datetime
from pathlib import Path

import yaml
from core.utils.collect_outputs import collect_outputs

# ensure 'core' package is importable when running as script
sys.path.insert(0, str(Path(__file__).parent))


def _load_yaml(path: Path) -> dict:
    """è¯»å– YAML æ–‡ä»¶ä¸ºå­—å…¸ã€‚"""
    with open(path, encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def _ensure_abs(path_like: str | Path, base: Path) -> Path:
    """å°†è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ä»¥ base ä¸ºæ ¹ï¼‰ã€‚"""
    p = Path(path_like)
    return p if p.is_absolute() else (base / p)


def _discover_instances(instances_dir: Path) -> list[Path]:
    """æšä¸¾å®ä¾‹ JSON æ–‡ä»¶ã€‚"""
    return sorted(instances_dir.glob("*.json"))


def _prepare_temp_space(inst_files: list[Path], tmp_root: Path) -> dict[str, Path]:
    """
    ç”Ÿæˆä¸´æ—¶ç©ºé—´ç»“æ„ï¼š
    - tmp_root/instance_name/instance_name.json

    è¿”å›ï¼š{instance_name: tmp_instance_dir}
    """
    tmp_root.mkdir(parents=True, exist_ok=True)
    mapping: dict[str, Path] = {}
    for fp in inst_files:
        name = fp.stem
        inst_dir = tmp_root / name
        inst_dir.mkdir(parents=True, exist_ok=True)
        target = inst_dir / f"{name}.json"
        shutil.copy(fp, target)
        mapping[name] = inst_dir
    return mapping


def _write_per_instance_config(
    base_cfg: dict,
    tmp_instance_dir: Path,
    instance_name: str,
    config_out_path: Path,
    timestamp: str,
    override_base_out: str | None = None,
) -> Path:
    """
    åŸºäºåŸå§‹ SE é…ç½®ç”Ÿæˆâ€œå•å®ä¾‹â€é…ç½®ï¼š
    - instances.instances_dir æŒ‡å‘ tmp_instance_dirï¼ˆä»…å«è¯¥å®ä¾‹ JSONï¼‰
    - output_dir åœ¨åŸæœ‰åŸºç¡€ä¸Šè¿½åŠ  "/instance_name"ï¼ˆperf_run å†…éƒ¨ä¼šå†æ‹¼æ¥è¿­ä»£ç›®å½•ï¼‰
    """
    cfg = dict(base_cfg)  # æµ…æ‹·è´è¶³å¤Ÿï¼ˆå­—æ®µå‡ä¸ºåŸç”Ÿç±»å‹æˆ–åµŒå¥—å­—å…¸ï¼‰

    instances = cfg.get("instances", {}) or {}
    instances["instances_dir"] = str(tmp_instance_dir)
    cfg["instances"] = instances

    orig_out = str(cfg.get("output_dir", "trajectories_perf/run_{timestamp}")).rstrip("/")
    base_out = str(override_base_out) if override_base_out else orig_out
    final_base_out = base_out.replace("{timestamp}", timestamp)
    cfg["output_dir"] = f"{final_base_out}/{instance_name}"

    # å¦‚æœé…ç½®äº† Global Memory ä¸” backend ä¸º chromaï¼Œä¸”æœªæ˜¾å¼æŒ‡å®š persist_path
    # åˆ™è‡ªåŠ¨å°† persist_path è®¾ç½®ä¸º {cfg["output_dir"]}/global_memory
    # è¿™æ ·æ¯ä¸ªå®ä¾‹çš„ ChromaDB å°±ä¼šå­˜å‚¨åœ¨å„è‡ªçš„è¾“å‡ºç›®å½•ä¸‹ï¼Œé¿å…å¹¶å‘å†™å…¥å†²çª
    global_memory_bank = cfg.get("global_memory_bank")
    if isinstance(global_memory_bank, dict) and global_memory_bank.get("enabled"):
        memory = global_memory_bank.get("memory", {})
        if memory.get("backend") == "chroma":
            chroma_cfg = memory.get("chroma", {})
            # å¼ºåˆ¶è¦†ç›– persist_path ä¸ºå®ä¾‹è¾“å‡ºç›®å½•ä¸‹çš„ global_memory
            chroma_cfg["persist_path"] = f"{final_base_out}/global_memory"
            memory["chroma"] = chroma_cfg
            global_memory_bank["memory"] = memory
            cfg["global_memory_bank"] = global_memory_bank

    config_out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(config_out_path, "w", encoding="utf-8") as f:
        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)
    return config_out_path


def _build_perf_cmd(config_path: Path, mode: str, project_root: Path) -> list[str]:
    """
    æ„å»ºè°ƒç”¨ perf_run.py çš„å‘½ä»¤ï¼ˆä¸æ‰§è¡Œï¼Œä»…è¿”å›å‘½ä»¤åˆ—è¡¨ï¼‰ã€‚
    """
    return [
        sys.executable,
        str(project_root / "SE_Perf" / "perf_run.py"),
        "--config",
        str(config_path),
        "--mode",
        mode,
    ]


import json


def _log_token_usage(output_dir: Path, logger=None):
    """
    ç»Ÿè®¡å¹¶è®°å½• Token ä½¿ç”¨æƒ…å†µ
    """
    token_log_file = output_dir / "token_usage.jsonl"
    if not token_log_file.exists():
        return

    total_prompt = 0
    total_completion = 0
    total = 0
    by_context: dict[str, dict[str, int]] = {}

    try:
        with open(token_log_file, encoding="utf-8") as f:
            for line in f:
                try:
                    rec = json.loads(line)
                    pt = int(rec.get("prompt_tokens") or 0)
                    ct = int(rec.get("completion_tokens") or 0)
                    tt = int(rec.get("total_tokens") or (pt + ct))
                    ctx = str(rec.get("context") or "unknown")

                    total_prompt += pt
                    total_completion += ct
                    total += tt

                    agg = by_context.setdefault(ctx, {"prompt": 0, "completion": 0, "total": 0})
                    agg["prompt"] += pt
                    agg["completion"] += ct
                    agg["total"] += tt
                except Exception:
                    continue

        print("\nğŸ“ˆ Token ä½¿ç”¨ç»Ÿè®¡:")
        print(f"  Total: {total} (Prompt: {total_prompt}, Completion: {total_completion})")
        if by_context:
            print("  æŒ‰ä¸Šä¸‹æ–‡åˆ†ç±»:")
            for ctx, vals in by_context.items():
                print(f"    - {ctx}: prompt={vals['prompt']}, completion={vals['completion']}, total={vals['total']}")

        # å¦‚æœæä¾›äº† loggerï¼Œåˆ™è®°å½•è¯¦ç»† JSON
        if logger:
            logger.info(
                json.dumps(
                    {
                        "token_usage_total": {"prompt": total_prompt, "completion": total_completion, "total": total},
                        "by_context": by_context,
                        "token_log_file": str(token_log_file),
                    },
                    ensure_ascii=False,
                )
            )
    except Exception:
        pass


def _aggregate_token_stats(instance_output_dirs: list[Path], base_root_dir: str):
    """
    èšåˆæ‰€æœ‰å®ä¾‹çš„ Token æ¶ˆè€—
    """
    total_prompt = 0
    total_completion = 0
    total = 0
    by_context: dict[str, dict[str, int]] = {}

    print("\n=== å…¨å±€ Token æ¶ˆè€—ç»Ÿè®¡ ===")

    for inst_dir in instance_output_dirs:
        token_file = inst_dir / "token_usage.jsonl"
        if not token_file.exists():
            continue

        try:
            with open(token_file, encoding="utf-8") as f:
                for line in f:
                    try:
                        rec = json.loads(line)
                        pt = int(rec.get("prompt_tokens") or 0)
                        ct = int(rec.get("completion_tokens") or 0)
                        tt = int(rec.get("total_tokens") or (pt + ct))
                        ctx = str(rec.get("context") or "unknown")

                        total_prompt += pt
                        total_completion += ct
                        total += tt

                        agg = by_context.setdefault(ctx, {"prompt": 0, "completion": 0, "total": 0})
                        agg["prompt"] += pt
                        agg["completion"] += ct
                        agg["total"] += tt
                    except Exception:
                        continue
        except Exception:
            pass

    print(f"ğŸ“ˆ Total All Instances: {total} (Prompt: {total_prompt}, Completion: {total_completion})")
    if by_context:
        print("  æŒ‰ä¸Šä¸‹æ–‡åˆ†ç±» (All Instances):")
        for ctx, vals in by_context.items():
            print(f"    - {ctx}: prompt={vals['prompt']}, completion={vals['completion']}, total={vals['total']}")

    # å°è¯•å†™å…¥æ€»çš„ token_usage.json åˆ°æ ¹ç›®å½•
    try:
        root_token_file = Path(base_root_dir) / "total_token_usage.json"
        with open(root_token_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "token_usage_total": {"prompt": total_prompt, "completion": total_completion, "total": total},
                    "by_context": by_context,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        print(f"å·²ä¿å­˜å…¨å±€ Token ç»Ÿè®¡è‡³: {root_token_file}")
    except Exception as e:
        print(f"ä¿å­˜å…¨å±€ Token ç»Ÿè®¡å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description="æŒ‰å®ä¾‹è¿è¡Œçš„å…¥å£è„šæœ¬ï¼ˆæ§åˆ¶å¹¶è¡Œåº¦ï¼‰ï¼Œå°è£…è°ƒç”¨ perf_run.py")
    parser.add_argument("--config", required=True, help="SE é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä½œä¸ºåŸºç¡€æ¨¡æ¿ï¼‰")
    parser.add_argument("--instances-dir", help="è¦†ç›–é…ç½®ä¸­çš„ instances_dirï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max-parallel", type=int, default=1, help="åŒæ—¶å¹¶å‘çš„å®ä¾‹æ•°ï¼ˆå…¨å±€å¹¶è¡Œåº¦ï¼‰")
    parser.add_argument("--mode", choices=["demo", "execute"], default="execute", help="perf_run.py çš„è¿è¡Œæ¨¡å¼")
    parser.add_argument("--output-dir", help="è¦†ç›–é…ç½®ä¸­çš„ output_dirï¼ˆå¯é€‰ï¼Œç”¨äºç»­è·‘æˆ–æŒ‡å®šè¾“å‡ºæ ¹ç›®å½•ï¼‰")
    parser.add_argument("--limit", type=int, help="ä»…å¤„ç†å‰ N ä¸ªå®ä¾‹ï¼ˆå¿«é€Ÿè¯•è·‘ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="ä»…é¢„è§ˆå‘½ä»¤ï¼Œä¸å®é™…æ‰§è¡Œ")

    args = parser.parse_args()

    project_root = Path(__file__).resolve().parents[1]
    base_cfg_path = _ensure_abs(args.config, project_root)
    base_cfg = _load_yaml(base_cfg_path)

    # è§£æå®ä¾‹ç›®å½•ï¼ˆä¼˜å…ˆ CLI è¦†ç›–ï¼‰
    if args.instances_dir:
        inst_dir = _ensure_abs(args.instances_dir, project_root)
    else:
        inst_dir = _ensure_abs((base_cfg.get("instances", {}) or {}).get("instances_dir", "instances"), project_root)

    inst_files = _discover_instances(inst_dir)
    if args.limit is not None:
        inst_files = inst_files[: max(0, int(args.limit))]

    if not inst_files:
        print(f"æœªåœ¨ {inst_dir} æ‰¾åˆ°å®ä¾‹ JSON æ–‡ä»¶")
        sys.exit(1)

    import uuid

    # ä¸´æ—¶ç©ºé—´ï¼štmp/instance_runner/<timestamp-like>_<random_suffix>
    # æ·»åŠ éšæœºåç¼€é˜²æ­¢å¤šæ¬¡è¿è¡Œæ—¶ç›®å½•å†²çª
    random_suffix = str(uuid.uuid4())[:8]
    tmp_root = project_root / "tmp" / "instance_runner" / f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random_suffix}"
    tmp_root.mkdir(parents=True, exist_ok=True)

    mapping = _prepare_temp_space(inst_files, tmp_root)

    # ç”Ÿæˆ timestamp å¹¶è®¡ç®—è¾“å‡ºæ ¹ç›®å½•ï¼ˆæ”¯æŒ CLI è¦†ç›–ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if args.output_dir:
        # å¦‚æœä¼ å…¥çš„è¦†ç›–ç›®å½•åŒ…å«å ä½ç¬¦ï¼Œåˆ™æ›¿æ¢ï¼›å¦åˆ™æŒ‰åŸæ ·ä½¿ç”¨
        override_root = str(_ensure_abs(args.output_dir, project_root))
        base_root_dir = override_root.replace("{timestamp}", timestamp)
    else:
        base_root_dir = base_cfg.get("output_dir", "trajectories_perf/run_{timestamp}").replace(
            "{timestamp}", timestamp
        )

    # æ„å»º per-instance é…ç½®æ–‡ä»¶å¹¶è¿è¡Œ
    per_instance_cfg_paths: dict[str, Path] = {}
    for name, tmp_inst_dir in mapping.items():
        cfg_out = tmp_inst_dir / "se_config.yaml"
        per_instance_cfg_paths[name] = _write_per_instance_config(
            base_cfg,
            tmp_inst_dir,
            name,
            cfg_out,
            timestamp,
            override_base_out=base_root_dir,
        )

    # å¹¶è¡Œæ‰§è¡Œï¼ˆç®€å•çš„æ‰¹æ¬¡è°ƒåº¦ï¼šçª—å£å¤§å° = max_parallelï¼‰
    names = list(per_instance_cfg_paths.keys())
    total = len(names)
    max_parallel = max(1, int(args.max_parallel))
    i = 0
    successes = 0
    failures = 0
    print(f"å‡†å¤‡è¿è¡Œ {total} ä¸ªå®ä¾‹ï¼›å¹¶è¡Œåº¦ = {max_parallel}ï¼›æ¨¡å¼ = {args.mode}ï¼›dry_run = {args.dry_run}")

    # å‡†å¤‡ä¸€ä¸ªå…¨å±€åˆ—è¡¨æ¥è·Ÿè¸ªæ­£åœ¨è¿è¡Œçš„å­è¿›ç¨‹
    active_processes: list[subprocess.Popen] = []
    active_processes_lock = threading.Lock()

    def _cleanup_processes():
        """ç»ˆæ­¢æ‰€æœ‰æ´»è·ƒçš„å­è¿›ç¨‹ã€‚"""
        with active_processes_lock:
            # æ— è®ºæ˜¯å¦æœ‰è®°å½•çš„ active_processesï¼Œéƒ½å°è¯•æ¸…ç†æ•´ä¸ªè¿›ç¨‹ç»„
            # è¿™å¯ä»¥æ•è·é‚£äº›è¿˜æ²¡æ¥å¾—åŠåŠ å…¥ active_processes åˆ—è¡¨çš„å­™å­è¿›ç¨‹
            try:
                # ç»™æ•´ä¸ªè¿›ç¨‹ç»„å‘é€ SIGTERM
                os.killpg(0, signal.SIGTERM)
            except Exception:
                pass

            if not active_processes:
                return

            print(f"\næ­£åœ¨ç»ˆæ­¢ {len(active_processes)} ä¸ªæ´»è·ƒå­è¿›ç¨‹...")
            for p in active_processes:
                try:
                    if p.poll() is None:  # ä»…ç»ˆæ­¢è¿˜åœ¨è¿è¡Œçš„è¿›ç¨‹
                        p.terminate()
                except Exception:
                    pass
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©è¿›ç¨‹ä¼˜é›…é€€å‡º
            try:
                for p in active_processes:
                    try:
                        p.wait(timeout=2)
                    except subprocess.TimeoutExpired:
                        p.kill()  # å¼ºåˆ¶æ€æ­»
                    except Exception:
                        pass
            except Exception:
                pass

    def _signal_handler(sig, frame):
        """å¤„ç†ç»ˆæ­¢ä¿¡å·ã€‚"""
        print(f"\næ¥æ”¶åˆ°ä¿¡å· {sig}ï¼Œå‡†å¤‡é€€å‡º...")
        _cleanup_processes()
        sys.exit(1)

    # æ³¨å†Œä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, _signal_handler)
    signal.signal(signal.SIGTERM, _signal_handler)

    # åˆ›å»ºä¸€ä¸ªè¿›ç¨‹ç»„ï¼Œä»¥ä¾¿èƒ½å¤Ÿä¸€æ¬¡æ€§æ€æ­»æ‰€æœ‰å­è¿›ç¨‹
    # æ³¨æ„ï¼šåœ¨æŸäº›ç¯å¢ƒä¸­ï¼Œos.setsid() å¯èƒ½ä¼šæ”¹å˜è¿›ç¨‹ç»„ IDï¼Œè¿™åœ¨ shell è„šæœ¬æˆ– CI ç¯å¢ƒä¸­å¯èƒ½ä¼šæœ‰å‰¯ä½œç”¨ã€‚
    # ä½†å¯¹äºäº¤äº’å¼è¿è¡Œï¼Œè¿™æ˜¯ç¡®ä¿å½»åº•æ¸…ç†å­è¿›ç¨‹çš„å¥½æ–¹æ³•ã€‚
    # ä¸ºäº†é¿å…å½±å“ä¸»è¿›ç¨‹çš„ä¿¡å·å¤„ç†ï¼Œæˆ‘ä»¬åªåœ¨åˆ›å»ºæ–°è¿›ç¨‹ç»„æ—¶è¿™æ ·åšã€‚
    # ä½†è¿™é‡Œæˆ‘ä»¬æ˜¯åœ¨ä¸»è¿›ç¨‹ä¸­è¿è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬æ›´å€¾å‘äºåœ¨ _cleanup_processes ä¸­å¤„ç†ã€‚
    # ä¸è¿‡ï¼Œå¦‚æœ instance_runner.py è¢«æ€æ­»ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒçš„å­è¿›ç¨‹ä¹Ÿè¢«æ€æ­»ã€‚
    # ä¸€ä¸ªå¸¸è§çš„åšæ³•æ˜¯æŠŠå½“å‰è¿›ç¨‹è®¾ä¸ºè¿›ç¨‹ç»„ç»„é•¿ã€‚
    try:
        os.setpgrp()
    except Exception:
        pass

    # åŒ…è£… subprocess.run ä»¥ä¾¿è·Ÿè¸ª Popen å¯¹è±¡
    def _run_process(cmd, cwd):
        try:
            # ä½¿ç”¨ Popen æ›¿ä»£ subprocess.run ä»¥ä¾¿è·å–å¥æŸ„
            with subprocess.Popen(cmd, cwd=cwd, text=True) as proc:
                with active_processes_lock:
                    active_processes.append(proc)

                try:
                    proc.wait()
                    return subprocess.CompletedProcess(proc.args, proc.returncode)
                finally:
                    with active_processes_lock:
                        if proc in active_processes:
                            active_processes.remove(proc)
        except Exception as e:
            raise e

    if args.dry_run:
        for name in names:
            cfg_path = per_instance_cfg_paths[name]
            cmd = _build_perf_cmd(cfg_path, args.mode, project_root)
            print(f"[DRY-RUN] {name}: {' '.join(cmd)}")
            successes += 1
    else:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallel) as executor:
            future_map: dict[concurrent.futures.Future, str] = {}
            for name in names:
                cfg_path = per_instance_cfg_paths[name]
                cmd = _build_perf_cmd(cfg_path, args.mode, project_root)
                # æ”¹ä¸ºè°ƒç”¨ _run_process
                fut = executor.submit(_run_process, cmd, cwd=str(project_root))
                future_map[fut] = name
            for fut in concurrent.futures.as_completed(future_map):
                name = future_map[fut]
                try:
                    res = fut.result()
                    rc = getattr(res, "returncode", None)
                    if rc == 0:
                        print(f"âœ… å®Œæˆå®ä¾‹: {name}")
                        successes += 1
                    else:
                        print(f"âŒ å¤±è´¥å®ä¾‹: {name}ï¼ˆè¿”å›ç  {rc}ï¼‰")
                        failures += 1
                except Exception as e:
                    print(f"âŒ å¤±è´¥å®ä¾‹: {name}ï¼ˆå¼‚å¸¸ {e}ï¼‰")
                    failures += 1

    print(f"è¿è¡Œç»“æŸï¼šæˆåŠŸ {successes}/{total}ï¼›å¤±è´¥ {failures}")

    if not args.dry_run:
        try:
            res = collect_outputs(base_root_dir, names)
            print(f"èšåˆ traj.pool: {res.get('traj_pool')}")
            print(f"èšåˆ all_hist.json: {res.get('all_hist')}")
            print(f"èšåˆ final.json: {res.get('final')}")
        except Exception:
            print("æ‰¹æ¬¡èšåˆå¤±è´¥")

        # ç»Ÿè®¡æ‰€æœ‰å®ä¾‹çš„ Token æ¶ˆè€—
        try:
            # per_instance_cfg_paths é‡Œçš„ key æ˜¯ instance_nameï¼Œä½†æˆ‘ä»¬éœ€è¦çš„æ˜¯å®é™…çš„è¾“å‡ºç›®å½•
            # åœ¨ main å‡½æ•°å‰é¢æˆ‘ä»¬è®¡ç®—äº†: final_base_out = orig_out.replace("{timestamp}", timestamp)
            # å¹¶ä¸” per instance çš„ output_dir æ˜¯ f"{final_base_out}/{instance_name}"
            # æ‰€ä»¥æˆ‘ä»¬å¯ä»¥é‡å»ºè¿™äº›è·¯å¾„

            instance_output_dirs = []
            for name in names:
                inst_out_dir = Path(base_root_dir) / name
                instance_output_dirs.append(inst_out_dir)

            _aggregate_token_stats(instance_output_dirs, base_root_dir)
        except Exception as e:
            print(f"Token èšåˆç»Ÿè®¡å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
