#!/usr/bin/env python3
"""
LLMå®¢æˆ·ç«¯æ¨¡å—
ä¸ºSEæ¡†æ¶æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£
"""

from typing import Any, Dict, List, Optional  # noqa: UP035
import re
import json
import time

from openai import OpenAI

from core.utils.se_logger import get_se_logger


class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’ŒAPIç«¯ç‚¹"""

    def __init__(self, model_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            model_config: æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«name, api_base, api_keyç­‰
        """
        self.config = model_config
        self.logger = get_se_logger("llm_client", emoji="ğŸ¤–")

        # éªŒè¯å¿…éœ€çš„é…ç½®å‚æ•°
        required_keys = ["name", "api_base", "api_key"]
        missing_keys = [key for key in required_keys if key not in model_config]
        if missing_keys:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„é…ç½®å‚æ•°: {missing_keys}")

        # è¯·æ±‚æ§åˆ¶å‚æ•°ï¼ˆå¸¦é»˜è®¤å€¼ï¼‰
        self.request_timeout: float = float(self.config.get("request_timeout", 600.0))
        self.max_retries: int = int(self.config.get("max_retries", 3))
        self.retry_delay: float = float(self.config.get("retry_delay", 1.5))

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œéµå¾ªapi_test.pyçš„å·¥ä½œæ¨¡å¼ï¼Œå¹¶è®¾ç½®è¶…æ—¶
        self.client = OpenAI(
            api_key=self.config["api_key"],
            base_url=self.config["api_base"],
            timeout=self.request_timeout,
        )

        self.logger.info(f"åˆå§‹åŒ–LLMå®¢æˆ·ç«¯: {self.config['name']}")

    def clean_think_tags(self, text: str) -> str:
        """ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶ä¸­å†…å®¹ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºç™½"""
        try:
            return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE).strip()
        except Exception:
            return text

    def call_llm(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.3,
        max_tokens: Optional[int] = None,
        enable_thinking: Optional[bool] = None,
    ) -> str:
        """
        è°ƒç”¨LLMå¹¶è¿”å›å“åº”å†…å®¹

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼

        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹

        Raises:
            Exception: LLMè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ä½¿ç”¨é…ç½®ä¸­çš„ max_output_tokens ä½œä¸ºé»˜è®¤å€¼
        if max_tokens is None:
            max_tokens = self.config.get("max_output_tokens", 4000)

        attempt = 0
        last_err: Optional[Exception] = None
        while attempt < self.max_retries:
            try:
                self.logger.debug(
                    f"è°ƒç”¨LLM: {len(messages)} æ¡æ¶ˆæ¯, temp={temperature}, max_tokens={max_tokens}"
                )

                # è§„èŒƒåŒ–æ¨¡å‹åï¼šä»…ç§»é™¤ openai/ å‰ç¼€ï¼Œå…¶ä»–ä¿æŒåŸæ ·
                raw_name = self.config.get("name", "")
                if isinstance(raw_name, str) and raw_name.startswith("openai/"):
                    model_name = raw_name.split("/", 1)[1]
                else:
                    model_name = raw_name

                # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è°ƒç”¨ï¼Œä¼ å…¥å¿…éœ€çš„å‚æ•°
                # ç”Ÿæˆå¯é€‰çš„ extra_bodyï¼Œç”¨äºæ§åˆ¶æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡æ¿
                extra_body: Dict[str, Any] = {}
                if enable_thinking is not None:
                    extra_body = {"chat_template_kwargs": {"enable_thinking": bool(enable_thinking)}}

                response = self.client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **({"extra_body": extra_body} if extra_body else {}),
                )

                # æå–å“åº”å†…å®¹
                content = response.choices[0].message.content

                # è®°å½•ä½¿ç”¨æƒ…å†µ
                if getattr(response, "usage", None):
                    self.logger.debug(
                        f"Tokenä½¿ç”¨: è¾“å…¥={getattr(response.usage, 'prompt_tokens', 'æœªçŸ¥')}, "
                        f"è¾“å‡º={getattr(response.usage, 'completion_tokens', 'æœªçŸ¥')}, "
                        f"æ€»è®¡={getattr(response.usage, 'total_tokens', 'æœªçŸ¥')}"
                    )

                return content

            except Exception as e:
                last_err = e
                attempt += 1
                self.logger.warning(
                    f"LLMè°ƒç”¨å¤±è´¥: {e}; attempt={attempt}/{self.max_retries}"
                )
                if attempt < self.max_retries:
                    time.sleep(self.retry_delay)
                else:
                    break

        assert last_err is not None
        raise last_err

    def call_with_system_prompt(
        self, system_prompt: str, user_prompt: str, temperature: float = 0.3, max_tokens: Optional[int] = None
    ) -> str:
        """
        ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯è°ƒç”¨LLM

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°

        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹
        """
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

        return self.call_llm(messages, temperature, max_tokens)

    @classmethod
    def from_se_config(cls, se_config: Dict[str, Any], use_operator_model: bool = False) -> "LLMClient":
        """
        ä»SEæ¡†æ¶é…ç½®åˆ›å»ºLLMå®¢æˆ·ç«¯

        Args:
            se_config: SEæ¡†æ¶é…ç½®å­—å…¸
            use_operator_model: æ˜¯å¦ä½¿ç”¨operator_modelsé…ç½®è€Œä¸æ˜¯ä¸»æ¨¡å‹é…ç½®

        Returns:
            LLMå®¢æˆ·ç«¯å®ä¾‹
        """
        if use_operator_model and "operator_models" in se_config:
            model_config = se_config["operator_models"]
        else:
            model_config = se_config["model"]

        return cls(model_config)


class TrajectorySummarizer:
    """ä¸“é—¨ç”¨äºè½¨è¿¹æ€»ç»“çš„LLMå®¢æˆ·ç«¯åŒ…è£…å™¨"""

    def __init__(self, llm_client: LLMClient):
        """
        åˆå§‹åŒ–è½¨è¿¹æ€»ç»“å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
        """
        self.llm_client = llm_client
        self.logger = get_se_logger("traj_summarizer", emoji="ğŸ“Š")

    def summarize_trajectory(
        self, trajectory_content: str, patch_content: str, iteration: int, problem_description: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæ€»ç»“è½¨è¿¹å†…å®¹

        Args:
            trajectory_content: .traæ–‡ä»¶å†…å®¹
            patch_content: .patch/.predæ–‡ä»¶å†…å®¹ (é¢„æµ‹ç»“æœ)
            iteration: è¿­ä»£æ¬¡æ•°
            problem_description: é—®é¢˜æè¿°ï¼ˆå¯é€‰ï¼Œå°†å¹¶å…¥æç¤ºè¯ï¼‰

        Returns:
            è½¨è¿¹æ€»ç»“å­—å…¸
        """
        from .traj_summarizer import TrajSummarizer

        summarizer = TrajSummarizer()

        # è·å–æç¤ºè¯
        system_prompt = summarizer.get_system_prompt()
        user_prompt = summarizer.format_user_prompt(trajectory_content, patch_content, problem_description)

        self.logger.info(f"å¼€å§‹LLMè½¨è¿¹æ€»ç»“ (è¿­ä»£{iteration})")
        self.logger.debug(f"LLMç³»ç»Ÿæç¤ºè¯ (è¿­ä»£{iteration}):\n{system_prompt}")
        self.logger.debug(f"LLMç”¨æˆ·æç¤ºè¯ (è¿­ä»£{iteration}):\n{user_prompt}")

        # é‡è¯•æœºåˆ¶ï¼šè§£æå¤±è´¥æˆ–è°ƒç”¨å¤±è´¥æ—¶é‡è¯•ï¼Œæ€»æ¬¡æ•°3æ¬¡
        last_error: Optional[str] = None
        for attempt in range(1, 4):
            try:
                response = self.llm_client.call_with_system_prompt(
                    system_prompt=system_prompt, user_prompt=user_prompt, temperature=0.6, max_tokens=10000
                )
                self.logger.debug(f"LLMåŸå§‹å“åº” (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡):\n{response}")
                # å»é™¤æ€è€ƒå†…å®¹
                response = self.llm_client.clean_think_tags(response)
                self.logger.debug(f"LLMæ¸…ç†åå“åº” (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡):\n{response}")

                # ä»…æ‰§è¡Œå­—ç¬¦ä¸²åˆ°JSONçš„è§£æï¼Œæ ¼å¼ä¸æ­£ç¡®/è§£æå¤±è´¥ä¼šæŠ›å¼‚å¸¸
                summary = summarizer.parse_response(response)

                self.logger.info(f"LLMè½¨è¿¹æ€»ç»“æˆåŠŸ (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡)")
                return summary

            except json.JSONDecodeError as e:
                last_error = "json_decode_error"
                self.logger.warning(
                    f"LLMè½¨è¿¹æ€»ç»“è§£æå¤±è´¥: JSONè§£æé”™è¯¯ (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡): {e}"
                )
            except ValueError as e:
                last_error = "invalid_json_format"
                self.logger.warning(
                    f"LLMè½¨è¿¹æ€»ç»“è§£æå¤±è´¥: æ— æœ‰æ•ˆJSONç‰‡æ®µ (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡): {e}"
                )
            except Exception as e:
                last_error = "llm_call_failed"
                self.logger.warning(f"LLMè½¨è¿¹æ€»ç»“è°ƒç”¨å¤±è´¥ (è¿­ä»£{iteration}, ç¬¬{attempt}æ¬¡): {e}")

        # æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œè¿”å›å¤‡ç”¨æ€»ç»“
        if last_error:
            self.logger.error(f"LLMè½¨è¿¹æ€»ç»“æœ€ç»ˆå¤±è´¥ (è¿­ä»£{iteration}): {last_error}")
        return summarizer.create_fallback_summary(trajectory_content, patch_content, iteration)
