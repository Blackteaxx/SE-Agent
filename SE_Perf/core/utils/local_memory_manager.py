#!/usr/bin/env python3

"""
Local Memory Manager

ç®¡ç†çŸ­æœŸå·¥ä½œè®°å¿†ï¼ˆLocal Memoryï¼‰ï¼Œç”¨äºåœ¨è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼š
- ç»´æŠ¤å…¨å±€çŠ¶æ€ï¼ˆå½“å‰ä»£æ•°ã€æœ€ä½³æ€§èƒ½ã€æœ€ä½³è§£IDã€å½“å‰æ–¹æ³•ï¼‰
- è®°å½•å°è¯•è¿‡çš„é«˜å±‚æ–¹å‘åŠå…¶æˆè´¥ï¼ˆattempted_directionsï¼‰
- æ²‰æ·€å¯è¿ç§»çš„æˆåŠŸ/å¤±è´¥ç»éªŒï¼ˆreasoning_bankï¼‰

è¯¥æ¨¡å—å‚è€ƒ reasoningbank çš„ Memory è®¾è®¡æ€æƒ³ï¼Œæä¾›ç»“æ„åŒ–çš„ JSON å­˜å‚¨ä¸å¢é‡æ›´æ–°ï¼Œ
å¹¶åœ¨éœ€è¦æ—¶è°ƒç”¨ LLM è¿›è¡Œè®°å¿†æç‚¼ï¼ˆExtractionï¼‰ã€‚
"""

from __future__ import annotations

import json
import math
from pathlib import Path
from typing import Any

from .se_logger import get_se_logger


class LocalMemoryManager:
    """
    æœ¬åœ°è®°å¿†ç®¡ç†å™¨ï¼ˆJSON åç«¯ï¼‰

    å­˜å‚¨ç»“æ„ï¼ˆç¤ºä¾‹ï¼‰ï¼š
    {
      "global_status": {
        "current_generation": 5,
        "best_runtime": "120ms",
        "best_solution_id": "Gen3_Sol_4",
        "current_approach": "Dynamic Programming with Bitmask"
      },
      "attempted_directions": [
        {"direction": "Use fast IO", "outcome": "Success", "source_ref": "iter_4", "evidence": "..."}
      ],
      "reasoning_bank": [
        {
          "type": "Success",
          "title": "Bitwise Operation Optimization",
          "description": "Replace modulo with bitwise AND for powers of 2.",
          "content": "Using x & (MOD-1) improved constant factor.",
          "related_operator": "Refinement",
          "source_ref": {"generation": 3, "solution_id": "Sol_5", "parent_id": "Gen_2_Sol_2"},
          "evidence": {
            "code_change": "Changed dp[i] % 1024 -> dp[i] & 1023",
            "metrics_delta": "Runtime: 150ms -> 120ms (-20%)",
            "context": "Effective when MOD=1024"
          }
        }
      ]
    }
    """

    def __init__(
        self,
        memory_path: str | Path,
        llm_client: Any | None = None,
        token_limit: int = 1500,
    ) -> None:
        """
        åˆå§‹åŒ–æœ¬åœ°è®°å¿†ç®¡ç†å™¨ã€‚

        Args:
            memory_path: è®°å¿†åº“ JSON æ–‡ä»¶è·¯å¾„ã€‚
            llm_client: å¯é€‰çš„ LLM å®¢æˆ·ç«¯ï¼Œç”¨äºè¿›è¡Œè®°å¿†æç‚¼ã€‚
            token_limit: è§¦å‘å‹ç¼©çš„è¿‘ä¼¼ token/å­—ç¬¦é˜ˆå€¼ã€‚
        """
        self.path = Path(memory_path)
        self.llm_client = llm_client
        self.token_limit = int(token_limit)
        self.logger = get_se_logger("local_memory", emoji="ğŸ§ ")

    def initialize(self) -> None:
        """ç¡®ä¿è®°å¿†åº“æ–‡ä»¶å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨åˆ™åˆ›å»ºç©ºç»“æ„ã€‚"""
        self.path.parent.mkdir(parents=True, exist_ok=True)
        if not self.path.exists():
            empty = {
                "global_status": {
                    "current_generation": 0,
                    "best_runtime": None,
                    "best_solution_id": None,
                    "current_approach": None,
                },
                "attempted_directions": [],
                "reasoning_bank": [],
            }
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(empty, f, ensure_ascii=False, indent=2)
            self.logger.info(f"åˆå§‹åŒ–æœ¬åœ°è®°å¿†åº“: {self.path}")

    def load(self) -> dict[str, Any]:
        """åŠ è½½è®°å¿†åº“ JSONã€‚"""
        try:
            with open(self.path, encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            return {"global_status": {}, "attempted_directions": [], "reasoning_bank": []}
        except Exception as e:
            self.logger.warning(f"åŠ è½½æœ¬åœ°è®°å¿†åº“å¤±è´¥: {e}")
            return {"global_status": {}, "attempted_directions": [], "reasoning_bank": []}

    def save(self, memory: dict[str, Any]) -> None:
        """ä¿å­˜è®°å¿†åº“ JSONã€‚"""
        try:
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(memory, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜æœ¬åœ°è®°å¿†åº“å¤±è´¥: {e}")
            raise

    def render_as_markdown(self, memory: dict[str, Any]) -> str:
        """
        å°†ç»“æ„åŒ–è®°å¿†æ¸²æŸ“ä¸ºç®€æ´çš„ Markdown æ–‡æœ¬ï¼Œä¾¿äºæ³¨å…¥ System Promptã€‚
        """
        gs = memory.get("global_status") or {}
        dirs = memory.get("attempted_directions") or []
        bank = memory.get("reasoning_bank") or []

        lines: list[str] = []
        lines.append("## Global Status")
        lines.append(f"- Generation: {gs.get('current_generation', 'N/A')}")
        lines.append(f"- Best Runtime: {gs.get('best_runtime', 'N/A')}")
        lines.append(f"- Best Solution ID: {gs.get('best_solution_id', 'N/A')}")
        lines.append(f"- Current Approach: {gs.get('current_approach', 'N/A')}")
        lines.append("")
        lines.append("## Attempted Directions")
        for d in dirs[:8]:
            lines.append(f"- [{d.get('outcome', 'Unknown')}] {d.get('direction', '')} â€” {d.get('evidence', '')}")
        lines.append("")
        lines.append("## Reasoning Bank (Latest)")
        for item in bank[-5:]:
            lines.append(f"- ({item.get('type', '')}) {item.get('title', '')} â€” {item.get('description', '')}")
        return "\n".join(lines)

    def _estimate_chars(self, memory: dict[str, Any]) -> int:
        """ç²—ç•¥ä¼°è®¡è®°å¿†ä½“é‡ï¼ˆæŒ‰å­—ç¬¦è®¡ï¼‰ã€‚"""
        try:
            return len(json.dumps(memory, ensure_ascii=False))
        except Exception:
            return 0

    def _format_metrics_delta(self, perf_old: float | None, perf_new: float | None) -> str:
        """å°†æ€§èƒ½å˜åŒ–æ ¼å¼åŒ–ä¸ºæ˜“è¯»å­—ç¬¦ä¸²ã€‚"""
        try:
            if perf_old is None or perf_new is None:
                return "N/A"
            if math.isinf(perf_old) and not math.isinf(perf_new):
                return f"Runtime: inf -> {perf_new}"
            if math.isinf(perf_new):
                return f"Runtime: {perf_old} -> inf"
            delta = perf_new - perf_old
            pct = (delta / perf_old * 100.0) if perf_old and not math.isinf(perf_old) else None
            if pct is None:
                return f"Runtime: {perf_old} -> {perf_new}"
            sign = "+" if pct >= 0 else ""
            return f"Runtime: {perf_old} -> {perf_new} ({sign}{pct:.1f}%)"
        except Exception:
            return "N/A"

    def _build_extraction_prompts(
        self,
        perf_old: float | None,
        perf_new: float | None,
        code_diff: str,
        current_directions: list[dict[str, Any]],
    ) -> tuple[str, str]:
        """
        æ„é€ è®°å¿†æç‚¼çš„ System/User æç¤ºè¯ã€‚
        """
        outcome = "SUCCESS" if (perf_old is not None and perf_new is not None and perf_new < perf_old) else "FAILURE"
        metrics_line = self._format_metrics_delta(perf_old, perf_new)
        sys = """You are the Memory Manager for an evolutionary coding agent.

Return a JSON object with keys `updated_directions` and `new_memory_item`.
Do not include commentary outside JSON.
"""
        user = (
            "## Context\n"
            f"- {metrics_line} [Outcome: {outcome}]\n\n"
            "## Inputs\n"
            "1. Code Diff:\n" + (code_diff or "N/A") + "\n\n"
            "2. Current Directions (JSON):\n" + json.dumps(current_directions or [], ensure_ascii=False) + "\n\n"
            "## Task\n"
            "1. Update Directions (replace list).\n"
            "2. Create Reasoning Item (or null).\n"
        )
        return sys, user

    def _parse_llm_json(self, text: str) -> dict[str, Any]:
        """æå–å¹¶è§£æ LLM è¿”å›çš„ JSON å†…å®¹ã€‚"""
        content = (text or "").strip()
        if not content:
            return {}
        # ç›´æ¥è§£ææˆ–ä»ä¸‰å¼•å·ä¸­æå–
        try:
            return json.loads(content)
        except Exception:
            start = content.find("{")
            end = content.rfind("}")
            if start >= 0 and end > start:
                frag = content[start : end + 1]
                try:
                    return json.loads(frag)
                except Exception:
                    return {}

    def compress_if_needed(self, memory: dict[str, Any]) -> None:
        """
        å½“è®°å¿†ä½“é‡è¶…è¿‡é˜ˆå€¼æ—¶è¿›è¡Œè§¦å‘å¼å‹ç¼©ï¼š
        - ä¿ç•™ Top-3 Successï¼ˆæŒ‰æå‡å¹…åº¦æ’åºï¼‰ä¸ Top-2 Failureï¼ˆæŒ‰å›å½’å¹…åº¦æ’åºï¼‰
        - åˆå¹¶ç›¸ä¼¼æ¡ç›®ï¼ˆç®€åŒ–ä¸ºæŒ‰æ ‡é¢˜å»é‡ï¼‰
        """
        try:
            if self._estimate_chars(memory) <= self.token_limit:
                return
            bank = memory.get("reasoning_bank") or []
            if not isinstance(bank, list) or not bank:
                return

            def _score(item: dict[str, Any]) -> float:
                md = str(item.get("evidence", {}).get("metrics_delta", ""))
                # ç®€å•è§£æç™¾åˆ†æ¯”ï¼Œå¤±è´¥åˆ™æŒ‰ 0 å¤„ç†
                try:
                    if "%" in md:
                        p = md.split("(")[-1].split("%")[0]
                        return float(p)
                except Exception:
                    pass
                return 0.0

            success = [x for x in bank if str(x.get("type")) == "Success"]
            failure = [x for x in bank if str(x.get("type")) == "Failure"]
            success_sorted = sorted(success, key=_score, reverse=True)[:3]
            failure_sorted = sorted(failure, key=_score)[:2]
            kept = success_sorted + failure_sorted

            # æŒ‰æ ‡é¢˜å»é‡
            seen_titles: set[str] = set()
            deduped: list[dict[str, Any]] = []
            for it in kept + [x for x in bank if x not in kept]:
                title = str(it.get("title") or "").strip()
                if title and title in seen_titles:
                    continue
                seen_titles.add(title)
                deduped.append(it)

            memory["reasoning_bank"] = deduped
        except Exception as e:
            self.logger.warning(f"å‹ç¼©è®°å¿†å¤±è´¥: {e}")

    def extract_and_update(
        self,
        instance_name: str,
        iteration: int,
        summary: dict[str, Any],
        patch_content: str,
        perf_metrics: dict[str, Any] | None = None,
        previous_code: str | None = None,
        current_label: str | None = None,
        operator_name: str | None = None,
    ) -> None:
        """
        æ ¹æ®ä¸€æ¬¡è¿­ä»£çš„æ€»ç»“ä¸æ€§èƒ½æ•°æ®ï¼Œè¿›è¡Œè®°å¿†æç‚¼å¹¶æ›´æ–°æœ¬åœ°è®°å¿†åº“ã€‚

        Args:
            instance_name: å®ä¾‹åç§°ã€‚
            iteration: å½“å‰è¿­ä»£å·ï¼ˆç”¨äºå…¨å±€çŠ¶æ€ï¼‰ã€‚
            summary: è½¨è¿¹æ€»ç»“å­—å…¸ï¼ˆå« approach_summary/analysis/best_strategy ç­‰ï¼‰ã€‚
            patch_content: å½“å‰è¿­ä»£çš„æœ€ç»ˆä»£ç æˆ–å ä½ï¼ˆå¤±è´¥æ—¶å¯èƒ½ä¸º "FAILED_NO_PATCH"ï¼‰ã€‚
            perf_metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸ï¼ˆæ¥è‡ª result.json æå–çš„ç²¾ç®€å­—æ®µï¼‰ã€‚
            previous_code: ä¸Šä¸€æ¬¡è§£çš„ä»£ç ï¼ˆç”¨äºç”Ÿæˆç®€åŒ– diffï¼‰ã€‚
            current_label: å½“å‰æ¡ç›®çš„æ ‡ç­¾ï¼ˆå¦‚ "sol1" / "iterN"ï¼‰ã€‚
            operator_name: ç®—å­åç§°ï¼ˆç”¨äºè®°å½•æ¥æºï¼‰ã€‚
        """
        memory = self.load()
        attempted = memory.get("attempted_directions") or []

        # è®¡ç®—æ€§èƒ½å·®å¼‚ï¼ˆold vs newï¼‰
        perf_old = None
        perf_new = None
        try:
            if perf_metrics:
                # ä¼˜å…ˆä½¿ç”¨ performance_before / final_performance
                po = perf_metrics.get("performance_before")
                pn = perf_metrics.get("final_performance")
                perf_old = float(po) if po is not None else None
                perf_new = float(pn) if pn is not None else None
        except Exception:
            perf_old = None
            perf_new = None

        # æ„é€ ç®€åŒ– diffï¼ˆä»…æ–‡æœ¬çº§ï¼Œé¿å…ä¾èµ–å¤–éƒ¨åº“ï¼‰
        code_diff = ""
        try:
            prev = (previous_code or "").splitlines()
            curr = (patch_content or "").splitlines()
            # ä»…åŒ…å«å¤´å°¾ç‰‡æ®µæé«˜å¯è¯»æ€§
            head = "\n".join(curr[:20])
            tail = "\n".join(curr[-10:]) if len(curr) > 20 else ""
            code_diff = "# New Code (head)\n" + head + ("\n# New Code (tail)\n" + tail if tail else "")
            if prev:
                phead = "\n".join(prev[:10])
                code_diff = "# Old Code (head)\n" + phead + "\n\n" + code_diff
        except Exception:
            code_diff = patch_content or ""

        # LLM æç‚¼ï¼šæ›´æ–° Directions + ç”Ÿæˆ Reasoning Item
        updated_dirs: list[dict[str, Any]] = attempted
        new_item: dict[str, Any] | None = None
        if self.llm_client:
            try:
                sys_prompt, user_prompt = self._build_extraction_prompts(perf_old, perf_new, code_diff, attempted)
                resp = self.llm_client.call_with_system_prompt(
                    system_prompt=sys_prompt,
                    user_prompt=user_prompt,
                    temperature=0.3,
                    max_tokens=3000,
                    usage_context="local_memory_manager",
                )
                parsed = self._parse_llm_json(resp)
                if isinstance(parsed.get("updated_directions"), list):
                    updated_dirs = parsed["updated_directions"]
                nmi = parsed.get("new_memory_item")
                if isinstance(nmi, dict):
                    new_item = nmi
            except Exception as e:
                self.logger.warning(f"LLMè®°å¿†æç‚¼å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å›é€€: {e}")

        # è§„åˆ™å›é€€ï¼šè‹¥æœªç”Ÿæˆé¡¹ï¼Œåˆ™æ ¹æ® perf_diff ç®€å•è¿½åŠ ä¸€æ¡ç»éªŒ
        if new_item is None:
            try:
                perf_line = self._format_metrics_delta(perf_old, perf_new)
                itype = (
                    "Success" if (perf_old is not None and perf_new is not None and perf_new < perf_old) else "Failure"
                )
                new_item = {
                    "type": itype,
                    "title": "Performance Change",
                    "description": "Observed performance change between iterations.",
                    "content": summary.get("approach_summary") or "",
                    "related_operator": operator_name or "unknown",
                    "source_ref": {"generation": iteration, "solution_id": current_label, "instance": instance_name},
                    "evidence": {
                        "code_change": "see diff head/tail",
                        "metrics_delta": perf_line,
                        "context": summary.get("analysis", {}).get("best_strategy", {}).get("high_level")
                        if isinstance(summary.get("analysis"), dict)
                        else None,
                    },
                }
            except Exception:
                pass

        # å†™å›ï¼šæ›´æ–° Directionsï¼ˆå…¨é‡æ›¿æ¢ï¼‰ä¸è¿½åŠ  Reasoning é¡¹
        memory["attempted_directions"] = list(updated_dirs or [])
        if isinstance(new_item, dict):
            bank = memory.get("reasoning_bank") or []
            bank.append(new_item)
            memory["reasoning_bank"] = bank

        # æ›´æ–°å…¨å±€çŠ¶æ€
        gs = memory.get("global_status") or {}
        gs["current_generation"] = int(iteration)
        try:
            approach = summary.get("solution_name") or summary.get("strategy") or summary.get("approach_summary")
        except Exception:
            approach = None
        gs["current_approach"] = approach

        # ç»´æŠ¤æœ€ä½³æ€§èƒ½
        try:
            current_best = gs.get("best_runtime")
            cb_val = float(current_best) if current_best is not None else float("inf")
        except Exception:
            cb_val = float("inf")
        try:
            if perf_new is not None and float(perf_new) < cb_val:
                gs["best_runtime"] = float(perf_new)
                gs["best_solution_id"] = current_label or f"iter_{iteration}"
        except Exception:
            pass
        memory["global_status"] = gs

        # å‹ç¼©ï¼ˆå¿…è¦æ—¶ï¼‰å¹¶ä¿å­˜
        self.compress_if_needed(memory)
        self.save(memory)
        self.logger.info(
            json.dumps(
                {
                    "memory_update": {
                        "instance": instance_name,
                        "iteration": iteration,
                        "label": current_label,
                        "best_runtime": memory.get("global_status", {}).get("best_runtime"),
                    }
                },
                ensure_ascii=False,
            )
        )
