"""
PerfAgent æ ¸å¿ƒç±»

å®ç°ä»£ç æ€§èƒ½ä¼˜åŒ–çš„ä¸»è¦é€»è¾‘ï¼ŒåŒ…æ‹¬è¿­ä»£ä¼˜åŒ–ã€diff åº”ç”¨ã€æ€§èƒ½è¯„ä¼°ç­‰åŠŸèƒ½ã€‚
"""

import json
import logging
import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

from .config import PerfAgentConfig
from .diff_applier import DiffApplier
from .effibench.benchmark import run_performance_benchmark
from .llm_client import LLMClient
from .trajectory import TrajectoryLogger
from .utils.log import get_se_logger


@dataclass
class EffiBenchXInstance:
    id: str
    title: str
    title_slug: str
    description: str
    description_md: str
    source: str
    url: str
    type: str
    starter_code: str | None = None
    solutions: dict[str, dict[str, str]] = field(default_factory=dict)
    language: str | None = None
    generated_tests: list[dict[str, Any]] = field(default_factory=list)
    evaluator: str | None = None
    # ä»»åŠ¡åï¼ˆæ¥æºäºå®ä¾‹æ–‡ä»¶åï¼Œä¸å«æ‰©å±•åï¼‰
    task_name: str | None = None

    @staticmethod
    def from_dict(data: dict[str, Any]) -> "EffiBenchXInstance":
        # Robustly parse generated_tests when it can be a list or a JSON string
        gt_raw = data.get("generated_tests", [])
        if isinstance(gt_raw, str):
            try:
                gt_parsed = json.loads(gt_raw)
            except Exception:
                gt_parsed = []
        elif isinstance(gt_raw, list):
            gt_parsed = gt_raw
        else:
            gt_parsed = []

        return EffiBenchXInstance(
            id=str(data.get("id", "unknown")),
            title=data.get("title", ""),
            title_slug=data.get("title_slug", ""),
            description=data.get("description", ""),
            description_md=data.get("description_md", ""),
            source=data.get("source", ""),
            url=data.get("url", ""),
            type=data.get("type", ""),
            starter_code=data.get("starter_code"),
            solutions=data.get("solutions", {}),
            language=data.get("language"),
            generated_tests=gt_parsed,
            evaluator=data.get("evaluator"),
        )

    def to_dict(self) -> dict[str, Any]:
        return {
            "id": self.id,
            "title": self.title,
            "title_slug": self.title_slug,
            "description": self.description,
            "description_md": self.description_md,
            "source": self.source,
            "url": self.url,
            "type": self.type,
            "starter_code": self.starter_code,
            "solutions": self.solutions,
            "language": self.language,
            "generated_tests": self.generated_tests,
            "evaluator": self.evaluator,
            "task_name": self.task_name,
        }


class PerfAgent:
    """æ€§èƒ½ä¼˜åŒ– Agent"""

    def __init__(self, config: PerfAgentConfig):
        self.config = config

        # ç®€åŒ–é€»è¾‘ï¼šå‡­æ®å­˜åœ¨å³åˆå§‹åŒ– LLMClientï¼Œæ— éœ€ use_llm æ ‡å¿—
        self.llm_client = None
        if self.config.model.api_base and self.config.model.api_key:
            client_cfg = {
                "name": self.config.model.name,
                "api_base": self.config.model.api_base,
                "api_key": self.config.model.api_key,
                "max_output_tokens": self.config.model.max_output_tokens,
                "request_timeout": self.config.model.request_timeout,
                "max_retries": self.config.model.max_retries,
                "retry_delay": self.config.model.retry_delay,
                "log_inputs_outputs": self.config.model.log_inputs_outputs,
                "log_sanitize": self.config.model.log_sanitize,
            }
            # å°† LLM I/O ç‹¬ç«‹å†™å…¥ log_dir/llm_io.log
            io_log_file = Path(self.config.logging.log_dir) / "llm_io.log"
            self.llm_client = LLMClient(
                client_cfg,
                io_log_path=io_log_file,
                log_inputs_outputs=self.config.model.log_inputs_outputs,
                log_sanitize=self.config.model.log_sanitize,
                request_timeout=self.config.model.request_timeout,
            )

        self.diff_applier = DiffApplier()

        # è®¾ç½®æ—¥å¿—ï¼šç»Ÿä¸€ç»‘å®šåˆ°å•ä¸€æ–‡ä»¶
        # ä½¿ç”¨åŒ…å«æ—¥å¿—ç›®å½•åçš„å”¯ä¸€ logger åç§°ï¼Œé¿å…å¹¶å‘å®ä¾‹å¤ç”¨åŒåå¯¼è‡´ä¸²å†™
        agent_logger_name = f"perfagent.agent.{Path(self.config.logging.log_dir).name}"
        get_se_logger(
            agent_logger_name,
            Path(self.config.logging.log_dir) / "perfagent.log",
            emoji="ğŸ”§",
            level=getattr(logging, self.config.logging.log_level.upper()),
            also_stream=False,
        )
        self.logger = logging.getLogger(agent_logger_name)

        # ä¼˜åŒ–å†å²
        self.optimization_history: list[dict[str, Any]] = []

        # åˆå§‹ä»£ç æ¥æºï¼š"default" | "text" | "dir"
        self._initial_code_source: str = "default"

    def _normalize_language(self, lang: str | None) -> str:
        # æ ‡å‡†åŒ–è¯­è¨€åç§°
        if not lang:
            return "python3"
        l = lang.lower()
        if l in ("python", "py", "python3"):
            return "python3"
        if l in ("cpp", "c++", "cxx"):
            return "cpp"
        if l in ("javascript", "js"):
            return "javascript"
        if l in ("java",):
            return "java"
        return l

    def _get_default_placeholder(self, language: str | None = None) -> str:
        """è·å–é»˜è®¤å ä½ç¬¦ä»£ç ï¼ˆæ ¹æ®è¯­è¨€ï¼‰"""
        lang = self._normalize_language(language or self.config.language_cfg.language)
        placeholder_map = {
            "python3": "# Start your code here\n",
            "cpp": "// Start your code here\n",
            "java": "// Start your code here\n",
            "javascript": "// Start your code here\n",
            "golang": "// Start your code here\n",
        }
        return placeholder_map.get(lang, "# Start your code here\n")

    def _extract_initial_code(
        self, instance: EffiBenchXInstance, language: str | None = None, optimization_target: str | None = None
    ) -> str:
        """ä»é…ç½®/æ–‡ä»¶ç³»ç»Ÿæ³¨å…¥æˆ–ç”Ÿæˆåˆå§‹ä»£ç ã€‚

        ä¼˜å…ˆçº§ï¼š
        1) é…ç½® overrides.initial_code_textï¼ˆç›´æ¥æ–‡æœ¬ï¼‰
        2) é…ç½® overrides.initial_code_dirï¼ˆæŒ‰å®ä¾‹ååŒ¹é…æ–‡ä»¶ï¼‰
        3) é»˜è®¤å ä½ç¬¦ä»£ç ï¼ˆæ ¹æ®è¯­è¨€ï¼‰
        """
        try:
            # é»˜è®¤æ¥æº
            self._initial_code_source = "default"
            # 1) ç›´æ¥æ–‡æœ¬è¦†ç›–
            override_text = getattr(getattr(self.config, "overrides", None), "initial_code_text", None)
            if isinstance(override_text, str) and override_text.strip():
                self._initial_code_source = "text"
                return override_text if override_text.endswith("\n") else override_text + "\n"

            # 2) ç›®å½•è¦†ç›–ï¼ˆæŒ‰å®ä¾‹ååŒ¹é…æ–‡ä»¶ï¼‰
            code_dir = getattr(getattr(self.config, "overrides", None), "initial_code_dir", None)
            task_name = getattr(instance, "task_name", None) or getattr(instance, "id", None)
            if code_dir and task_name:
                lang = self._normalize_language(language or self.config.language_cfg.language)
                # è¯­è¨€æ‰©å±•æ˜ å°„
                ext_map = {
                    "python3": [".py"],
                    "cpp": [".cpp", ".cc", ".cxx"],
                    "java": [".java"],
                    "javascript": [".js", ".mjs"],
                    "golang": [".go"],
                }
                candidates: list[Path] = []
                for ext in ext_map.get(lang, []):
                    candidates.append(Path(code_dir) / f"{task_name}{ext}")
                # é€€åŒ–ï¼šä»»æ„åŒ¹é…åŒåæ–‡ä»¶ï¼ˆä¸åŒºåˆ†æ‰©å±•åï¼‰
                try:
                    for fp in Path(code_dir).iterdir():
                        if fp.is_file() and fp.stem == task_name and fp not in candidates:
                            candidates.append(fp)
                except Exception:
                    pass

                for fp in candidates:
                    try:
                        if fp.exists():
                            code = fp.read_text(encoding="utf-8")
                            if isinstance(code, str) and code.strip():
                                self.logger.info(f"ä½¿ç”¨è¦†ç›–åˆå§‹ä»£ç : {fp}")
                                self._initial_code_source = "dir"
                                return code if code.endswith("\n") else code + "\n"
                    except Exception as e:
                        self.logger.warning(f"è¯»å–åˆå§‹ä»£ç æ–‡ä»¶å¤±è´¥ {fp}: {e}")
        except Exception as e:
            # è¦†ç›–æµç¨‹å¤±è´¥åˆ™å›é€€åˆ°å ä½ç¬¦
            self.logger.warning(f"åˆå§‹ä»£ç è¦†ç›–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å ä½ç¬¦: {e}")

        # 3) é»˜è®¤å ä½ç¬¦ï¼ˆä¿æŒç°æœ‰æµ‹è¯•å…¼å®¹ï¼‰
        return self._get_default_placeholder(language)

    def _prepare_test_cases(self, instance: EffiBenchXInstance) -> list[dict[str, Any]]:
        """å‡†å¤‡æµ‹è¯•ç”¨ä¾‹ï¼ˆå®ä¾‹ä»…ä¸º dataclassï¼‰"""
        return instance.generated_tests or []

    def _detect_language(self, instance: EffiBenchXInstance) -> str:
        """æ£€æµ‹ç¼–ç¨‹è¯­è¨€ï¼ˆä»…ä¿ç•™ä»¥å…¼å®¹è°ƒç”¨è·¯å¾„ï¼Œä½†ä¸ä½¿ç”¨ï¼‰"""
        return self._normalize_language(self.config.language_cfg.language)

    def _evaluate_performance(
        self, language: str, code: str, test_cases: list[dict[str, Any]], instance: Any
    ) -> dict[str, Any]:
        """è¯„ä¼°ä»£ç æ€§èƒ½ï¼Œä¿æŒå‚æ•°å…¼å®¹"""

        # å¦‚æœä»£ç ä¸å ä½ç¬¦ä»£ç ç›¸åŒï¼Œè¿”å›é»˜è®¤å¤±è´¥ç»“æ„
        if code == self._get_default_placeholder(language):
            perf = {
                "original_n": 0,
                "n": 0,
                "runtime": float("inf"),
                "memory": float("inf"),
                "integral": float("inf"),
                "analysis": {
                    "runtime": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                    "memory": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                    "integral": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                },
            }
            return {
                "performance_analysis": perf,
                "first_run_details": [],
                "failed_submission_exit_codes": [],
                "pass_rates": [],
                "pass_rate_consistent": True,
            }

        # è‹¥ evaluator æˆ–æµ‹è¯•ç”¨ä¾‹ç¼ºå¤±/æ ¼å¼ä¸åˆæ³•ï¼Œç›´æ¥è¿”å›é»˜è®¤ç»“æ„ä»¥é¿å…é•¿æ—¶é—´çš„åç«¯è°ƒç”¨
        evaluator = getattr(instance, "evaluator", None)
        tc_valid = bool(test_cases) and isinstance(test_cases, list) and isinstance(test_cases[0], dict)
        if not evaluator or not tc_valid:
            perf = {
                "original_n": 0,
                "n": 0,
                "runtime": float("inf"),
                "memory": float("inf"),
                "integral": float("inf"),
                "analysis": {
                    "runtime": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                    "memory": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                    "integral": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                },
            }
            return {
                "performance_analysis": perf,
                "first_run_details": [],
                "failed_test_details": [],
                "failed_submission_exit_codes": [],
                "pass_rates": [],
                "pass_rate_consistent": True,
            }

        # çº§è”è¯„ä¼°ï¼šå…ˆç”¨ benchmark è¿›è¡Œä¸€æ¬¡è¿è¡Œï¼ˆnum_runs=1ï¼‰ï¼Œè‹¥æœªå…¨éƒ¨é€šè¿‡åˆ™ç›´æ¥è¿”å›
        try:
            single_run_summary = run_performance_benchmark(
                lang=language,
                solution=code,
                test_cases=test_cases,
                evaluator=evaluator,
                num_runs=1,
                time_limit=self.config.runtime.time_limit,
                memory_limit=self.config.runtime.memory_limit,
                trim_ratio=self.config.runtime.trim_ratio,
                max_workers=self.config.runtime.max_workers,
            )
        except Exception as e:
            # å•æ¬¡è¿è¡Œå¤±è´¥åˆ™å›é€€åˆ°é»˜è®¤å¤±è´¥ç»“æ„ï¼Œä¿æŒä¸ç°æœ‰æµ‹è¯•å…¼å®¹
            self.logger.warning(f"å•æ¬¡è¿è¡Œè¯„ä¼°å¤±è´¥ï¼Œè¿”å›é»˜è®¤æ€§èƒ½ç»“æ„: {e}")
            perf = {
                "original_n": 0,
                "n": 0,
                "runtime": float("inf"),
                "memory": float("inf"),
                "integral": float("inf"),
                "analysis": {
                    "runtime": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                    "memory": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                    "integral": {
                        "original_n": 0,
                        "n": 0,
                        "mean": float("inf"),
                        "std": float("inf"),
                        "min": float("inf"),
                        "max": float("inf"),
                        "max_diff": float("inf"),
                        "95%_CI": (float("inf"), float("inf")),
                        "trimmed_mean": float("inf"),
                    },
                },
            }
            return {
                "performance_analysis": perf,
                "first_run_details": [],
                "failed_test_details": [],
                "pass_rates": [],
                "pass_rate_consistent": True,
            }

        # è®¡ç®—å•æ¬¡è¿è¡Œé€šè¿‡ç‡ï¼ˆä¼˜å…ˆä½¿ç”¨è¿”å›çš„ pass_ratesï¼‰
        pr_list = single_run_summary.get("pass_rates", [])
        if pr_list:
            single_pass_rate = float(pr_list[0])
        else:
            try:
                first_run_details = single_run_summary.get("first_run_details", [])
                total_cases = len(first_run_details) if first_run_details else 0
                num_passed = sum(1 for tc in (first_run_details or []) if tc.get("passed", False))
                single_pass_rate = num_passed / total_cases if total_cases > 0 else 0.0
            except Exception:
                single_pass_rate = 0.0

        # è‹¥æœªå…¨éƒ¨é€šè¿‡ï¼Œç›´æ¥è¿”å›å•æ¬¡è¿è¡Œçš„ç»“æœï¼ˆä¸è¿›è¡Œå¤šæ¬¡æ€§èƒ½è¯„ä¼°ï¼‰
        if single_pass_rate < 1.0:
            return single_run_summary

        # æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼Œè¿›è¡Œæ­£å¼çš„å¤šæ¬¡æ€§èƒ½è¯„ä¼°
        try:
            result = run_performance_benchmark(
                lang=language,
                solution=code,
                test_cases=test_cases,
                evaluator=evaluator,
                num_runs=self.config.runtime.num_runs,
                time_limit=self.config.runtime.time_limit,
                memory_limit=self.config.runtime.memory_limit,
                trim_ratio=self.config.runtime.trim_ratio,
                max_workers=self.config.runtime.max_workers,
            )
            return result
        except Exception as e:
            self.logger.error(f"æ€§èƒ½è¯„ä¼°å¤±è´¥: {e}")
            return {
                "performance_analysis": {"trimmed_mean": float("inf")},
                "first_run_details": [],
                "failed_test_details": [],
                "pass_rates": [],
                "pass_rate_consistent": False,
            }

    def run(self, instance: EffiBenchXInstance) -> dict[str, Any]:
        """è¿è¡Œæ€§èƒ½ä¼˜åŒ–æµç¨‹ï¼ˆä»…ä½¿ç”¨é…ç½®è¯­è¨€ï¼Œå®ä¾‹ä¸º dataclassï¼‰"""
        inst = instance
        # ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶åï¼ˆtask_nameï¼‰ä½œä¸ºå®ä¾‹ IDï¼Œè‹¥ä¸å­˜åœ¨åˆ™å›é€€åˆ° JSON ä¸­çš„ id
        instance_id = getattr(inst, "task_name", None) or getattr(inst, "id", "unknown")

        # åˆå§‹åŒ–è½¨è¿¹è®°å½•å™¨ï¼ˆç»Ÿä¸€æ—¥å¿—ç›®å½•åˆ° config.logging.log_dirï¼‰
        trajectory = TrajectoryLogger(
            instance_id,
            self.config.logging.trajectory_dir,
            log_dir=self.config.logging.log_dir,
        )

        try:
            self.logger.info(f"å¼€å§‹ä¼˜åŒ–å®ä¾‹: {instance_id}")

            # ä½¿ç”¨é…ç½®ä¸­çš„è¯­è¨€
            language = self._normalize_language(self.config.language_cfg.language)

            # è®¾ç½®è½¨è¿¹è¯­è¨€ä¸ä¼˜åŒ–æ–¹å‘
            trajectory.metadata.language = language
            trajectory.metadata.optimization_target = self.config.optimization.target

            # å°†ç³»ç»Ÿæç¤ºåœ¨å¯¹è¯å†å²æœ€å¼€å¤´è®°å½•ä¸€æ¬¡
            system_prompt_header = self._build_system_prompt(
                language=language,
                optimization_target=self.config.optimization.target,
                task_description=inst.description_md,
            )
            trajectory.add_history(role="system", content=system_prompt_header, message_type="system_prompt")

            # æå–åˆå§‹ä»£ç ä¸æµ‹è¯•ç”¨ä¾‹
            initial_code = self._extract_initial_code(
                inst, language=language, optimization_target=self.config.optimization.target
            )
            test_cases = self._prepare_test_cases(inst)

            if not initial_code:
                raise ValueError("æ— æ³•æå–åˆå§‹ä»£ç ")

            # è‹¥æ¥å—äº†å¤–éƒ¨åˆå§‹ä»£ç ï¼ˆæ–‡æœ¬æˆ–ç›®å½•ï¼‰ï¼Œåˆ™åˆå§‹è¯„ä¼°è®¡ä¸ºç¬¬1æ¬¡è¿­ä»£
            iter_offset = 1 if self._initial_code_source in ("text", "dir") else 0

            # åˆå§‹åŒ–å½“å‰ä»£ç ä¸æœ€ä½³æ€§èƒ½
            current_code = initial_code
            best_performance = float("inf")
            best_code = initial_code
            latest_optimized_code = current_code

            # è¯„ä¼°åˆå§‹æ€§èƒ½
            step_id = trajectory.start_step(
                "initial_evaluation", query="Evaluate the initial code performance.", code_snapshot=current_code
            )
            initial_performance = self._evaluate_performance(language, current_code, test_cases, inst)
            initial_evaluation_summary = {
                "performance_analysis": initial_performance.get("performance_analysis", {}),
                "failed_test_details": initial_performance.get("failed_test_details", [])[:3],
                "pass_rates": initial_performance.get("pass_rates", []),
                "pass_rate_consistent": initial_performance.get("pass_rate_consistent", False),
            }
            initial_summary_text = self._build_summary_text(
                iteration=1 if iter_offset else 0,
                code_changed=False,
                diff_text=None,
                benchmark_results=initial_performance,
                current_program=current_code,
            )
            trajectory.end_step(
                step_id,
                response=initial_summary_text,
                thought="æ”¶é›†åˆå§‹æ€§èƒ½åŸºçº¿ä»¥æŒ‡å¯¼åç»­ä¼˜åŒ–",
                code_changed=False,
                performance_metrics=initial_evaluation_summary,
                code_snapshot=current_code,
            )

            def _extract_pass_rate(results: dict[str, Any]) -> float:
                pr_list = results.get("pass_rates") or []
                try:
                    if isinstance(pr_list, list) and pr_list:
                        return float(min(float(p) for p in pr_list))
                except Exception:
                    pass
                try:
                    fr = results.get("first_run_details") or []
                    total = len(fr)
                    passed = sum(1 for tc in fr if tc.get("passed", False))
                    return (passed / total) if total > 0 else 0.0
                except Exception:
                    return 0.0

            best_pass_rate = _extract_pass_rate(initial_performance)
            target = self.config.optimization.target
            init_metric = initial_evaluation_summary["performance_analysis"].get(target, float("inf"))
            if init_metric <= best_performance:
                best_performance = init_metric
                best_code = current_code

            # è®°å½•å½“å‰ä»£ç å¯¹åº”çš„æœ€æ–°è¯„ä¼°ç»“æœï¼ˆç”¨äºæç¤ºæ„é€ ï¼‰
            current_benchmark_results = initial_performance

            # è¿­ä»£ä¼˜åŒ–
            no_improve_count = 0  # è¿ç»­æœªæ”¹è¿›è®¡æ•°ï¼ˆè·¨è¿­ä»£ç´¯ç§¯ï¼‰

            # ä¸»è¿­ä»£å¾ªç¯
            # è‹¥å­˜åœ¨å¤–éƒ¨åˆå§‹ä»£ç ï¼ˆæ–‡æœ¬æˆ–ç›®å½•ï¼‰ï¼Œåˆå§‹è¯„ä¼°è®°ä¸ºç¬¬1æ¬¡è¿­ä»£ï¼Œä¼˜åŒ–å¾ªç¯æ¬¡æ•°ç›¸åº”å‡ä¸€
            remaining_iterations = max(0, self.config.max_iterations - iter_offset)
            for iteration in range(remaining_iterations):
                self.logger.info(f"å¼€å§‹ç¬¬ {iteration + 1 + iter_offset} æ¬¡è¿­ä»£")

                # ç”Ÿæˆä¼˜åŒ–å»ºè®®
                opt_prompt = self._build_optimization_prompt(
                    current_program=current_code,
                    language=language,
                    benchmark_results=current_benchmark_results,
                )
                step_id = trajectory.start_step(
                    "generate_optimization",
                    query=opt_prompt,
                    code_snapshot=current_code,
                )

                # multi-turn chat: æ„é€ æ¶ˆæ¯åºåˆ—ï¼ˆä¿ç•™æœ€è¿‘ä¼šè¯ä¸Šä¸‹æ–‡ï¼‰
                system_prompt = self._build_system_prompt(
                    language=language,
                    optimization_target=self.config.optimization.target,
                    task_description=inst.description_md,
                )
                messages = self._build_messages(system_prompt, trajectory.history, opt_prompt)

                if self.llm_client:
                    optimization_response = self.llm_client.call_llm(
                        messages,
                        temperature=self.config.model.temperature,
                        max_tokens=self.config.model.max_output_tokens,
                        usage_context="perfagent.optimize",
                    )
                else:
                    # ä¿å®ˆå›é€€ï¼šLLM æœªé…ç½®æ—¶è¿”å›ç©ºå»ºè®®ï¼Œé¿å…å¼•å…¥æ— æ•ˆ diff
                    optimization_response = "LLM æœªé…ç½®æˆ–ä¸å¯ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡ä¼˜åŒ–å»ºè®®ã€‚è¯·æ£€æŸ¥ API é…ç½®ã€‚"

                # æå–ä»£ç å˜æ›´
                diff_text = None
                optimized_code = None

                if self.config.optimization.code_generation_mode == "direct":
                    optimized_code = self._extract_full_code_from_response(optimization_response)
                    if not optimized_code:
                        summary_text = self._build_summary_text(
                            iteration=iteration + 1 + iter_offset,
                            code_changed=False,
                            diff_text=None,
                            benchmark_results=None,
                            current_program=current_code,
                            error_message="æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„å®Œæ•´ä»£ç ",
                        )
                        trajectory.end_step(
                            step_id,
                            response=optimization_response,
                            thought="æœªèƒ½æå–æœ‰æ•ˆçš„å®Œæ•´ä»£ç åŒºå—",
                            code_changed=False,
                            diff=None,
                            error="æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„å®Œæ•´ä»£ç ",
                            code_snapshot=current_code,
                            summary=summary_text,
                        )
                        continue
                else:
                    # æå– diff
                    diff_text = self._extract_diff_from_response(optimization_response)

                    if not diff_text:
                        summary_text = self._build_summary_text(
                            iteration=iteration + 1 + iter_offset,
                            code_changed=False,
                            diff_text=None,
                            benchmark_results=None,
                            current_program=current_code,
                            error_message="æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„ diff",
                        )
                        trajectory.end_step(
                            step_id,
                            response=optimization_response,
                            thought="æœªèƒ½æå–æœ‰æ•ˆçš„ SEARCH/REPLACE åŒºå—",
                            code_changed=False,
                            diff=None,
                            error="æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„ diff",
                            code_snapshot=current_code,
                            summary=summary_text,
                        )
                        continue

                # åº”ç”¨å˜æ›´
                try:
                    if self.config.optimization.code_generation_mode == "diff":
                        optimized_code = self.diff_applier.apply_diff(current_code, diff_text)

                    # å¦‚æœä»£ç æœªå‘ç”Ÿå˜åŒ–ï¼Œä»…ç»“æŸè¯¥æ­¥éª¤å¹¶è·³è¿‡è¿­ä»£
                    if optimized_code == current_code:
                        summary_text = self._build_summary_text(
                            iteration=iteration + 1 + iter_offset,
                            code_changed=False,
                            diff_text=diff_text,
                            benchmark_results=current_benchmark_results,
                            current_program=current_code,
                        )
                        trajectory.end_step(
                            step_id,
                            response=optimization_response,
                            thought="diff åº”ç”¨åä»£ç æœªå˜åŒ–ï¼Œè·³è¿‡",
                            code_changed=False,
                            diff=diff_text,
                            code_snapshot=current_code,
                            summary=summary_text,
                        )
                        self.logger.warning("ä»£ç æœªå‘ç”Ÿå˜åŒ–ï¼Œè·³è¿‡æ­¤æ¬¡è¿­ä»£")
                        # è®°å½•æœªæ”¹è¿›ä¸€æ¬¡å¹¶æ£€æŸ¥æ—©åœ
                        no_improve_count += 1
                        if self.config.early_stop_no_improve and no_improve_count >= self.config.early_stop_no_improve:
                            self.logger.info(f"è¿ç»­æœªæ”¹è¿›è¾¾åˆ°é˜ˆå€¼ {self.config.early_stop_no_improve}ï¼Œæå‰åœæ­¢ã€‚")
                            break
                        continue

                    # è¯„ä¼°ä¼˜åŒ–åçš„æ€§èƒ½ï¼Œå¹¶å°†ç»“æœä½œä¸º performance_metrics é™„åŠ åˆ° generate_optimization
                    try:
                        latest_optimized_code = optimized_code
                        self.logger.info("å¼€å§‹è¯„ä¼°ä¼˜åŒ–åçš„ä»£ç æ€§èƒ½")
                        performance_result = self._evaluate_performance(language, optimized_code, test_cases, inst)

                        target = self.config.optimization.target
                        current_performance = performance_result.get("performance_analysis", {}).get(
                            target, float("inf")
                        )
                        current_pass_rate = _extract_pass_rate(performance_result)

                        # ä»…ä¿ç•™æ ¸å¿ƒè¯„ä¼°ç»“æœ
                        evaluation_summary = {
                            "performance_analysis": performance_result.get("performance_analysis", {}),
                            "failed_test_details": performance_result.get("failed_test_details", [])[:3],
                            "pass_rates": performance_result.get("pass_rates", []),
                            "pass_rate_consistent": performance_result.get("pass_rate_consistent", False),
                        }

                        # è®°å½•ä¼˜åŒ–å†å²
                        self.optimization_history.append(
                            {
                                "iteration": iteration + 1 + iter_offset,
                                "diff": diff_text,
                                "performance_before": best_performance,
                                "performance_after": current_performance,
                                "improvement": best_performance - current_performance,
                                # å¼ºåˆ¶è½¬æ¢ä¸º Python boolï¼Œé¿å… numpy.bool_ å¯¼è‡´ JSON åºåˆ—åŒ–é”™è¯¯
                                "success": bool(
                                    (current_pass_rate > best_pass_rate)
                                    or (
                                        current_pass_rate == best_pass_rate
                                        and current_pass_rate == 1.0
                                        and current_performance < best_performance
                                    )
                                ),
                            }
                        )

                        improved = False
                        if current_pass_rate > best_pass_rate:
                            improved = True
                        elif (
                            current_pass_rate == best_pass_rate
                            and current_pass_rate == 1.0
                            and current_performance < best_performance
                        ):
                            improved = True

                        if improved:
                            best_pass_rate = current_pass_rate
                            best_performance = current_performance
                            best_code = optimized_code
                            self.logger.info(
                                f"é‡‡ç”¨æ›´ä¼˜ä»£ç : pass_rate {best_pass_rate:.2f}, {target} {best_performance:.4f}"
                            )
                            no_improve_count = 0
                        else:
                            self.logger.info(
                                f"æœªæ”¹è¿›: pass_rate {current_pass_rate:.2f} vs {best_pass_rate:.2f}; {target} {current_performance:.4f} vs {best_performance:.4f}"
                            )
                            no_improve_count += 1

                        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦é‡‡ç”¨ä¼˜åŒ–åçš„ä»£ç ä½œä¸ºä¸‹ä¸€è½®åŸºç¡€
                        if self.config.optimization.adopt_only_if_improved:
                            if improved:
                                current_code = optimized_code
                            else:
                                current_code = best_code
                        else:
                            current_code = optimized_code
                        # æ›´æ–°æœ€æ–°è¯„ä¼°ç»“æœï¼Œä¾›ä¸‹ä¸€è½®æç¤ºç”Ÿæˆä½¿ç”¨
                        current_benchmark_results = performance_result

                        adopted = True
                        if self.config.optimization.adopt_only_if_improved:
                            adopted = improved
                        summary_text = self._build_summary_text(
                            iteration=iteration + 1 + iter_offset,
                            code_changed=adopted,
                            diff_text=diff_text,
                            benchmark_results=performance_result,
                            current_program=current_code,
                        )
                        trajectory.end_step(
                            step_id,
                            response=optimization_response,
                            thought=("åº”ç”¨ diff å¹¶å®Œæˆæ€§èƒ½è¯„ä¼°" if adopted else "è¯„ä¼°æœªæ”¹è¿›ï¼Œæœªé‡‡ç”¨ä¼˜åŒ–"),
                            code_changed=adopted,
                            diff=diff_text,
                            performance_metrics=evaluation_summary,
                            code_snapshot=current_code,
                            summary=summary_text,
                        )

                        # æ—©åœæ£€æŸ¥ï¼ˆè¯„ä¼°åï¼‰
                        if self.config.early_stop_no_improve and no_improve_count >= self.config.early_stop_no_improve:
                            self.logger.info(f"è¿ç»­æœªæ”¹è¿›è¾¾åˆ°é˜ˆå€¼ {self.config.early_stop_no_improve}ï¼Œæå‰åœæ­¢ã€‚")
                            break

                    except Exception as e:
                        summary_text = self._build_summary_text(
                            iteration=iteration + 1,
                            code_changed=True,
                            diff_text=diff_text,
                            benchmark_results=None,
                            current_program=current_code,
                            error_message=f"æ€§èƒ½è¯„ä¼°å¤±è´¥: {e}",
                        )
                        trajectory.end_step(
                            step_id,
                            response=optimization_response,
                            thought="æ€§èƒ½è¯„ä¼°é˜¶æ®µå‘ç”Ÿå¼‚å¸¸",
                            code_changed=True,
                            diff=diff_text,
                            performance_metrics=None,
                            error=f"æ€§èƒ½è¯„ä¼°å¤±è´¥: {e}",
                            code_snapshot=current_code,
                            summary=summary_text,
                        )
                        continue

                except Exception as e:
                    summary_text = self._build_summary_text(
                        iteration=iteration + 1 + iter_offset,
                        code_changed=False,
                        diff_text=diff_text,
                        benchmark_results=None,
                        current_program=current_code,
                        error_message=f"åº”ç”¨ diff å¤±è´¥: {e}",
                    )
                    trajectory.end_step(
                        step_id,
                        response=optimization_response,
                        thought="åº”ç”¨ diff é˜¶æ®µå‘ç”Ÿå¼‚å¸¸",
                        code_changed=None,
                        diff=diff_text,
                        performance_metrics=None,
                        error=f"åº”ç”¨ diff å¤±è´¥: {e}",
                        code_snapshot=current_code,
                        summary=summary_text,
                    )
                    continue

            # å®Œæˆä¼˜åŒ–
            # è®¡ç®— success æ—¶ç¡®ä¿å‚ä¸æ¯”è¾ƒçš„å€¼ä¸ºåŸç”Ÿ Python ç±»å‹
            target = self.config.optimization.target
            initial_trimmed = initial_performance.get("performance_analysis", {}).get(target, float("inf"))
            try:
                item_fn = getattr(initial_trimmed, "item", None)
                if callable(item_fn):
                    initial_trimmed = item_fn()
            except Exception:
                pass
            if isinstance(initial_trimmed, str):
                s = initial_trimmed.strip().lower()
                if s in ("inf", "+inf", "infinity", "+infinity"):
                    initial_trimmed = float("inf")
                elif s in ("-inf", "-infinity"):
                    initial_trimmed = float("-inf")
                elif s == "nan":
                    initial_trimmed = float("nan")
                else:
                    try:
                        initial_trimmed = float(initial_trimmed)
                    except Exception:
                        initial_trimmed = float("inf")

            try:
                bp_item = getattr(best_performance, "item", None)
                if callable(bp_item):
                    best_performance = bp_item()
            except Exception:
                pass

            final_result = {
                "instance_id": instance_id,
                "initial_code": initial_code,
                "optimized_code": latest_optimized_code,
                "initial_performance": initial_trimmed,
                "final_performance": best_performance,
                # æ€»è¿­ä»£æ•° = åˆå§‹è¯„ä¼°(è‹¥å­˜åœ¨) + å®é™…ä¼˜åŒ–å¾ªç¯æ¬¡æ•°
                "total_iterations": (1 if self._initial_code_source in ("text", "dir") else 0) + remaining_iterations,
                "optimization_history": self.optimization_history,
                # æ˜¾å¼è½¬æ¢ä¸º Python boolï¼Œé¿å… numpy.bool_
                "success": bool(best_performance < initial_trimmed),
            }

            unit = "s" if target == "runtime" else ("MB" if target == "memory" else "MB*s")
            final_result["language"] = language
            final_result["optimization_target"] = target
            final_result["performance_unit"] = unit

            try:
                md_metrics, md_artifacts = self._build_metrics_and_artifacts(current_benchmark_results)
                final_result["final_artifacts"] = self._format_artifacts_md(md_artifacts)
            except Exception:
                final_result["final_artifacts"] = None

            # è®°å½•æœ€ç»ˆè½¨è¿¹
            trajectory_file = trajectory.finalize(
                success=final_result["success"],
                final_performance={
                    "target": self.config.optimization.target,
                    "trimmed_mean": best_performance,
                    "unit": unit,
                },
                final_submission_code=latest_optimized_code,
            )

            final_result["trajectory_file"] = trajectory_file

            return final_result

        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–è¿‡ç¨‹å¤±è´¥: {e}")
            try:
                trajectory.finalize(success=False, error_message=str(e), final_submission_code=best_code)
            except Exception:
                trajectory.finalize(success=False, error_message=str(e), final_submission_code=None)
            raise

    def _build_optimization_prompt(
        self,
        current_program: str,
        language: str,
        benchmark_results: dict[str, Any],
    ) -> str:
        """æ„å»ºä¼˜åŒ–æç¤ºè¯ï¼Œå¡«å……å½“å‰ç¨‹åºã€è¯„ä¼°æŒ‡æ ‡ä¸æ„ä»¶(section)ã€‚"""
        if self.config.optimization.code_generation_mode == "direct":
            return self.config.prompts.optimization_template
        # æ„é€  metrics ä¸ artifacts
        metrics_dict, artifacts_dict = self._build_metrics_and_artifacts(benchmark_results)
        # ä»¥ Markdown æ ¼å¼åŒ–ï¼Œä¾¿äºæ¨¡å‹é˜…è¯»
        current_metrics_str = self._format_metrics_md(metrics_dict)
        current_artifacts_str = self._format_artifacts_md(artifacts_dict)
        current_program_md = f"```\n{current_program}\n```"

        try:
            return self.config.prompts.optimization_template.format(
                current_program=current_program_md,
                current_metrics=current_metrics_str,
                current_artifacts_section=current_artifacts_str,
                language=language,
            )
        except Exception:
            # è‹¥æ¨¡æ¿å ä½ç¬¦ä¸åŒ¹é…ï¼Œå›é€€ä¸ºä¸€ä¸ªé€šç”¨æç¤º
            return (
                "# Task\n"
                "è¯·åˆ†æä»¥ä¸‹ç¨‹åºä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç³»ç»Ÿæç¤ºç”Ÿæˆ `## Thinking` ä¸ `## Diffs`ï¼š\n\n"
                "## Current Program\n" + current_program_md + "\n\n"
                "## Current Metrics\n" + current_metrics_str + "\n\n"
                "## Current Artifacts\n" + current_artifacts_str
            )

    def _build_system_prompt(self, language: str, optimization_target: str, task_description: str) -> str:
        """æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯ï¼Œå¡«å……è¯­è¨€/ä¼˜åŒ–ç›®æ ‡/ä»»åŠ¡æè¿°/é™„åŠ è¦æ±‚ã€‚"""
        tmpl = self.config.prompts.system_template
        additional = self.config.prompts.additional_requirements or ""
        local_memory = getattr(self.config.prompts, "local_memory", None) or ""
        if tmpl:
            try:
                return tmpl.format(
                    language=language,
                    optimization_target=optimization_target,
                    task_description=task_description,
                    additional_requirements=additional,
                    local_memory=local_memory,
                )
            except Exception:
                return tmpl
        # é»˜è®¤æç¤º
        return (
            f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚ç›®æ ‡æ˜¯æå‡ {optimization_target}ã€‚\n"
            f"å½“å‰è¯­è¨€ï¼š{language}ã€‚ä»»åŠ¡æè¿°ï¼š{task_description}\n\n"
            f"é™„åŠ è¦æ±‚ï¼š{additional}\n\n"
            f"æœ¬åœ°è®°å¿†ï¼š{local_memory}"
        )

    def _build_metrics_and_artifacts(self, benchmark_results: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        """æ ¹æ®åŸºå‡†è¯„ä¼°ç»“æœæ„é€  current_metrics ä¸ current_artifacts_sectionã€‚"""
        performance_metrics = benchmark_results.get("performance_analysis", {})
        failed_test_details = benchmark_results.get("failed_test_details", []) or []

        # å¤±è´¥æƒ…å†µï¼šæ±‡æ€»å¤±è´¥ä¿¡æ¯å¹¶è¿”å›é”™è¯¯æŒ‡æ ‡
        target = self.config.optimization.target
        target_value = performance_metrics.get(target, float("inf"))
        if failed_test_details or target_value == float("inf"):
            num_failed = len(failed_test_details)
            num_total = len(benchmark_results.get("first_run_details", []))
            pass_rate = (num_total - num_failed) / num_total if num_total > 0 else 0

            representative_failures: dict[str, Any] = {}
            for failure in failed_test_details:
                status = failure.get("status", "unknown")
                if status not in representative_failures:
                    representative_failures[status] = failure

            failure_details_summary: list[str] = []
            for status, failure in representative_failures.items():
                text = failure.get("text", "No additional error text.")
                if isinstance(text, str) and len(text) > 300:
                    text = text[-300:] + "..."
                failure_details_summary.append(f"- Status: {status}, Details (last 300 chars of Output): {text}")

            failures_text = "\n".join(failure_details_summary)
            all_statuses = ", ".join(representative_failures.keys())

            error_artifacts = {
                "error_type": f"SolutionFailedTests (statuses: {all_statuses})",
                "error_message": (f"Solution passed {pass_rate:.2%} of test cases. Failure details:\n{failures_text}"),
                "suggestion": (
                    "Review the solution to ensure it correctly handles all test cases, including edge cases."
                ),
            }

            metrics = {
                "pass_rate": pass_rate,
                f"trimmed_mean_{target}": "Infinity",
                "target": target,
                "error": (
                    f"Solution failed {len(failed_test_details)} test case(s) with statuses: {all_statuses}. See artifacts for details."
                ),
            }
            return metrics, error_artifacts

        # æˆåŠŸæƒ…å†µï¼šè®¡ç®—æ—¶é—´åˆ†æ•°ä¸ç»¼åˆåˆ†æ•°
        pass_rate = 1.0
        trimmed_mean_runtime = performance_metrics.get(target, float("inf"))

        metrics = {
            "pass_rate": pass_rate,
            f"trimmed_mean_{target}": trimmed_mean_runtime,
            "target": target,
        }
        artifacts = {"details": "All test cases passed."}
        return metrics, artifacts

    def _format_metrics_md(self, metrics: dict[str, Any]) -> str:
        """å°†æ€§èƒ½æŒ‡æ ‡æ ¼å¼åŒ–ä¸º Markdown æ–‡æœ¬ã€‚"""
        lines: list[str] = []
        # pass_rate -> ç™¾åˆ†æ¯”
        pr = metrics.get("pass_rate")
        if pr is not None:
            try:
                pr_pct = f"{float(pr) * 100:.2f}%"
            except Exception:
                pr_pct = str(pr)
            lines.append(f"- Pass rate: {pr_pct}")

        # trimmed_mean_target
        tmr_key = next((k for k in metrics.keys() if k.startswith("trimmed_mean_")), None)
        tmr = metrics.get(tmr_key) if tmr_key else None
        if tmr is not None and tmr_key:
            tgt = tmr_key.split("_", 2)[-1]
            unit = "s" if tgt == "runtime" else ("MB" if tgt == "memory" else "MB*s")
            if isinstance(tmr, (int, float)):
                if tmr == float("inf"):
                    lines.append(f"- Trimmed mean {tgt}: Infinity")
                else:
                    lines.append(f"- Trimmed mean {tgt}: {float(tmr):.6f} {unit}")
            else:
                val = str(tmr)
                low = val.strip().lower()
                if low in ("inf", "+inf", "infinity", "+infinity"):
                    lines.append(f"- Trimmed mean {tgt}: Infinity")
                else:
                    lines.append(f"- Trimmed mean {tgt}: {val} {unit}")

        # é”™è¯¯ä¿¡æ¯ï¼ˆä»…åœ¨å¤±è´¥æ—¶å­˜åœ¨ï¼‰
        err = metrics.get("error")
        if err:
            lines.append(f"- Error: {err}")

        return "\n".join(lines) if lines else "- No metrics available."

    def _format_artifacts_md(self, artifacts: dict[str, Any]) -> str:
        """å°†æ„ä»¶ä¿¡æ¯æ ¼å¼åŒ–ä¸º Markdown æ–‡æœ¬ã€‚"""
        if not artifacts:
            return "- No artifacts available."
        lines: list[str] = []
        for k, v in artifacts.items():
            if isinstance(v, str) and "\n" in v:
                indented = "\n  ".join(v.split("\n"))
                lines.append(f"- {k}: {indented}")
            else:
                lines.append(f"- {k}: {v}")
        return "\n".join(lines)

    def _build_summary_text(
        self,
        iteration: int,
        code_changed: bool,
        diff_text: str | None,
        benchmark_results: dict[str, Any] | None,
        current_program: str | None = None,
        error_message: str | None = None,
    ) -> str:
        """æ„å»ºä¸€æ­¥è¿­ä»£çš„ Markdown æ‘˜è¦æ–‡æœ¬ï¼ŒåŒ…å«ç¨‹åºæ›´æ–°ã€å½“å‰ç¨‹åºã€æŒ‡æ ‡ä¸æ„ä»¶ã€‚

        - metrics/artifacts ç”± `_build_metrics_and_artifacts` ç”Ÿæˆå¹¶é€šè¿‡ `_format_*_md` æ ¼å¼åŒ–ã€‚
        - æ— è¯„ä¼°æˆ–å¤±è´¥æ—¶ï¼Œè¾“å‡ºé”™è¯¯ä¿¡æ¯å’Œå ä½æ„ä»¶ã€‚
        """
        # æ„é€ æŒ‡æ ‡ä¸æ„ä»¶
        if benchmark_results:
            metrics_dict, artifacts_dict = self._build_metrics_and_artifacts(benchmark_results)
        else:
            metrics_dict = {}
            artifacts_dict = {}
            if error_message:
                metrics_dict["error"] = error_message
                if not artifacts_dict:
                    artifacts_dict["details"] = "No evaluation due to error."

        metrics_md = self._format_metrics_md(metrics_dict)
        artifacts_md = self._format_artifacts_md(artifacts_dict)
        diff_size = len(diff_text) if diff_text else 0

        prog_text = current_program or ""

        return (
            "## Program Update\n"
            f"- Iteration: {iteration}\n"
            f"- Code changed: {'yes' if code_changed else 'no'}\n"
            f"- Diff size: {diff_size} chars\n\n"
            "## Current Program\n" + prog_text + "\n\n"
            "## Current Metrics\n" + metrics_md + "\n\n"
            "## Current Artifacts\n" + artifacts_md
        )

    def _extract_full_code_from_response(self, response: str) -> str:
        """ä»æ¨¡å‹å“åº”ä¸­æå–å®Œæ•´ä»£ç ï¼ˆMarkdown ä»£ç å—ï¼‰ã€‚"""
        if not response:
            return ""
        # åŒ¹é… ```language ... ```
        # å°è¯•åŒ¹é… python, cpp, java, etc. æˆ–è€…ä¸æŒ‡å®š
        pattern = r"```(?:\w+)?\n(.*?)```"
        matches = re.findall(pattern, response, re.DOTALL)
        if matches:
            # è¿”å›æœ€åä¸€ä¸ªåŒ¹é…çš„ä»£ç å—ï¼Œé€šå¸¸æ˜¯æœ€ç»ˆä»£ç 
            return matches[-1].strip()
        return ""

    def _extract_diff_from_response(self, response: str) -> str:
        """ä»æ¨¡å‹å“åº”ä¸­æå– diff
        ä»…æ”¯æŒ SEARCH/REPLACE åŒºå—æ ¼å¼ã€‚
        """
        if not response:
            return ""
        if "<<<<<<< SEARCH" in response and ">>>>>>> REPLACE" in response:
            try:
                start_idx = response.find("<<<<<<< SEARCH")
                end_idx = response.rfind(">>>>>>> REPLACE")
                if start_idx != -1 and end_idx != -1 and end_idx >= start_idx:
                    return response[start_idx : end_idx + len(">>>>>>> REPLACE")].strip()
            except Exception:
                return ""
        return ""

    def _build_messages(
        self, system_prompt: str, history: list[dict[str, Any]], user_prompt: str, limit: int = 200
    ) -> list[dict[str, str]]:
        use_all = bool(getattr(self.config.prompts, "include_all_history", False))
        if use_all:
            msgs: list[dict[str, str]] = []
            tail = history[-limit:] if len(history) > limit else history
            for h in tail:
                role = h.get("role")
                content = h.get("content", "")
                if role in ("system", "user", "assistant") and content:
                    msgs.append({"role": role, "content": content})
            return msgs
        return [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
