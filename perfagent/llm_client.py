"""
LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’ŒAPIç«¯ç‚¹
"""

import json
import logging
import os
import threading
import time
from pathlib import Path
from typing import Any

from openai import OpenAI

from .utils.log import get_se_logger


class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’ŒAPIç«¯ç‚¹"""

    def __init__(
        self,
        model_config: dict[str, Any],
        max_retries: int = 3,
        retry_delay: float = 1.5,
        io_log_path: str | Path | None = None,
        log_inputs_outputs: bool = True,
        log_sanitize: bool = True,
        request_timeout: float = 60.0,
    ):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            model_config: æ¨¡å‹é…ç½®å­—å…¸ï¼ŒåŒ…å«name, api_base, api_keyç­‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: æ¯æ¬¡é‡è¯•çš„ç­‰å¾…ç§’æ•°
            io_log_path: LLM è¾“å…¥/è¾“å‡ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
            log_inputs_outputs: æ˜¯å¦è®°å½•åŸå§‹è¾“å…¥è¾“å‡º
        """
        self.config = model_config
        # ç»Ÿä¸€ä½¿ç”¨æ–‡ä»¶æ—¥å¿—ï¼ˆå¸¦ emojiï¼‰ï¼Œä¸ IO æ—¥å¿—åŒç›®å½•
        self.io_log_path = Path(io_log_path) if io_log_path else Path("./logs/llm_io.log")
        # Logger åç§°å¢åŠ ä»»åŠ¡ååç¼€ï¼ˆå–æ—¥å¿—ç›®å½•åï¼‰ï¼Œé¿å…å¹¶å‘ä»»åŠ¡å†²çª
        task_suffix = self.io_log_path.parent.name or "default"
        logger_name = f"perfagent.llm_client.{task_suffix}"
        get_se_logger(logger_name, self.io_log_path, emoji="ğŸ¤–", also_stream=False)
        self.logger = logging.getLogger(logger_name)
        self.token_log_path = os.getenv("SE_TOKEN_LOG_PATH")
        self._token_lock = threading.Lock()

        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„å¢å¼ºå‚æ•°
        self.max_retries = int(model_config.get("max_retries", max_retries))
        self.retry_delay = float(model_config.get("retry_delay", retry_delay))
        self.log_inputs_outputs = bool(model_config.get("log_inputs_outputs", log_inputs_outputs))
        self.log_sanitize = bool(model_config.get("log_sanitize", log_sanitize))
        self.request_timeout = float(model_config.get("request_timeout", request_timeout))

        # éªŒè¯å¿…éœ€çš„é…ç½®å‚æ•°
        required_keys = ["name", "api_base", "api_key"]
        missing_keys = [key for key in required_keys if key not in model_config]
        if missing_keys:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„é…ç½®å‚æ•°: {missing_keys}")

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œéµå¾ªapi_test.pyçš„å·¥ä½œæ¨¡å¼
        self.client = OpenAI(
            api_key=self.config["api_key"],
            base_url=self.config["api_base"],
            timeout=self.request_timeout,
        )

        self.logger.info(f"åˆå§‹åŒ–LLMå®¢æˆ·ç«¯: {self.config['name']}")

    def _format_content_for_log(self, content: str | None, indent: int = 2) -> str:
        """å°†æ–‡æœ¬å†…å®¹æ ¼å¼åŒ–ä¸ºå¤šè¡Œæ—¥å¿—ï¼Œä¿ç•™çœŸå®æ¢è¡Œå¹¶ç¼©è¿›ã€‚

        Args:
            content: æ–‡æœ¬å†…å®¹
            indent: ç¼©è¿›ç©ºæ ¼æ•°é‡

        Returns:
            å‹å¥½çš„å¤šè¡Œå­—ç¬¦ä¸²ï¼Œå¸¦ç¼©è¿›å¹¶ä¿ç•™æ¢è¡Œ
        """
        prefix = " " * indent
        if content is None:
            return f"{prefix}content: (None)"
        text = str(content)
        if text == "":
            return f"{prefix}content: (empty)"
        lines = text.splitlines() or [text]
        formatted = [f"{prefix}content:"]
        formatted.extend(f"{prefix}  {line}" for line in lines)
        return "\n".join(formatted)

    def _format_messages_for_log(self, messages: list[dict[str, str]], indent: int = 0) -> str:
        """å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå¤šè¡Œæ—¥å¿—ï¼Œä¿ç•™çœŸå®æ¢è¡Œå¹¶ç¼©è¿›å†…å®¹ã€‚"""
        base_prefix = " " * indent
        out_lines = [f"{base_prefix}messages:"]
        for i, m in enumerate(messages, start=1):
            role = m.get("role", "unknown")
            out_lines.append(f"{base_prefix}  [{i}] role: {role}")
            out_lines.append(self._format_content_for_log(m.get("content"), indent=indent + 4))
        return "\n".join(out_lines)

    def call_llm(
        self,
        messages: list[dict[str, str]],
        temperature: float = 0.3,
        max_tokens: int | None = None,
        usage_context: str | None = None,
    ) -> str:
        """
        è°ƒç”¨LLMå¹¶è¿”å›å“åº”å†…å®¹

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼

        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹

        Raises:
            Exception: LLMè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ä½¿ç”¨é…ç½®ä¸­çš„max_output_tokensä½œä¸ºé»˜è®¤å€¼
        if max_tokens is None:
            max_tokens = self.config.get("max_output_tokens", 4000)

        attempt = 0
        last_err: Exception | None = None
        while attempt < self.max_retries:
            try:
                self.logger.debug(f"è°ƒç”¨LLM: {len(messages)} æ¡æ¶ˆæ¯, temp={temperature}, max_tokens={max_tokens}")

                # è®°å½•åŸå§‹è¾“å…¥
                if self.log_inputs_outputs:
                    try:
                        # æ—¥å¿—è„±æ•ï¼šç§»é™¤å¯èƒ½çš„å¯†é’¥ä¸ç«¯ç‚¹ä¿¡æ¯
                        model_name = self.config.get("name", "unknown")
                        api_base = self.config.get("api_base", "")
                        safe_api_base = "<redacted>" if self.log_sanitize and api_base else api_base
                        req_lines = [
                            "LLM Request:",
                            f"model: {model_name}",
                            f"temperature: {temperature}",
                            f"max_tokens: {max_tokens}",
                            f"api_base: {safe_api_base}",
                            self._format_messages_for_log(messages, indent=0),
                        ]
                        self.logger.info("\n".join(req_lines))
                    except Exception as log_e:
                        self.logger.error(f"è®°å½•è¯·æ±‚å¤±è´¥: {log_e}")

                # ä½¿ç”¨åŸºæœ¬çš„OpenAIå®¢æˆ·ç«¯è°ƒç”¨ï¼Œéµå¾ªapi_test.pyçš„å·¥ä½œæ¨¡å¼
                # ä¸ä½¿ç”¨é¢å¤–å‚æ•°ï¼Œé¿å…æœåŠ¡å™¨é”™è¯¯
                # ç¦æ­¢æ€è€ƒï¼Œä»…è¿”å›ç›´æ¥å›ç­”
                response = self.client.chat.completions.create(
                    model="/".join(self.config["name"].split("/")[1:]),
                    messages=messages,
                    temperature=temperature,
                    extra_body={
                        "chat_template_kwargs": {"enable_thinking": False},
                    },
                )

                # æå–å“åº”å†…å®¹
                content = response.choices[0].message.content

                # è®°å½•ä½¿ç”¨æƒ…å†µ
                if getattr(response, "usage", None):
                    self.logger.debug(
                        f"Tokenä½¿ç”¨: è¾“å…¥={getattr(response.usage, 'prompt_tokens', 'æœªçŸ¥')}, "
                        f"è¾“å‡º={getattr(response.usage, 'completion_tokens', 'æœªçŸ¥')}, "
                        f"æ€»è®¡={getattr(response.usage, 'total_tokens', 'æœªçŸ¥')}"
                    )
                    try:
                        if self.token_log_path:
                            entry = {
                                "ts": time.time(),
                                "context": usage_context or "perfagent",
                                "model": "/".join(self.config["name"].split("/")[1:]),
                                "prompt_tokens": getattr(response.usage, "prompt_tokens", None),
                                "completion_tokens": getattr(response.usage, "completion_tokens", None),
                                "total_tokens": getattr(response.usage, "total_tokens", None),
                                "messages_chars": sum(
                                    len(str(m.get("content", ""))) for m in messages if isinstance(m, dict)
                                ),
                            }
                            with self._token_lock:
                                with open(self.token_log_path, "a", encoding="utf-8") as f:
                                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                    except Exception:
                        pass

                # è®°å½•åŸå§‹è¾“å‡º
                if self.log_inputs_outputs:
                    try:
                        usage = getattr(response, "usage", None)
                        usage_dict = None
                        if usage:
                            usage_dict = {
                                "prompt_tokens": getattr(usage, "prompt_tokens", None),
                                "completion_tokens": getattr(usage, "completion_tokens", None),
                                "total_tokens": getattr(usage, "total_tokens", None),
                            }
                        # å“åº”æ—¥å¿—è„±æ•ï¼šä¸è®°å½•åŸå§‹å¤´ä¿¡æ¯
                        resp_lines = [
                            "LLM Response:",
                            self._format_content_for_log(content, indent=0),
                            "usage: " + json.dumps(usage_dict, ensure_ascii=False),
                        ]
                        self.logger.info("\n".join(resp_lines))
                    except Exception as log_e:
                        self.logger.error(f"è®°å½•å“åº”å¤±è´¥: {log_e}")

                return content

            except Exception as e:
                last_err = e
                self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}; attempt={attempt + 1}/{self.max_retries}")
                if self.log_inputs_outputs:
                    self.logger.error(f"LLM è°ƒç”¨å¼‚å¸¸: {e}")
                attempt += 1
                if attempt < self.max_retries:
                    time.sleep(self.retry_delay)
                else:
                    break

        # é‡è¯•åä»å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
        assert last_err is not None
        raise last_err

    def call_with_system_prompt(
        self, system_prompt: str, user_prompt: str, temperature: float = 0.3, max_tokens: int | None = None
    ) -> str:
        """
        ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯è°ƒç”¨LLM

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°

        Returns:
            LLMå“åº”çš„æ–‡æœ¬å†…å®¹
        """
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

        return self.call_llm(messages, temperature, max_tokens)
